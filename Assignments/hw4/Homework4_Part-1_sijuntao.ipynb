{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdMTmAw0_U6t"
   },
   "source": [
    "# Homework 4 - Code Search using Deep Learning Bi-Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uplu6Pe27p09"
   },
   "source": [
    "Modern search engines are capable of working with more than just text: images, videos, audio, songs, ... the list goes on and on! In Homework 4, you'll develop an IR system that works in two modalities: text queries and _code_ documents. Code may be written in text characters (it's human readable, unlike image files) but queries like ``python function to test if file exists`` may look nothing like the code that matches the query's intent.\n",
    "\n",
    "To bridge these two languages, we'll use a relative recent technique known as a *bi-encoder* model to convert text queries and code documents into the same representation space. If you recall from class, our tf-idf vectorizer was an encoder—-it would convert some text query into a high-dimensional vector, _encoding_ the text into some numeric space. Here, we'll use deep learning encoders that also produce numeric vectors. However, unlike our tf-idf vectors which are large (the size of the vocabulary) and sparse (most queries don't have most words), our deep learning-produced vectors will be relatively small (hundreds of dimensions) and dense (all dimensions have a non-zero value). \n",
    "\n",
    "For text, most of the common deep learning models are similar to the **BERT** model we talked about in class during the deep learning week. If you recall, this was the model that was trained using a MadLibs-style approach where it's shown most of a sentence and asked to fill in the blanks for a few words that have been removed. This type of training lets the model ultimately produce a dense vector representation for the \"meaning\" of a text input--essentially acting as a text encoder. If you're curious, there are a few [good](https://huggingface.co/blog/bert-101) [tutorials](https://wandb.ai/mukilan/BERT_Sentiment_Analysis/reports/An-Introduction-to-BERT-And-How-To-Use-It--VmlldzoyNTIyOTA1) on what this model is all about, though you don't need to know it for the purposes of this assignment. The most relevant thing to know is that other folks have adapted this approach to produce other models that can encode the meanings of other kinds of input, _like code_.\n",
    "\n",
    "<img src=\"https://weaviate.io/img/blog/cross-encoders/bi-encoder.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "The bi-encoder architecture (shown above for two _sentence_ inputs) use two of these deep learning encoder models to compare the encodings of each model's input in some way. Here, we'll compare an encoded query and an encoded-code-document to measure the IR _relevance_ of that document for the given query. The bi-encoder setup lets us bridge between the two languages by _training the model to adjust its representations_. The key idea here is that both encoders know how to represent each of their respective inputs (they've already been pre-trained to do this) and by further training, we can get the encoders to adjust how they represent things so that representations for very relevant query-document papers look similar and representations for non-relevant pairs look dissimilar.\n",
    "\n",
    "### Homework summary\n",
    "\n",
    "This homework provides a substantial amount of code to get you started and has you focus on just a few implementation pieces.\n",
    "\n",
    "* **Part 1** of the homework will be training the bi-encoder model. You will need a GPU for this, so you'll want to run this on a Great Lakes machine with a GPU. We've done testing and set the hyperparameters below so that it should run quickly enough. _But_ it will still take an hour to complete training once you get it up and running. Once you have the model trained, you'll export the query-document relevance scores from the bi-encoder to a file for use later.\n",
    "\n",
    "* **Part 2** will have you learning how to do Learning to Rank (L2R) again and incorporating the bi-encoder's predictions. For this part, you will need to have access to Java to run Pyterrier. However, you will not need a GPU and this part can be run either on a non-GPU Great Lakes machine or on your local machine. You will still need access to the outputs of the bi-encoder which are in a file, so if you work locally, you'll need to copy those still. Part 2 will show you how to set up new pipelines that make use of precomputed features.\n",
    "\n",
    "For both parts, we've put in extensive comments throughout so you can get a sense of what is happening and why. You don't need to understand all of the code, but it will be helpful to try to understand the general flow of things and how you train these kinds of models at a high level (possibly relating to questions on the final exam).\n",
    "\n",
    "### Implementation Notes \n",
    "\n",
    "This notebook contains a lot of code that is intended to expose you to various deep learning concepts _but_ where you don't need to modify it. To streamline the process, we've put even more low-level code into two files that you don't need to read: \n",
    "\n",
    "- **edited_roberta.py** contains the pre-trained RoBERTa model architecture.\n",
    "- **run_classifiers.py** contains the some helpful scripts for running things and loading data.\n",
    "\n",
    "If you've never seen PyTorch code, that's ok (and expected!). We've added detailed instructions on what you need to know or learn in each step. Your pytorch code will be very (_very_) minimal, but we do want you to try a few things. There are also many good tutorials out there for pytorch _but_ these tutorial cover more information than what is needed too, so please read judiciously so you don't feel overwhelmed. If you do decide to read more, perhaps one general place to start is tutorials on using PyTorch for [logistic regression](https://towardsdatascience.com/logistic-regression-with-pytorch-3c8bbea594be).\n",
    "\n",
    "\n",
    "### What to do\n",
    "\n",
    "For Part 1, we've marked your parts with TODO. To get a sense of what and why you're doing something, be sure to read the nearby parts of the notebook for context. \n",
    "\n",
    "In general, you will need a GPU to run this. However, we've provided a very tiny input file `data/train_10.txt` that has only 10 training examples. Training only on this file will produce horrible results. _However_ it is small enough that you can probably run the code on a CPU to start debugging, though this may take a lot of memory. Worst case, you can start and debug this file on a Great Lakes machine (without a GPU) and once you think it's working, switch to a GPU.\n",
    "\n",
    "\n",
    "### Learning goals for Part 1 of Homework 4:\n",
    "* Expose you to how to run and train deep learning systems\n",
    "* Gain familiarity with bi-encoder models for IR with different modalities\n",
    "* Learn how to read and import data for training bi-encoder models\n",
    "* Learn how to export relevance predictions from deep learning models for use in IR ranking systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 12:53:05.570043: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-06 12:53:05.826366: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-06 12:53:05.830458: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /sw/pkgs/arc/openjdk/jdk-18.0.1.1/lib:/opt/slurm/lib64::\n",
      "2022-12-06 12:53:05.830483: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-06 12:53:08.714125: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /sw/pkgs/arc/openjdk/jdk-18.0.1.1/lib:/opt/slurm/lib64::\n",
      "2022-12-06 12:53:08.714543: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /sw/pkgs/arc/openjdk/jdk-18.0.1.1/lib:/opt/slurm/lib64::\n",
      "2022-12-06 12:53:08.714557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from edited_roberta import *\n",
    "from run_classifier import evaluate, load_and_cache_examples, accuracy, set_seed\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import (WEIGHTS_NAME, get_linear_schedule_with_warmup, AdamW,RobertaConfig,RobertaTokenizer)\n",
    "from utils import (compute_metrics, convert_examples_to_features,\n",
    "                        output_modes, processors)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining our Bi-Encoder model\n",
    "\n",
    "In the following block, we'll create our neural network bi-encoder model. To do so, we'll define a new class `CodeSearchBiencoderModel` that contains the two encoders, which are the encoder for query and the encoder for code. \n",
    "\n",
    "In pytorch, neural networks are defined by specifying their _parameters_ (the things that get updated during training) and a `forward` function that determines how to turn the inputs into outputs. For our bi-encoder, we'll need to fill these in as follows:\n",
    "\n",
    "* Specify the two encoders in the `__init__` function as fields of the class (e.g., `self.x = 1` makes `x` a field of the object), which will tell PyTorch that we'll be updating their parameters during training\n",
    "* Write the `forward` function so that we...\n",
    "  * encode the query as a vector\n",
    "  * encode the code-document as a vector\n",
    "  * compute the cosine similarity of the two vectors (where 1 is relevant, 0 is not-relevant)\n",
    "  \n",
    "We'll detail these next.\n",
    "\n",
    "## Creating the encoders\n",
    "\n",
    "How do we instantiate an encoder? There are two steps. First we need to figure out what is the _architecture_ of the model. This defines things like how many layers are in a neural network and how the layers are connected. In our case, _both_ of our encoders will use the RoBERTa architecture; as you might have guessed, RoBERTa is related to BERT and just has slightly different tweaks. There are [many BERT variants](https://towardsdatascience.com/exploring-bert-variants-albert-roberta-electra-642dfe51bc23) and for the purposes of this excise, you can safely think of RoBERTa as the same as BERT.\n",
    "\n",
    "Second, once we have our model architecture, we need to specify which parameters we'll start with. You can think of the difference between the model architecture and parameters as if you were specifying a meal: The architecture is a bit like specifying the plates/bowls/container based on what kind of food you want and the parameters are like filling the container with a specific kind of food. There are many pre-trained sets of parameters for architectures, so a neural network starts with some existing knowledge of certain kinds of things (e.g., what human  language looks like,  what programming languages look like, or how to classify images). In neural network land, the [Huggingface Model Repository](https://huggingface.co/models) is a common place to look for parameters that people have shared with others.\n",
    "\n",
    "Returning to our IR problem, in our setting (conveniently), both of our encoders will use the same architecture: `RobertaModel`. If you _really_ want to know more, the code for this is provided in `edited_roberta.py`, but many end-users of these models (like us) will never need to look at this kind of code--and you certainly don't to complete this homework!\n",
    "\n",
    "## Writing the forward function \n",
    "\n",
    "The `forward` function defines how the neural network goes from inputs to outputs. In our case, we're going to feed the different inputs (query and document) to separate encoders and then compare the outputs. \n",
    "\n",
    "\n",
    "Your task here:\n",
    "1. Define the two encoders(query encoder and code_encoder) which are RobertaModel loaded from edit_roberta.py. You should two arguments when creating the instance of the Roberta Model, which are config and add_pooling_layer(equal to False).\n",
    "2. define and calculate the cosine similarity between two embeddings.\n",
    "3. define the loss function and loss.\n",
    "\n",
    "### TODOs in this block are worth 20 points total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeSearchBiencoderModel(RobertaPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        ## TODO: \n",
    "        # Fill in the following parts where you specify each encoder's architecture. \n",
    "        # You'll need to pass in \"config\" as an argument to the architecture's constructor\n",
    "        # so it knows how to set things up.\n",
    "        #\n",
    "        # NOTE 1: Notice that we haven't specified the *parameters* here, just the architecture.\n",
    "        # We'll fill in the parameters later\n",
    "        #\n",
    "        # NOTE 2: If you were ever curious how to do other kinds of non-text IR (e.g., images),\n",
    "        # this part is where you'd specify a different kind of encoder architecture, such as\n",
    "        # ResNet50 for encoding images. The rest of the code for this class would be mostly the same!\n",
    "        # (The one caveat is that both models need to produce vector representations of the same size)\n",
    "        \n",
    "        self.query_encoder = RobertaModel(config,add_pooling_layer = False)\n",
    "        self.code_encoder = RobertaModel(config,add_pooling_layer = False)\n",
    "\n",
    "        # This is our loss function that determines how \"good\" our model's output is.\n",
    "        # We'll use this in the forward() function to evaluate the model's outputs and\n",
    "        # then return the predictions and loss.        \n",
    "        self.loss_fn = BCEWithLogitsLoss()\n",
    "        \n",
    "        # This will initialize weights and apply final processing\n",
    "        self.post_init()\n",
    " \n",
    "    def forward(\n",
    "        self,\n",
    "        query_token_ids: Optional[torch.LongTensor] = None,\n",
    "        code_token_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "       \n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "       \n",
    "\n",
    "        outputs = self.query_encoder(\n",
    "            query_token_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        query_emb = outputs[0][:, 0, :]\n",
    "        \n",
    "        outputs_code = self.code_encoder(\n",
    "            code_token_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        code_emb = outputs_code[0][:, 0, :]\n",
    "\n",
    "        # TODO: using the cosine_similarity function (imported above),\n",
    "        # compute the similarity of the query and code embeddings.\n",
    "        # ref: RobertaForSequenceClassification\n",
    "        cosine_sim = cosine_similarity(query_emb, code_emb)\n",
    "                       \n",
    "        # TODO: use the self.loss_fn (our loss function) to measure how good/bad\n",
    "        # the predictions are. This function will return a value you should \n",
    "        # call \"loss\". You can see how to call the function here\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n",
    "        # \n",
    "        # NOTE: The \"labels\" input to this function is the ground truth\n",
    "        # relevance scores (labels) for each input. You'll want to compare\n",
    "        # the cosine similarities with these labels when calling the function\n",
    "        #\n",
    "        # NOTE 2: There are many kinds of loss functions so understanding how\n",
    "        # to call them and which order the arguments go in is important\n",
    "#         loss = self.loss_fn(cosine_sim, labels)\n",
    "        loss = self.loss_fn(cosine_sim.squeeze(), labels.squeeze())\n",
    "\n",
    "        # Finally, let's return some output! Pytorch has provided\n",
    "        # some structure for us to say what-is-what in the output\n",
    "        # values. We'll return our loss (how \"bad\" the model's prediction was)\n",
    "        # and the logits, which is our predictions. \n",
    "        #\n",
    "        # You don't need to worry about the other two outputs.\n",
    "        #\n",
    "        # NOTE: Normally, we'd be passing the cosine similarity through\n",
    "        # some non-linear function (like a sigmoid!) to get \"logits\" as\n",
    "        # our output values. However, in a bi-encoder, often you just \n",
    "        # return the cosine similarity as the logits, so the name is wrong\n",
    "        # but the value is what's expected. Later on, when we access the\n",
    "        # \"logits\" part of the output, remember these are the query-doc\n",
    "        # cosine similarity scores\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cosine_sim,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the Models and Training Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training deep learning models often involves _lots_ of hyperparameter decisions--not to mention a bunch of seemingly-random bookkeeping options for where and when to save things. We have defined these all for you (yay) but it's worth at least looking through to see what kinds of decisions you'll need to make. Most importantly, we've specified the number of epochs and the batch size (more on that later) so that the model trains quickly.\n",
    "\n",
    "You will eventually need to edit the input file name to use the full dataset. Everything else can stay the same, though you're welcome to try changing some things and seeing what happens once you've completed the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Where to save things\n",
    "        self.data_dir = './data'\n",
    "        self.model_type = 'roberta'\n",
    "        self.model_name_or_path = 'microsoft/codebert-base'\n",
    "        self.task_name = 'codesearch'\n",
    "        self.output_dir = './models'\n",
    "        self.output_mode = 'codesearch'\n",
    "\n",
    "        # These are going to be your most common hyperparameters to change.\n",
    "        # If you want to do deep learning stuff, it's worth learning a bit\n",
    "        # about what they are and what they do.\n",
    "        self.train_batch_size = 64\n",
    "        self.eval_batch_size = 64\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 1e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 3 # NOTE: Change this to 1 if debugging so it runs faster\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.n_gpu = 1\n",
    "        self.no_cuda = False\n",
    "\n",
    "        # These are mostly configuration options for which pieces to run\n",
    "        self.config_name = \"\"\n",
    "        self.tokenizer_name = \"\"\n",
    "        self.cache_dir = \"\"\n",
    "        self.max_seq_length = 200\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.do_predict = False\n",
    "        self.evaluate_during_training = False\n",
    "        self.do_lower_case = False\n",
    "\n",
    "        # How often we save things\n",
    "        self.logging_steps = 1000\n",
    "        self.save_steps = 1000\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.seed = 42\n",
    "        \n",
    "        # Ignore all of these\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "        self.local_rank = -1\n",
    "        self.server_ip = \"\"\n",
    "        self.server_port = \"\"\n",
    "        \n",
    "        # Input and output files.\n",
    "        #\n",
    "        # TODO: Change the training file to train_300k.txt when ready\n",
    "        #\n",
    "        self.train_file = \"train_300k.txt\" # CHANGE ME WHEN READY TO TRAIN!!!!!\n",
    "        self.dev_file = \"valid.txt\"\n",
    "        self.test_file = \"test_data.txt\"\n",
    "        self.pred_model_dir = './models/checkpoint-best'\n",
    "        self.test_result_dir = './results/'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following blocks, we are starting to train our model. Here we will firstly define the train function.\n",
    "\n",
    "In the train function, we will define the procedure of training the model, the main steps are:\n",
    "1. Define the dataloader (to do). You should use the function DataLoader(). Three arguments are required for you to input, which are dataset, batch_size, sampler.\n",
    "2. set the gradient to zero and train the model using back propagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the training process\n",
    "\n",
    "Let's see how the training procedure works! The code block below specifies how we'll train our model. There's one part for you to fill in that loads the data using the `DataLoader` class. The rest is helpful to understand how \n",
    "\n",
    "### TODO in this block is worth 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer, optimizer):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "\n",
    "    # The sampler specifies how we should access the training data, which\n",
    "    # in this case is in a random order\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    \n",
    "    # TODO: Initailize the DataLoader (https://pytorch.org/docs/stable/data.html)\n",
    "    # so that it \n",
    "    # - loads from the provided train_dataset \n",
    "    # - samples using our sampler\n",
    "    # - uses the specified batch size\n",
    "    #\n",
    "    # NOTE: The batch size is pretty important! This says how many examples to train on \n",
    "    # at one time. If you recall, we talked about Stocastic Gradient Descent (SGD) that\n",
    "    # updates based on one instance at a time (e.g., changing the dog t-shirt size after seeing one dog)\n",
    "    # versus Gradient Descent (GD) that updates after all the data. SGD is much faster to\n",
    "    # converge to the \"right\" parameters but can make many missteps. The batch size \n",
    "    # says we can look at more than one instance at a time in determining how to update\n",
    "    # our parameters (e.g., look at a few dogs at a time to determine how to best update \n",
    "    # the t-shirt size, rather than just one dog or all the dogs)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.train_batch_size, sampler=train_sampler)\n",
    "    \n",
    "    # How many total steps we'll take\n",
    "    t_total = len(train_dataloader) //  args.num_train_epochs\n",
    "\n",
    "    # The scheduler helps decide how quickly to update the weights based on how much\n",
    "    # training data we've seen. \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, args.warmup_steps, t_total)\n",
    "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "    scheduler_last = os.path.join(checkpoint_last, 'scheduler.pt')\n",
    "    if os.path.exists(scheduler_last):\n",
    "        scheduler.load_state_dict(torch.load(scheduler_last))\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                args.train_batch_size * args.gradient_accumulation_steps * (\n",
    "                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = args.start_step\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    best_acc = 0.0\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Note that this \"train_iterator\" is just tdqm wrapper that prints out which\n",
    "    # epoch we're currently in.     \n",
    "    train_iterator = trange(args.start_epoch, int(args.num_train_epochs), desc=\"Epoch\")\n",
    "    \n",
    "    set_seed(args) \n",
    "    \n",
    "    # This tells pytorch that we're going to be changing the parameters so it needs\n",
    "    # to start keeping track of stuff\n",
    "    model.train()\n",
    "    for idx, _ in enumerate(train_iterator):\n",
    "        \n",
    "        # Keep train of the training loss (how \"bad\" the performance is) for this epohch\n",
    "        tr_loss = 0.0\n",
    "        \n",
    "        # For one epoch, loop over all the data, one batch at a time\n",
    "        for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            inputs = {'query_token_ids': batch[0],\n",
    "                      'code_token_ids': batch[1],\n",
    "                      'labels': batch[3]}\n",
    "            \n",
    "            ouputs = model(**inputs)\n",
    "            loss = ouputs[0]        \n",
    "            \n",
    "            # Do the back propagration to figure out which parameters to change.\n",
    "            # It's that easy!\n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            \n",
    "            # Update the parameters of our model based on the gradient and whatever\n",
    "            # else the optimizer is keeping track of\n",
    "            optimizer.step() \n",
    "            scheduler.step()  \n",
    "            \n",
    "            # This sets the gradient to zero before doing next update so we don't\n",
    "            # accidentally update the model based on the last batch's performance\n",
    "            model.zero_grad() \n",
    "            global_step += 1\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                break\n",
    "\n",
    "        # Once we finish an epoch, evaluate the model on the development data and see\n",
    "        # how well it does. We'll use this information to decide which version of\n",
    "        # the parameters to use.\n",
    "        results = evaluate(args, model, tokenizer, checkpoint=str(args.start_epoch + idx))\n",
    "\n",
    "        # \n",
    "        # Save the model and if we've already saved it, overwrite that saved model with \n",
    "        # the newly-trained parameters\n",
    "        #\n",
    "        last_output_dir = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "        if not os.path.exists(last_output_dir):\n",
    "            os.makedirs(last_output_dir)\n",
    "        model_to_save = model.module if hasattr(model,\n",
    "                                                'module') else model \n",
    "        model_to_save.save_pretrained(last_output_dir)\n",
    "        logger.info(\"Saving model checkpoint to %s\", last_output_dir)\n",
    "        idx_file = os.path.join(last_output_dir, 'idx_file.txt')\n",
    "        with open(idx_file, 'w', encoding='utf-8') as idxf:\n",
    "            idxf.write(str(args.start_epoch + idx) + '\\n')\n",
    "\n",
    "        torch.save(optimizer.state_dict(), os.path.join(last_output_dir, \"optimizer.pt\"))\n",
    "        torch.save(scheduler.state_dict(), os.path.join(last_output_dir, \"scheduler.pt\"))\n",
    "        logger.info(\"Saving optimizer and scheduler states to %s\", last_output_dir)\n",
    "\n",
    "        step_file = os.path.join(last_output_dir, 'step_file.txt')\n",
    "        with open(step_file, 'w', encoding='utf-8') as stepf:\n",
    "            stepf.write(str(global_step) + '\\n')\n",
    "\n",
    "        # Optional part 1 goes here\n",
    "\n",
    "        #\n",
    "        # If this model is better (on the training data) than the models from any of the \n",
    "        # past checkpoints, then keep a separate record of that too\n",
    "        #\n",
    "        if (results['acc'] > best_acc):\n",
    "            best_acc = results['acc']\n",
    "            output_dir = os.path.join(args.output_dir, 'checkpoint-best')\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model  \n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            torch.save(args, os.path.join(output_dir, 'training_{}.bin'.format(idx)))\n",
    "            logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "            torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "            torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "            logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the training environment\n",
    "\n",
    "This will get a few things ready for the model to train. You don't need to really do much in this block but it's worth seeing how it works if you want to train models in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/02/2022 17:45:12 - INFO - __main__ -   reload model from ./models/checkpoint-last, resume from 2 epoch\n"
     ]
    }
   ],
   "source": [
    "# Setup CUDA so we can run on the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "args.device = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "\n",
    "# Set seed\n",
    "set_seed(args)\n",
    "\n",
    "# This code will help us if we restart training and want to pick back up where we left off\n",
    "args.start_epoch = 0\n",
    "args.start_step = 0\n",
    "checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\n",
    "    args.model_name_or_path = os.path.join(checkpoint_last, 'pytorch_model.bin')\n",
    "    args.config_name = os.path.join(checkpoint_last, 'config.json')\n",
    "    idx_file = os.path.join(checkpoint_last, 'idx_file.txt')\n",
    "    with open(idx_file, encoding='utf-8') as idxf:\n",
    "        args.start_epoch = int(idxf.readlines()[0].strip()) + 1\n",
    "\n",
    "    step_file = os.path.join(checkpoint_last, 'step_file.txt')\n",
    "    if os.path.exists(step_file):\n",
    "        with open(step_file, encoding='utf-8') as stepf:\n",
    "            args.start_step = int(stepf.readlines()[0].strip())\n",
    "    logger.info(\"reload model from {}, resume from {} epoch\".format(checkpoint_last, args.start_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Setting up the bi-encoder model and its parameters\n",
    "\n",
    "### the TODO in this block is worth 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sijuntao/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We'll specify some general configurations that tell the models what kind\n",
    "# of parameters to use and how to turn incoming text/code data into identifiers for \n",
    "# processing with the neural network \n",
    "#\n",
    "# We set num_labels = 1 because this is a regression class\n",
    "# (compared to a classification task with many class labels)\n",
    "\n",
    "num_labels = 1\n",
    "config = RobertaConfig.from_pretrained('microsoft/codebert-base',\n",
    "                                      num_labels=num_labels, finetuning_task=args.task_name)\n",
    "# We'll treat relevance as a regression problem\n",
    "config.problem_type = 'regression'\n",
    "\n",
    "# If you remember from our neural language model part of the lecture, we talked\n",
    "# about one language model that gets fed a series of words to predict the next\n",
    "# and each word is mapped to an embedding. The \"tokenizer\" specifies how to \n",
    "# break up words into tokens but it frequently doesn't use just spaces!\n",
    "# In fact, most tokenizers break words into *pieces* to reduce the size of the\n",
    "# vocabularly (fewer embeddings!) so we need to specify which tokenizer to use.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "\n",
    "# Now comes the time to define our model. Let's specify the model class (which we'll need later)\n",
    "model_class = CodeSearchBiencoderModel\n",
    "# And we'll instantiate the model itself.\n",
    "model = CodeSearchBiencoderModel(config)\n",
    "\n",
    "# Now comes the magic where we specify the two encoders. Conveniently for us,\n",
    "# there's actually a very recent langauge model that knows *both* code and human language!!\n",
    "# We'll use this set of parameters to initialize *each* of our encoders. Over time,\n",
    "# each encoder's parameters will start to become different since one side is going\n",
    "# to learn how to encode queries better and the other will learn how to encode \n",
    "# code documents.\n",
    "#\n",
    "# NOTE: There's nothing stopping us from trying other parameters for the\n",
    "# encoders too. If you're feeling curious you could swap in any RoBERTa model\n",
    "# for the query encoder and it will just work.\n",
    "#\n",
    "# TODO: Initialize each of the coders using the \"from_pretrained\" method and\n",
    "# specifying the pretrained model you want. Here, we'll use the CodeBERT model, \n",
    "# which is hosted on Huggingface https://huggingface.co/microsoft/codebert-base\n",
    "# You should pass in the full name of the pretrained model (which includes the \"/\").\n",
    "# Note that this code is going to look the same for both encoders and may \n",
    "# seem kind of easy to do but we want you to see how to do it yourself. :) \n",
    "model.query_encoder = (model.query_encoder).from_pretrained(\"microsoft/codebert-base\")\n",
    "model.code_encoder = (model.code_encoder).from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# This will move the model's parameters onto the GPU so it runs fast\n",
    "model.to(args.device)\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': args.weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "# Remember how we talked about stochastic gradient descent (SGD)? Well, it's not\n",
    "# the only way to update parameters. There are many (many) ways to do this\n",
    "# and the usual standard is actually AdamW which uses a bit of bookkeeping to figure\n",
    "# out how to update the weights more efficiently so the model learns faster.\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "\n",
    "# If we're restarting, load the optimizer's state at the last time step\n",
    "optimizer_last = os.path.join(checkpoint_last, 'optimizer.pt')\n",
    "if os.path.exists(optimizer_last):\n",
    "    optimizer.load_state_dict(torch.load(optimizer_last))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the training!\n",
    "\n",
    "Finally!! Let's train that model and save it to a file so we can evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/02/2022 17:46:03 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x14f82afa3910>\n",
      "12/02/2022 17:46:03 - INFO - run_classifier -   Loading features from cached file ./data/cached_train_train_300k_pytorch_model.bin_200_codesearch\n",
      "12/02/2022 17:46:26 - INFO - __main__ -   ***** Running training *****\n",
      "12/02/2022 17:46:26 - INFO - __main__ -     Num examples = 439686\n",
      "12/02/2022 17:46:26 - INFO - __main__ -     Num Epochs = 3\n",
      "12/02/2022 17:46:26 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "12/02/2022 17:46:26 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "12/02/2022 17:46:26 - INFO - __main__ -     Total optimization steps = 2290\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:08,  8.06s/it]\u001b[A\n",
      "2it [00:08,  3.72s/it]\u001b[A\n",
      "3it [00:09,  2.33s/it]\u001b[A\n",
      "4it [00:10,  1.68s/it]\u001b[A\n",
      "5it [00:10,  1.32s/it]\u001b[A\n",
      "6it [00:11,  1.11s/it]\u001b[A\n",
      "7it [00:12,  1.03it/s]\u001b[A\n",
      "8it [00:12,  1.14it/s]\u001b[A\n",
      "9it [00:13,  1.22it/s]\u001b[A\n",
      "10it [00:14,  1.29it/s]\u001b[A\n",
      "11it [00:14,  1.34it/s]\u001b[A\n",
      "12it [00:15,  1.37it/s]\u001b[A\n",
      "13it [00:16,  1.40it/s]\u001b[A\n",
      "14it [00:16,  1.42it/s]\u001b[A\n",
      "15it [00:17,  1.43it/s]\u001b[A\n",
      "16it [00:18,  1.44it/s]\u001b[A\n",
      "17it [00:19,  1.45it/s]\u001b[A\n",
      "18it [00:19,  1.45it/s]\u001b[A\n",
      "19it [00:20,  1.45it/s]\u001b[A\n",
      "20it [00:21,  1.46it/s]\u001b[A\n",
      "21it [00:21,  1.46it/s]\u001b[A\n",
      "22it [00:22,  1.46it/s]\u001b[A\n",
      "23it [00:23,  1.46it/s]\u001b[A\n",
      "24it [00:23,  1.46it/s]\u001b[A\n",
      "25it [00:24,  1.46it/s]\u001b[A\n",
      "26it [00:25,  1.46it/s]\u001b[A\n",
      "27it [00:25,  1.46it/s]\u001b[A\n",
      "28it [00:26,  1.46it/s]\u001b[A\n",
      "29it [00:27,  1.46it/s]\u001b[A\n",
      "30it [00:27,  1.46it/s]\u001b[A\n",
      "31it [00:28,  1.46it/s]\u001b[A\n",
      "32it [00:29,  1.46it/s]\u001b[A\n",
      "33it [00:29,  1.46it/s]\u001b[A\n",
      "34it [00:30,  1.46it/s]\u001b[A\n",
      "35it [00:31,  1.46it/s]\u001b[A\n",
      "36it [00:32,  1.46it/s]\u001b[A\n",
      "37it [00:32,  1.46it/s]\u001b[A\n",
      "38it [00:33,  1.46it/s]\u001b[A\n",
      "39it [00:34,  1.46it/s]\u001b[A\n",
      "40it [00:34,  1.46it/s]\u001b[A\n",
      "41it [00:35,  1.46it/s]\u001b[A\n",
      "42it [00:36,  1.46it/s]\u001b[A\n",
      "43it [00:36,  1.46it/s]\u001b[A\n",
      "44it [00:37,  1.46it/s]\u001b[A\n",
      "45it [00:38,  1.46it/s]\u001b[A\n",
      "46it [00:38,  1.46it/s]\u001b[A\n",
      "47it [00:39,  1.46it/s]\u001b[A\n",
      "48it [00:40,  1.46it/s]\u001b[A\n",
      "49it [00:40,  1.46it/s]\u001b[A\n",
      "50it [00:41,  1.46it/s]\u001b[A\n",
      "51it [00:42,  1.46it/s]\u001b[A\n",
      "52it [00:43,  1.46it/s]\u001b[A\n",
      "53it [00:43,  1.46it/s]\u001b[A\n",
      "54it [00:44,  1.45it/s]\u001b[A\n",
      "55it [00:45,  1.45it/s]\u001b[A\n",
      "56it [00:45,  1.45it/s]\u001b[A\n",
      "57it [00:46,  1.45it/s]\u001b[A\n",
      "58it [00:47,  1.45it/s]\u001b[A\n",
      "59it [00:47,  1.45it/s]\u001b[A\n",
      "60it [00:48,  1.45it/s]\u001b[A\n",
      "61it [00:49,  1.45it/s]\u001b[A\n",
      "62it [00:49,  1.45it/s]\u001b[A\n",
      "63it [00:50,  1.45it/s]\u001b[A\n",
      "64it [00:51,  1.45it/s]\u001b[A\n",
      "65it [00:51,  1.45it/s]\u001b[A\n",
      "66it [00:52,  1.45it/s]\u001b[A\n",
      "67it [00:53,  1.45it/s]\u001b[A\n",
      "68it [00:54,  1.46it/s]\u001b[A\n",
      "69it [00:54,  1.45it/s]\u001b[A\n",
      "70it [00:55,  1.46it/s]\u001b[A\n",
      "71it [00:56,  1.45it/s]\u001b[A\n",
      "72it [00:56,  1.45it/s]\u001b[A\n",
      "73it [00:57,  1.45it/s]\u001b[A\n",
      "74it [00:58,  1.45it/s]\u001b[A\n",
      "75it [00:58,  1.45it/s]\u001b[A\n",
      "76it [00:59,  1.45it/s]\u001b[A\n",
      "77it [01:00,  1.45it/s]\u001b[A\n",
      "78it [01:00,  1.45it/s]\u001b[A\n",
      "79it [01:01,  1.45it/s]\u001b[A\n",
      "80it [01:02,  1.45it/s]\u001b[A\n",
      "81it [01:02,  1.45it/s]\u001b[A\n",
      "82it [01:03,  1.45it/s]\u001b[A\n",
      "83it [01:04,  1.45it/s]\u001b[A\n",
      "84it [01:05,  1.45it/s]\u001b[A\n",
      "85it [01:05,  1.45it/s]\u001b[A\n",
      "86it [01:06,  1.45it/s]\u001b[A\n",
      "87it [01:07,  1.45it/s]\u001b[A\n",
      "88it [01:07,  1.45it/s]\u001b[A\n",
      "89it [01:08,  1.45it/s]\u001b[A\n",
      "90it [01:09,  1.45it/s]\u001b[A\n",
      "91it [01:09,  1.45it/s]\u001b[A\n",
      "92it [01:10,  1.45it/s]\u001b[A\n",
      "93it [01:11,  1.45it/s]\u001b[A\n",
      "94it [01:11,  1.45it/s]\u001b[A\n",
      "95it [01:12,  1.45it/s]\u001b[A\n",
      "96it [01:13,  1.45it/s]\u001b[A\n",
      "97it [01:14,  1.45it/s]\u001b[A\n",
      "98it [01:14,  1.45it/s]\u001b[A\n",
      "99it [01:15,  1.45it/s]\u001b[A\n",
      "100it [01:16,  1.45it/s]\u001b[A\n",
      "101it [01:16,  1.45it/s]\u001b[A\n",
      "102it [01:17,  1.45it/s]\u001b[A\n",
      "103it [01:18,  1.45it/s]\u001b[A\n",
      "104it [01:18,  1.45it/s]\u001b[A\n",
      "105it [01:19,  1.45it/s]\u001b[A\n",
      "106it [01:20,  1.45it/s]\u001b[A\n",
      "107it [01:20,  1.45it/s]\u001b[A\n",
      "108it [01:21,  1.45it/s]\u001b[A\n",
      "109it [01:22,  1.45it/s]\u001b[A\n",
      "110it [01:22,  1.45it/s]\u001b[A\n",
      "111it [01:23,  1.45it/s]\u001b[A\n",
      "112it [01:24,  1.45it/s]\u001b[A\n",
      "113it [01:25,  1.45it/s]\u001b[A\n",
      "114it [01:25,  1.45it/s]\u001b[A\n",
      "115it [01:26,  1.45it/s]\u001b[A\n",
      "116it [01:27,  1.45it/s]\u001b[A\n",
      "117it [01:27,  1.45it/s]\u001b[A\n",
      "118it [01:28,  1.45it/s]\u001b[A\n",
      "119it [01:29,  1.45it/s]\u001b[A\n",
      "120it [01:29,  1.45it/s]\u001b[A\n",
      "121it [01:30,  1.45it/s]\u001b[A\n",
      "122it [01:31,  1.45it/s]\u001b[A\n",
      "123it [01:31,  1.45it/s]\u001b[A\n",
      "124it [01:32,  1.44it/s]\u001b[A\n",
      "125it [01:33,  1.44it/s]\u001b[A\n",
      "126it [01:34,  1.44it/s]\u001b[A\n",
      "127it [01:34,  1.45it/s]\u001b[A\n",
      "128it [01:35,  1.45it/s]\u001b[A\n",
      "129it [01:36,  1.44it/s]\u001b[A\n",
      "130it [01:36,  1.44it/s]\u001b[A\n",
      "131it [01:37,  1.45it/s]\u001b[A\n",
      "132it [01:38,  1.45it/s]\u001b[A\n",
      "133it [01:38,  1.45it/s]\u001b[A\n",
      "134it [01:39,  1.45it/s]\u001b[A\n",
      "135it [01:40,  1.45it/s]\u001b[A\n",
      "136it [01:40,  1.45it/s]\u001b[A\n",
      "137it [01:41,  1.45it/s]\u001b[A\n",
      "138it [01:42,  1.44it/s]\u001b[A\n",
      "139it [01:43,  1.44it/s]\u001b[A\n",
      "140it [01:43,  1.44it/s]\u001b[A\n",
      "141it [01:44,  1.44it/s]\u001b[A\n",
      "142it [01:45,  1.44it/s]\u001b[A\n",
      "143it [01:45,  1.44it/s]\u001b[A\n",
      "144it [01:46,  1.44it/s]\u001b[A\n",
      "145it [01:47,  1.45it/s]\u001b[A\n",
      "146it [01:47,  1.45it/s]\u001b[A\n",
      "147it [01:48,  1.45it/s]\u001b[A\n",
      "148it [01:49,  1.45it/s]\u001b[A\n",
      "149it [01:49,  1.44it/s]\u001b[A\n",
      "150it [01:50,  1.45it/s]\u001b[A\n",
      "151it [01:51,  1.45it/s]\u001b[A\n",
      "152it [01:52,  1.45it/s]\u001b[A\n",
      "153it [01:52,  1.45it/s]\u001b[A\n",
      "154it [01:53,  1.45it/s]\u001b[A\n",
      "155it [01:54,  1.44it/s]\u001b[A\n",
      "156it [01:54,  1.44it/s]\u001b[A\n",
      "157it [01:55,  1.44it/s]\u001b[A\n",
      "158it [01:56,  1.45it/s]\u001b[A\n",
      "159it [01:56,  1.45it/s]\u001b[A\n",
      "160it [01:57,  1.45it/s]\u001b[A\n",
      "161it [01:58,  1.45it/s]\u001b[A\n",
      "162it [01:58,  1.45it/s]\u001b[A\n",
      "163it [01:59,  1.44it/s]\u001b[A\n",
      "164it [02:00,  1.44it/s]\u001b[A\n",
      "165it [02:01,  1.44it/s]\u001b[A\n",
      "166it [02:01,  1.44it/s]\u001b[A\n",
      "167it [02:02,  1.45it/s]\u001b[A\n",
      "168it [02:03,  1.45it/s]\u001b[A\n",
      "169it [02:03,  1.44it/s]\u001b[A\n",
      "170it [02:04,  1.44it/s]\u001b[A\n",
      "171it [02:05,  1.44it/s]\u001b[A\n",
      "172it [02:05,  1.44it/s]\u001b[A\n",
      "173it [02:06,  1.44it/s]\u001b[A\n",
      "174it [02:07,  1.44it/s]\u001b[A\n",
      "175it [02:07,  1.44it/s]\u001b[A\n",
      "176it [02:08,  1.44it/s]\u001b[A\n",
      "177it [02:09,  1.45it/s]\u001b[A\n",
      "178it [02:10,  1.44it/s]\u001b[A\n",
      "179it [02:10,  1.45it/s]\u001b[A\n",
      "180it [02:11,  1.44it/s]\u001b[A\n",
      "181it [02:12,  1.44it/s]\u001b[A\n",
      "182it [02:12,  1.44it/s]\u001b[A\n",
      "183it [02:13,  1.44it/s]\u001b[A\n",
      "184it [02:14,  1.44it/s]\u001b[A\n",
      "185it [02:14,  1.44it/s]\u001b[A\n",
      "186it [02:15,  1.45it/s]\u001b[A\n",
      "187it [02:16,  1.45it/s]\u001b[A\n",
      "188it [02:16,  1.45it/s]\u001b[A\n",
      "189it [02:17,  1.45it/s]\u001b[A\n",
      "190it [02:18,  1.44it/s]\u001b[A\n",
      "191it [02:19,  1.44it/s]\u001b[A\n",
      "192it [02:19,  1.44it/s]\u001b[A\n",
      "193it [02:20,  1.44it/s]\u001b[A\n",
      "194it [02:21,  1.45it/s]\u001b[A\n",
      "195it [02:21,  1.45it/s]\u001b[A\n",
      "196it [02:22,  1.45it/s]\u001b[A\n",
      "197it [02:23,  1.45it/s]\u001b[A\n",
      "198it [02:23,  1.45it/s]\u001b[A\n",
      "199it [02:24,  1.45it/s]\u001b[A\n",
      "200it [02:25,  1.44it/s]\u001b[A\n",
      "201it [02:25,  1.44it/s]\u001b[A\n",
      "202it [02:26,  1.45it/s]\u001b[A\n",
      "203it [02:27,  1.45it/s]\u001b[A\n",
      "204it [02:28,  1.45it/s]\u001b[A\n",
      "205it [02:28,  1.44it/s]\u001b[A\n",
      "206it [02:29,  1.44it/s]\u001b[A\n",
      "207it [02:30,  1.44it/s]\u001b[A\n",
      "208it [02:30,  1.44it/s]\u001b[A\n",
      "209it [02:31,  1.45it/s]\u001b[A\n",
      "210it [02:32,  1.45it/s]\u001b[A\n",
      "211it [02:32,  1.44it/s]\u001b[A\n",
      "212it [02:33,  1.45it/s]\u001b[A\n",
      "213it [02:34,  1.45it/s]\u001b[A\n",
      "214it [02:34,  1.45it/s]\u001b[A\n",
      "215it [02:35,  1.44it/s]\u001b[A\n",
      "216it [02:36,  1.44it/s]\u001b[A\n",
      "217it [02:37,  1.44it/s]\u001b[A\n",
      "218it [02:37,  1.45it/s]\u001b[A\n",
      "219it [02:38,  1.45it/s]\u001b[A\n",
      "220it [02:39,  1.45it/s]\u001b[A\n",
      "221it [02:39,  1.45it/s]\u001b[A\n",
      "222it [02:40,  1.45it/s]\u001b[A\n",
      "223it [02:41,  1.45it/s]\u001b[A\n",
      "224it [02:41,  1.45it/s]\u001b[A\n",
      "225it [02:42,  1.44it/s]\u001b[A\n",
      "226it [02:43,  1.44it/s]\u001b[A\n",
      "227it [02:43,  1.44it/s]\u001b[A\n",
      "228it [02:44,  1.44it/s]\u001b[A\n",
      "229it [02:45,  1.44it/s]\u001b[A\n",
      "230it [02:46,  1.44it/s]\u001b[A\n",
      "231it [02:46,  1.45it/s]\u001b[A\n",
      "232it [02:47,  1.45it/s]\u001b[A\n",
      "233it [02:48,  1.45it/s]\u001b[A\n",
      "234it [02:48,  1.45it/s]\u001b[A\n",
      "235it [02:49,  1.45it/s]\u001b[A\n",
      "236it [02:50,  1.45it/s]\u001b[A\n",
      "237it [02:50,  1.45it/s]\u001b[A\n",
      "238it [02:51,  1.45it/s]\u001b[A\n",
      "239it [02:52,  1.45it/s]\u001b[A\n",
      "240it [02:52,  1.45it/s]\u001b[A\n",
      "241it [02:53,  1.45it/s]\u001b[A\n",
      "242it [02:54,  1.45it/s]\u001b[A\n",
      "243it [02:55,  1.45it/s]\u001b[A\n",
      "244it [02:55,  1.45it/s]\u001b[A\n",
      "245it [02:56,  1.45it/s]\u001b[A\n",
      "246it [02:57,  1.44it/s]\u001b[A\n",
      "247it [02:57,  1.45it/s]\u001b[A\n",
      "248it [02:58,  1.45it/s]\u001b[A\n",
      "249it [02:59,  1.45it/s]\u001b[A\n",
      "250it [02:59,  1.45it/s]\u001b[A\n",
      "251it [03:00,  1.45it/s]\u001b[A\n",
      "252it [03:01,  1.45it/s]\u001b[A\n",
      "253it [03:01,  1.45it/s]\u001b[A\n",
      "254it [03:02,  1.45it/s]\u001b[A\n",
      "255it [03:03,  1.45it/s]\u001b[A\n",
      "256it [03:03,  1.45it/s]\u001b[A\n",
      "257it [03:04,  1.45it/s]\u001b[A\n",
      "258it [03:05,  1.45it/s]\u001b[A\n",
      "259it [03:06,  1.45it/s]\u001b[A\n",
      "260it [03:06,  1.45it/s]\u001b[A\n",
      "261it [03:07,  1.45it/s]\u001b[A\n",
      "262it [03:08,  1.45it/s]\u001b[A\n",
      "263it [03:08,  1.45it/s]\u001b[A\n",
      "264it [03:09,  1.45it/s]\u001b[A\n",
      "265it [03:10,  1.45it/s]\u001b[A\n",
      "266it [03:10,  1.45it/s]\u001b[A\n",
      "267it [03:11,  1.45it/s]\u001b[A\n",
      "268it [03:12,  1.45it/s]\u001b[A\n",
      "269it [03:12,  1.45it/s]\u001b[A\n",
      "270it [03:13,  1.45it/s]\u001b[A\n",
      "271it [03:14,  1.45it/s]\u001b[A\n",
      "272it [03:15,  1.45it/s]\u001b[A\n",
      "273it [03:15,  1.45it/s]\u001b[A\n",
      "274it [03:16,  1.45it/s]\u001b[A\n",
      "275it [03:17,  1.45it/s]\u001b[A\n",
      "276it [03:17,  1.45it/s]\u001b[A\n",
      "277it [03:18,  1.45it/s]\u001b[A\n",
      "278it [03:19,  1.45it/s]\u001b[A\n",
      "279it [03:19,  1.45it/s]\u001b[A\n",
      "280it [03:20,  1.45it/s]\u001b[A\n",
      "281it [03:21,  1.45it/s]\u001b[A\n",
      "282it [03:21,  1.45it/s]\u001b[A\n",
      "283it [03:22,  1.45it/s]\u001b[A\n",
      "284it [03:23,  1.45it/s]\u001b[A\n",
      "285it [03:24,  1.45it/s]\u001b[A\n",
      "286it [03:24,  1.45it/s]\u001b[A\n",
      "287it [03:25,  1.45it/s]\u001b[A\n",
      "288it [03:26,  1.45it/s]\u001b[A\n",
      "289it [03:26,  1.45it/s]\u001b[A\n",
      "290it [03:27,  1.45it/s]\u001b[A\n",
      "291it [03:28,  1.45it/s]\u001b[A\n",
      "292it [03:28,  1.45it/s]\u001b[A\n",
      "293it [03:29,  1.45it/s]\u001b[A\n",
      "294it [03:30,  1.45it/s]\u001b[A\n",
      "295it [03:30,  1.45it/s]\u001b[A\n",
      "296it [03:31,  1.45it/s]\u001b[A\n",
      "297it [03:32,  1.45it/s]\u001b[A\n",
      "298it [03:33,  1.45it/s]\u001b[A\n",
      "299it [03:33,  1.45it/s]\u001b[A\n",
      "300it [03:34,  1.45it/s]\u001b[A\n",
      "301it [03:35,  1.45it/s]\u001b[A\n",
      "302it [03:35,  1.45it/s]\u001b[A\n",
      "303it [03:36,  1.45it/s]\u001b[A\n",
      "304it [03:37,  1.45it/s]\u001b[A\n",
      "305it [03:37,  1.45it/s]\u001b[A\n",
      "306it [03:38,  1.45it/s]\u001b[A\n",
      "307it [03:39,  1.45it/s]\u001b[A\n",
      "308it [03:39,  1.45it/s]\u001b[A\n",
      "309it [03:40,  1.45it/s]\u001b[A\n",
      "310it [03:41,  1.45it/s]\u001b[A\n",
      "311it [03:41,  1.45it/s]\u001b[A\n",
      "312it [03:42,  1.45it/s]\u001b[A\n",
      "313it [03:43,  1.45it/s]\u001b[A\n",
      "314it [03:44,  1.45it/s]\u001b[A\n",
      "315it [03:44,  1.45it/s]\u001b[A\n",
      "316it [03:45,  1.45it/s]\u001b[A\n",
      "317it [03:46,  1.45it/s]\u001b[A\n",
      "318it [03:46,  1.45it/s]\u001b[A\n",
      "319it [03:47,  1.45it/s]\u001b[A\n",
      "320it [03:48,  1.45it/s]\u001b[A\n",
      "321it [03:48,  1.45it/s]\u001b[A\n",
      "322it [03:49,  1.45it/s]\u001b[A\n",
      "323it [03:50,  1.45it/s]\u001b[A\n",
      "324it [03:50,  1.45it/s]\u001b[A\n",
      "325it [03:51,  1.45it/s]\u001b[A\n",
      "326it [03:52,  1.45it/s]\u001b[A\n",
      "327it [03:53,  1.45it/s]\u001b[A\n",
      "328it [03:53,  1.45it/s]\u001b[A\n",
      "329it [03:54,  1.45it/s]\u001b[A\n",
      "330it [03:55,  1.45it/s]\u001b[A\n",
      "331it [03:55,  1.45it/s]\u001b[A\n",
      "332it [03:56,  1.45it/s]\u001b[A\n",
      "333it [03:57,  1.45it/s]\u001b[A\n",
      "334it [03:57,  1.45it/s]\u001b[A\n",
      "335it [03:58,  1.45it/s]\u001b[A\n",
      "336it [03:59,  1.45it/s]\u001b[A\n",
      "337it [03:59,  1.45it/s]\u001b[A\n",
      "338it [04:00,  1.45it/s]\u001b[A\n",
      "339it [04:01,  1.45it/s]\u001b[A\n",
      "340it [04:02,  1.45it/s]\u001b[A\n",
      "341it [04:02,  1.45it/s]\u001b[A\n",
      "342it [04:03,  1.45it/s]\u001b[A\n",
      "343it [04:04,  1.45it/s]\u001b[A\n",
      "344it [04:04,  1.45it/s]\u001b[A\n",
      "345it [04:05,  1.45it/s]\u001b[A\n",
      "346it [04:06,  1.45it/s]\u001b[A\n",
      "347it [04:06,  1.45it/s]\u001b[A\n",
      "348it [04:07,  1.45it/s]\u001b[A\n",
      "349it [04:08,  1.45it/s]\u001b[A\n",
      "350it [04:08,  1.45it/s]\u001b[A\n",
      "351it [04:09,  1.45it/s]\u001b[A\n",
      "352it [04:10,  1.45it/s]\u001b[A\n",
      "353it [04:10,  1.45it/s]\u001b[A\n",
      "354it [04:11,  1.45it/s]\u001b[A\n",
      "355it [04:12,  1.45it/s]\u001b[A\n",
      "356it [04:13,  1.45it/s]\u001b[A\n",
      "357it [04:13,  1.45it/s]\u001b[A\n",
      "358it [04:14,  1.45it/s]\u001b[A\n",
      "359it [04:15,  1.45it/s]\u001b[A\n",
      "360it [04:15,  1.45it/s]\u001b[A\n",
      "361it [04:16,  1.45it/s]\u001b[A\n",
      "362it [04:17,  1.45it/s]\u001b[A\n",
      "363it [04:17,  1.45it/s]\u001b[A\n",
      "364it [04:18,  1.45it/s]\u001b[A\n",
      "365it [04:19,  1.45it/s]\u001b[A\n",
      "366it [04:19,  1.45it/s]\u001b[A\n",
      "367it [04:20,  1.45it/s]\u001b[A\n",
      "368it [04:21,  1.45it/s]\u001b[A\n",
      "369it [04:22,  1.45it/s]\u001b[A\n",
      "370it [04:22,  1.45it/s]\u001b[A\n",
      "371it [04:23,  1.45it/s]\u001b[A\n",
      "372it [04:24,  1.45it/s]\u001b[A\n",
      "373it [04:24,  1.45it/s]\u001b[A\n",
      "374it [04:25,  1.45it/s]\u001b[A\n",
      "375it [04:26,  1.45it/s]\u001b[A\n",
      "376it [04:26,  1.45it/s]\u001b[A\n",
      "377it [04:27,  1.45it/s]\u001b[A\n",
      "378it [04:28,  1.45it/s]\u001b[A\n",
      "379it [04:28,  1.45it/s]\u001b[A\n",
      "380it [04:29,  1.45it/s]\u001b[A\n",
      "381it [04:30,  1.45it/s]\u001b[A\n",
      "382it [04:31,  1.45it/s]\u001b[A\n",
      "383it [04:31,  1.45it/s]\u001b[A\n",
      "384it [04:32,  1.45it/s]\u001b[A\n",
      "385it [04:33,  1.45it/s]\u001b[A\n",
      "386it [04:33,  1.45it/s]\u001b[A\n",
      "387it [04:34,  1.45it/s]\u001b[A\n",
      "388it [04:35,  1.45it/s]\u001b[A\n",
      "389it [04:35,  1.45it/s]\u001b[A\n",
      "390it [04:36,  1.45it/s]\u001b[A\n",
      "391it [04:37,  1.45it/s]\u001b[A\n",
      "392it [04:37,  1.45it/s]\u001b[A\n",
      "393it [04:38,  1.45it/s]\u001b[A\n",
      "394it [04:39,  1.45it/s]\u001b[A\n",
      "395it [04:39,  1.45it/s]\u001b[A\n",
      "396it [04:40,  1.45it/s]\u001b[A\n",
      "397it [04:41,  1.45it/s]\u001b[A\n",
      "398it [04:42,  1.45it/s]\u001b[A\n",
      "399it [04:42,  1.45it/s]\u001b[A\n",
      "400it [04:43,  1.45it/s]\u001b[A\n",
      "401it [04:44,  1.45it/s]\u001b[A\n",
      "402it [04:44,  1.45it/s]\u001b[A\n",
      "403it [04:45,  1.45it/s]\u001b[A\n",
      "404it [04:46,  1.45it/s]\u001b[A\n",
      "405it [04:46,  1.45it/s]\u001b[A\n",
      "406it [04:47,  1.45it/s]\u001b[A\n",
      "407it [04:48,  1.45it/s]\u001b[A\n",
      "408it [04:48,  1.45it/s]\u001b[A\n",
      "409it [04:49,  1.45it/s]\u001b[A\n",
      "410it [04:50,  1.45it/s]\u001b[A\n",
      "411it [04:51,  1.45it/s]\u001b[A\n",
      "412it [04:51,  1.45it/s]\u001b[A\n",
      "413it [04:52,  1.45it/s]\u001b[A\n",
      "414it [04:53,  1.45it/s]\u001b[A\n",
      "415it [04:53,  1.45it/s]\u001b[A\n",
      "416it [04:54,  1.45it/s]\u001b[A\n",
      "417it [04:55,  1.45it/s]\u001b[A\n",
      "418it [04:55,  1.45it/s]\u001b[A\n",
      "419it [04:56,  1.45it/s]\u001b[A\n",
      "420it [04:57,  1.45it/s]\u001b[A\n",
      "421it [04:57,  1.45it/s]\u001b[A\n",
      "422it [04:58,  1.45it/s]\u001b[A\n",
      "423it [04:59,  1.45it/s]\u001b[A\n",
      "424it [04:59,  1.45it/s]\u001b[A\n",
      "425it [05:00,  1.45it/s]\u001b[A\n",
      "426it [05:01,  1.45it/s]\u001b[A\n",
      "427it [05:02,  1.45it/s]\u001b[A\n",
      "428it [05:02,  1.45it/s]\u001b[A\n",
      "429it [05:03,  1.45it/s]\u001b[A\n",
      "430it [05:04,  1.45it/s]\u001b[A\n",
      "431it [05:04,  1.45it/s]\u001b[A\n",
      "432it [05:05,  1.45it/s]\u001b[A\n",
      "433it [05:06,  1.45it/s]\u001b[A\n",
      "434it [05:06,  1.45it/s]\u001b[A\n",
      "435it [05:07,  1.45it/s]\u001b[A\n",
      "436it [05:08,  1.45it/s]\u001b[A\n",
      "437it [05:08,  1.45it/s]\u001b[A\n",
      "438it [05:09,  1.45it/s]\u001b[A\n",
      "439it [05:10,  1.45it/s]\u001b[A\n",
      "440it [05:11,  1.45it/s]\u001b[A\n",
      "441it [05:11,  1.45it/s]\u001b[A\n",
      "442it [05:12,  1.45it/s]\u001b[A\n",
      "443it [05:13,  1.45it/s]\u001b[A\n",
      "444it [05:13,  1.45it/s]\u001b[A\n",
      "445it [05:14,  1.45it/s]\u001b[A\n",
      "446it [05:15,  1.45it/s]\u001b[A\n",
      "447it [05:15,  1.45it/s]\u001b[A\n",
      "448it [05:16,  1.45it/s]\u001b[A\n",
      "449it [05:17,  1.45it/s]\u001b[A\n",
      "450it [05:17,  1.45it/s]\u001b[A\n",
      "451it [05:18,  1.45it/s]\u001b[A\n",
      "452it [05:19,  1.45it/s]\u001b[A\n",
      "453it [05:19,  1.45it/s]\u001b[A\n",
      "454it [05:20,  1.45it/s]\u001b[A\n",
      "455it [05:21,  1.45it/s]\u001b[A\n",
      "456it [05:22,  1.45it/s]\u001b[A\n",
      "457it [05:22,  1.45it/s]\u001b[A\n",
      "458it [05:23,  1.45it/s]\u001b[A\n",
      "459it [05:24,  1.45it/s]\u001b[A\n",
      "460it [05:24,  1.45it/s]\u001b[A\n",
      "461it [05:25,  1.45it/s]\u001b[A\n",
      "462it [05:26,  1.45it/s]\u001b[A\n",
      "463it [05:26,  1.45it/s]\u001b[A\n",
      "464it [05:27,  1.45it/s]\u001b[A\n",
      "465it [05:28,  1.45it/s]\u001b[A\n",
      "466it [05:28,  1.45it/s]\u001b[A\n",
      "467it [05:29,  1.45it/s]\u001b[A\n",
      "468it [05:30,  1.45it/s]\u001b[A\n",
      "469it [05:31,  1.45it/s]\u001b[A\n",
      "470it [05:31,  1.45it/s]\u001b[A\n",
      "471it [05:32,  1.45it/s]\u001b[A\n",
      "472it [05:33,  1.45it/s]\u001b[A\n",
      "473it [05:33,  1.45it/s]\u001b[A\n",
      "474it [05:34,  1.45it/s]\u001b[A\n",
      "475it [05:35,  1.45it/s]\u001b[A\n",
      "476it [05:35,  1.45it/s]\u001b[A\n",
      "477it [05:36,  1.45it/s]\u001b[A\n",
      "478it [05:37,  1.45it/s]\u001b[A\n",
      "479it [05:37,  1.45it/s]\u001b[A\n",
      "480it [05:38,  1.45it/s]\u001b[A\n",
      "481it [05:39,  1.45it/s]\u001b[A\n",
      "482it [05:39,  1.45it/s]\u001b[A\n",
      "483it [05:40,  1.45it/s]\u001b[A\n",
      "484it [05:41,  1.45it/s]\u001b[A\n",
      "485it [05:42,  1.45it/s]\u001b[A\n",
      "486it [05:42,  1.45it/s]\u001b[A\n",
      "487it [05:43,  1.45it/s]\u001b[A\n",
      "488it [05:44,  1.45it/s]\u001b[A\n",
      "489it [05:44,  1.45it/s]\u001b[A\n",
      "490it [05:45,  1.45it/s]\u001b[A\n",
      "491it [05:46,  1.45it/s]\u001b[A\n",
      "492it [05:46,  1.45it/s]\u001b[A\n",
      "493it [05:47,  1.45it/s]\u001b[A\n",
      "494it [05:48,  1.45it/s]\u001b[A\n",
      "495it [05:48,  1.45it/s]\u001b[A\n",
      "496it [05:49,  1.45it/s]\u001b[A\n",
      "497it [05:50,  1.45it/s]\u001b[A\n",
      "498it [05:51,  1.45it/s]\u001b[A\n",
      "499it [05:51,  1.45it/s]\u001b[A\n",
      "500it [05:52,  1.45it/s]\u001b[A\n",
      "501it [05:53,  1.45it/s]\u001b[A\n",
      "502it [05:53,  1.45it/s]\u001b[A\n",
      "503it [05:54,  1.45it/s]\u001b[A\n",
      "504it [05:55,  1.45it/s]\u001b[A\n",
      "505it [05:55,  1.45it/s]\u001b[A\n",
      "506it [05:56,  1.45it/s]\u001b[A\n",
      "507it [05:57,  1.45it/s]\u001b[A\n",
      "508it [05:57,  1.45it/s]\u001b[A\n",
      "509it [05:58,  1.45it/s]\u001b[A\n",
      "510it [05:59,  1.45it/s]\u001b[A\n",
      "511it [05:59,  1.45it/s]\u001b[A\n",
      "512it [06:00,  1.45it/s]\u001b[A\n",
      "513it [06:01,  1.45it/s]\u001b[A\n",
      "514it [06:02,  1.45it/s]\u001b[A\n",
      "515it [06:02,  1.45it/s]\u001b[A\n",
      "516it [06:03,  1.45it/s]\u001b[A\n",
      "517it [06:04,  1.45it/s]\u001b[A\n",
      "518it [06:04,  1.45it/s]\u001b[A\n",
      "519it [06:05,  1.45it/s]\u001b[A\n",
      "520it [06:06,  1.45it/s]\u001b[A\n",
      "521it [06:06,  1.45it/s]\u001b[A\n",
      "522it [06:07,  1.45it/s]\u001b[A\n",
      "523it [06:08,  1.45it/s]\u001b[A\n",
      "524it [06:08,  1.45it/s]\u001b[A\n",
      "525it [06:09,  1.45it/s]\u001b[A\n",
      "526it [06:10,  1.45it/s]\u001b[A\n",
      "527it [06:10,  1.45it/s]\u001b[A\n",
      "528it [06:11,  1.45it/s]\u001b[A\n",
      "529it [06:12,  1.45it/s]\u001b[A\n",
      "530it [06:13,  1.45it/s]\u001b[A\n",
      "531it [06:13,  1.45it/s]\u001b[A\n",
      "532it [06:14,  1.45it/s]\u001b[A\n",
      "533it [06:15,  1.45it/s]\u001b[A\n",
      "534it [06:15,  1.45it/s]\u001b[A\n",
      "535it [06:16,  1.45it/s]\u001b[A\n",
      "536it [06:17,  1.45it/s]\u001b[A\n",
      "537it [06:17,  1.45it/s]\u001b[A\n",
      "538it [06:18,  1.45it/s]\u001b[A\n",
      "539it [06:19,  1.45it/s]\u001b[A\n",
      "540it [06:19,  1.45it/s]\u001b[A\n",
      "541it [06:20,  1.45it/s]\u001b[A\n",
      "542it [06:21,  1.45it/s]\u001b[A\n",
      "543it [06:21,  1.45it/s]\u001b[A\n",
      "544it [06:22,  1.45it/s]\u001b[A\n",
      "545it [06:23,  1.45it/s]\u001b[A\n",
      "546it [06:24,  1.45it/s]\u001b[A\n",
      "547it [06:24,  1.45it/s]\u001b[A\n",
      "548it [06:25,  1.45it/s]\u001b[A\n",
      "549it [06:26,  1.45it/s]\u001b[A\n",
      "550it [06:26,  1.45it/s]\u001b[A\n",
      "551it [06:27,  1.45it/s]\u001b[A\n",
      "552it [06:28,  1.45it/s]\u001b[A\n",
      "553it [06:28,  1.45it/s]\u001b[A\n",
      "554it [06:29,  1.45it/s]\u001b[A\n",
      "555it [06:30,  1.45it/s]\u001b[A\n",
      "556it [06:30,  1.45it/s]\u001b[A\n",
      "557it [06:31,  1.45it/s]\u001b[A\n",
      "558it [06:32,  1.45it/s]\u001b[A\n",
      "559it [06:33,  1.45it/s]\u001b[A\n",
      "560it [06:33,  1.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "561it [06:34,  1.45it/s]\u001b[A\n",
      "562it [06:35,  1.45it/s]\u001b[A\n",
      "563it [06:35,  1.45it/s]\u001b[A\n",
      "564it [06:36,  1.45it/s]\u001b[A\n",
      "565it [06:37,  1.45it/s]\u001b[A\n",
      "566it [06:37,  1.45it/s]\u001b[A\n",
      "567it [06:38,  1.45it/s]\u001b[A\n",
      "568it [06:39,  1.45it/s]\u001b[A\n",
      "569it [06:39,  1.45it/s]\u001b[A\n",
      "570it [06:40,  1.45it/s]\u001b[A\n",
      "571it [06:41,  1.45it/s]\u001b[A\n",
      "572it [06:41,  1.45it/s]\u001b[A\n",
      "573it [06:42,  1.45it/s]\u001b[A\n",
      "574it [06:43,  1.45it/s]\u001b[A\n",
      "575it [06:44,  1.45it/s]\u001b[A\n",
      "576it [06:44,  1.45it/s]\u001b[A\n",
      "577it [06:45,  1.45it/s]\u001b[A\n",
      "578it [06:46,  1.45it/s]\u001b[A\n",
      "579it [06:46,  1.45it/s]\u001b[A\n",
      "580it [06:47,  1.45it/s]\u001b[A\n",
      "581it [06:48,  1.45it/s]\u001b[A\n",
      "582it [06:48,  1.45it/s]\u001b[A\n",
      "583it [06:49,  1.45it/s]\u001b[A\n",
      "584it [06:50,  1.45it/s]\u001b[A\n",
      "585it [06:50,  1.45it/s]\u001b[A\n",
      "586it [06:51,  1.45it/s]\u001b[A\n",
      "587it [06:52,  1.45it/s]\u001b[A\n",
      "588it [06:53,  1.45it/s]\u001b[A\n",
      "589it [06:53,  1.45it/s]\u001b[A\n",
      "590it [06:54,  1.45it/s]\u001b[A\n",
      "591it [06:55,  1.45it/s]\u001b[A\n",
      "592it [06:55,  1.45it/s]\u001b[A\n",
      "593it [06:56,  1.45it/s]\u001b[A\n",
      "594it [06:57,  1.45it/s]\u001b[A\n",
      "595it [06:57,  1.45it/s]\u001b[A\n",
      "596it [06:58,  1.45it/s]\u001b[A\n",
      "597it [06:59,  1.45it/s]\u001b[A\n",
      "598it [06:59,  1.45it/s]\u001b[A\n",
      "599it [07:00,  1.45it/s]\u001b[A\n",
      "600it [07:01,  1.45it/s]\u001b[A\n",
      "601it [07:01,  1.45it/s]\u001b[A\n",
      "602it [07:02,  1.45it/s]\u001b[A\n",
      "603it [07:03,  1.45it/s]\u001b[A\n",
      "604it [07:04,  1.45it/s]\u001b[A\n",
      "605it [07:04,  1.45it/s]\u001b[A\n",
      "606it [07:05,  1.45it/s]\u001b[A\n",
      "607it [07:06,  1.45it/s]\u001b[A\n",
      "608it [07:06,  1.45it/s]\u001b[A\n",
      "609it [07:07,  1.45it/s]\u001b[A\n",
      "610it [07:08,  1.45it/s]\u001b[A\n",
      "611it [07:08,  1.45it/s]\u001b[A\n",
      "612it [07:09,  1.45it/s]\u001b[A\n",
      "613it [07:10,  1.45it/s]\u001b[A\n",
      "614it [07:10,  1.45it/s]\u001b[A\n",
      "615it [07:11,  1.45it/s]\u001b[A\n",
      "616it [07:12,  1.45it/s]\u001b[A\n",
      "617it [07:12,  1.45it/s]\u001b[A\n",
      "618it [07:13,  1.45it/s]\u001b[A\n",
      "619it [07:14,  1.45it/s]\u001b[A\n",
      "620it [07:15,  1.45it/s]\u001b[A\n",
      "621it [07:15,  1.45it/s]\u001b[A\n",
      "622it [07:16,  1.45it/s]\u001b[A\n",
      "623it [07:17,  1.45it/s]\u001b[A\n",
      "624it [07:17,  1.45it/s]\u001b[A\n",
      "625it [07:18,  1.45it/s]\u001b[A\n",
      "626it [07:19,  1.45it/s]\u001b[A\n",
      "627it [07:19,  1.45it/s]\u001b[A\n",
      "628it [07:20,  1.45it/s]\u001b[A\n",
      "629it [07:21,  1.45it/s]\u001b[A\n",
      "630it [07:21,  1.45it/s]\u001b[A\n",
      "631it [07:22,  1.45it/s]\u001b[A\n",
      "632it [07:23,  1.45it/s]\u001b[A\n",
      "633it [07:24,  1.45it/s]\u001b[A\n",
      "634it [07:24,  1.45it/s]\u001b[A\n",
      "635it [07:25,  1.45it/s]\u001b[A\n",
      "636it [07:26,  1.45it/s]\u001b[A\n",
      "637it [07:26,  1.45it/s]\u001b[A\n",
      "638it [07:27,  1.45it/s]\u001b[A\n",
      "639it [07:28,  1.45it/s]\u001b[A\n",
      "640it [07:28,  1.45it/s]\u001b[A\n",
      "641it [07:29,  1.45it/s]\u001b[A\n",
      "642it [07:30,  1.45it/s]\u001b[A\n",
      "643it [07:30,  1.45it/s]\u001b[A\n",
      "644it [07:31,  1.45it/s]\u001b[A\n",
      "645it [07:32,  1.45it/s]\u001b[A\n",
      "646it [07:32,  1.45it/s]\u001b[A\n",
      "647it [07:33,  1.45it/s]\u001b[A\n",
      "648it [07:34,  1.45it/s]\u001b[A\n",
      "649it [07:35,  1.45it/s]\u001b[A\n",
      "650it [07:35,  1.45it/s]\u001b[A\n",
      "651it [07:36,  1.45it/s]\u001b[A\n",
      "652it [07:37,  1.45it/s]\u001b[A\n",
      "653it [07:37,  1.45it/s]\u001b[A\n",
      "654it [07:38,  1.45it/s]\u001b[A\n",
      "655it [07:39,  1.45it/s]\u001b[A\n",
      "656it [07:39,  1.45it/s]\u001b[A\n",
      "657it [07:40,  1.45it/s]\u001b[A\n",
      "658it [07:41,  1.45it/s]\u001b[A\n",
      "659it [07:41,  1.45it/s]\u001b[A\n",
      "660it [07:42,  1.45it/s]\u001b[A\n",
      "661it [07:43,  1.45it/s]\u001b[A\n",
      "662it [07:43,  1.45it/s]\u001b[A\n",
      "663it [07:44,  1.45it/s]\u001b[A\n",
      "664it [07:45,  1.45it/s]\u001b[A\n",
      "665it [07:46,  1.45it/s]\u001b[A\n",
      "666it [07:46,  1.45it/s]\u001b[A\n",
      "667it [07:47,  1.45it/s]\u001b[A\n",
      "668it [07:48,  1.45it/s]\u001b[A\n",
      "669it [07:48,  1.45it/s]\u001b[A\n",
      "670it [07:49,  1.45it/s]\u001b[A\n",
      "671it [07:50,  1.45it/s]\u001b[A\n",
      "672it [07:50,  1.45it/s]\u001b[A\n",
      "673it [07:51,  1.45it/s]\u001b[A\n",
      "674it [07:52,  1.45it/s]\u001b[A\n",
      "675it [07:52,  1.45it/s]\u001b[A\n",
      "676it [07:53,  1.45it/s]\u001b[A\n",
      "677it [07:54,  1.45it/s]\u001b[A\n",
      "678it [07:55,  1.45it/s]\u001b[A\n",
      "679it [07:55,  1.45it/s]\u001b[A\n",
      "680it [07:56,  1.45it/s]\u001b[A\n",
      "681it [07:57,  1.45it/s]\u001b[A\n",
      "682it [07:57,  1.45it/s]\u001b[A\n",
      "683it [07:58,  1.45it/s]\u001b[A\n",
      "684it [07:59,  1.45it/s]\u001b[A\n",
      "685it [07:59,  1.45it/s]\u001b[A\n",
      "686it [08:00,  1.45it/s]\u001b[A\n",
      "687it [08:01,  1.45it/s]\u001b[A\n",
      "688it [08:01,  1.45it/s]\u001b[A\n",
      "689it [08:02,  1.45it/s]\u001b[A\n",
      "690it [08:03,  1.45it/s]\u001b[A\n",
      "691it [08:03,  1.45it/s]\u001b[A\n",
      "692it [08:04,  1.45it/s]\u001b[A\n",
      "693it [08:05,  1.45it/s]\u001b[A\n",
      "694it [08:06,  1.45it/s]\u001b[A\n",
      "695it [08:06,  1.45it/s]\u001b[A\n",
      "696it [08:07,  1.45it/s]\u001b[A\n",
      "697it [08:08,  1.45it/s]\u001b[A\n",
      "698it [08:08,  1.45it/s]\u001b[A\n",
      "699it [08:09,  1.45it/s]\u001b[A\n",
      "700it [08:10,  1.45it/s]\u001b[A\n",
      "701it [08:10,  1.45it/s]\u001b[A\n",
      "702it [08:11,  1.45it/s]\u001b[A\n",
      "703it [08:12,  1.45it/s]\u001b[A\n",
      "704it [08:12,  1.45it/s]\u001b[A\n",
      "705it [08:13,  1.45it/s]\u001b[A\n",
      "706it [08:14,  1.45it/s]\u001b[A\n",
      "707it [08:14,  1.45it/s]\u001b[A\n",
      "708it [08:15,  1.45it/s]\u001b[A\n",
      "709it [08:16,  1.45it/s]\u001b[A\n",
      "710it [08:17,  1.45it/s]\u001b[A\n",
      "711it [08:17,  1.45it/s]\u001b[A\n",
      "712it [08:18,  1.45it/s]\u001b[A\n",
      "713it [08:19,  1.45it/s]\u001b[A\n",
      "714it [08:19,  1.45it/s]\u001b[A\n",
      "715it [08:20,  1.45it/s]\u001b[A\n",
      "716it [08:21,  1.45it/s]\u001b[A\n",
      "717it [08:21,  1.45it/s]\u001b[A\n",
      "718it [08:22,  1.45it/s]\u001b[A\n",
      "719it [08:23,  1.45it/s]\u001b[A\n",
      "720it [08:23,  1.45it/s]\u001b[A\n",
      "721it [08:24,  1.45it/s]\u001b[A\n",
      "722it [08:25,  1.45it/s]\u001b[A\n",
      "723it [08:26,  1.45it/s]\u001b[A\n",
      "724it [08:26,  1.45it/s]\u001b[A\n",
      "725it [08:27,  1.45it/s]\u001b[A\n",
      "726it [08:28,  1.45it/s]\u001b[A\n",
      "727it [08:28,  1.45it/s]\u001b[A\n",
      "728it [08:29,  1.45it/s]\u001b[A\n",
      "729it [08:30,  1.45it/s]\u001b[A\n",
      "730it [08:30,  1.45it/s]\u001b[A\n",
      "731it [08:31,  1.45it/s]\u001b[A\n",
      "732it [08:32,  1.45it/s]\u001b[A\n",
      "733it [08:32,  1.45it/s]\u001b[A\n",
      "734it [08:33,  1.45it/s]\u001b[A\n",
      "735it [08:34,  1.45it/s]\u001b[A\n",
      "736it [08:34,  1.45it/s]\u001b[A\n",
      "737it [08:35,  1.45it/s]\u001b[A\n",
      "738it [08:36,  1.45it/s]\u001b[A\n",
      "739it [08:37,  1.45it/s]\u001b[A\n",
      "740it [08:37,  1.45it/s]\u001b[A\n",
      "741it [08:38,  1.45it/s]\u001b[A\n",
      "742it [08:39,  1.45it/s]\u001b[A\n",
      "743it [08:39,  1.45it/s]\u001b[A\n",
      "744it [08:40,  1.45it/s]\u001b[A\n",
      "745it [08:41,  1.45it/s]\u001b[A\n",
      "746it [08:41,  1.45it/s]\u001b[A\n",
      "747it [08:42,  1.45it/s]\u001b[A\n",
      "748it [08:43,  1.45it/s]\u001b[A\n",
      "749it [08:43,  1.45it/s]\u001b[A\n",
      "750it [08:44,  1.45it/s]\u001b[A\n",
      "751it [08:45,  1.45it/s]\u001b[A\n",
      "752it [08:46,  1.45it/s]\u001b[A\n",
      "753it [08:46,  1.45it/s]\u001b[A\n",
      "754it [08:47,  1.45it/s]\u001b[A\n",
      "755it [08:48,  1.45it/s]\u001b[A\n",
      "756it [08:48,  1.45it/s]\u001b[A\n",
      "757it [08:49,  1.45it/s]\u001b[A\n",
      "758it [08:50,  1.45it/s]\u001b[A\n",
      "759it [08:50,  1.45it/s]\u001b[A\n",
      "760it [08:51,  1.45it/s]\u001b[A\n",
      "761it [08:52,  1.45it/s]\u001b[A\n",
      "762it [08:52,  1.45it/s]\u001b[A\n",
      "763it [08:53,  1.45it/s]\u001b[A\n",
      "764it [08:54,  1.45it/s]\u001b[A\n",
      "765it [08:54,  1.45it/s]\u001b[A\n",
      "766it [08:55,  1.45it/s]\u001b[A\n",
      "767it [08:56,  1.45it/s]\u001b[A\n",
      "768it [08:57,  1.45it/s]\u001b[A\n",
      "769it [08:57,  1.45it/s]\u001b[A\n",
      "770it [08:58,  1.45it/s]\u001b[A\n",
      "771it [08:59,  1.45it/s]\u001b[A\n",
      "772it [08:59,  1.45it/s]\u001b[A\n",
      "773it [09:00,  1.45it/s]\u001b[A\n",
      "774it [09:01,  1.45it/s]\u001b[A\n",
      "775it [09:01,  1.45it/s]\u001b[A\n",
      "776it [09:02,  1.45it/s]\u001b[A\n",
      "777it [09:03,  1.45it/s]\u001b[A\n",
      "778it [09:03,  1.45it/s]\u001b[A\n",
      "779it [09:04,  1.45it/s]\u001b[A\n",
      "780it [09:05,  1.45it/s]\u001b[A\n",
      "781it [09:05,  1.45it/s]\u001b[A\n",
      "782it [09:06,  1.45it/s]\u001b[A\n",
      "783it [09:07,  1.45it/s]\u001b[A\n",
      "784it [09:08,  1.45it/s]\u001b[A\n",
      "785it [09:08,  1.45it/s]\u001b[A\n",
      "786it [09:09,  1.45it/s]\u001b[A\n",
      "787it [09:10,  1.45it/s]\u001b[A\n",
      "788it [09:10,  1.45it/s]\u001b[A\n",
      "789it [09:11,  1.45it/s]\u001b[A\n",
      "790it [09:12,  1.45it/s]\u001b[A\n",
      "791it [09:12,  1.45it/s]\u001b[A\n",
      "792it [09:13,  1.45it/s]\u001b[A\n",
      "793it [09:14,  1.45it/s]\u001b[A\n",
      "794it [09:14,  1.45it/s]\u001b[A\n",
      "795it [09:15,  1.45it/s]\u001b[A\n",
      "796it [09:16,  1.45it/s]\u001b[A\n",
      "797it [09:17,  1.45it/s]\u001b[A\n",
      "798it [09:17,  1.45it/s]\u001b[A\n",
      "799it [09:18,  1.45it/s]\u001b[A\n",
      "800it [09:19,  1.45it/s]\u001b[A\n",
      "801it [09:19,  1.45it/s]\u001b[A\n",
      "802it [09:20,  1.45it/s]\u001b[A\n",
      "803it [09:21,  1.45it/s]\u001b[A\n",
      "804it [09:21,  1.45it/s]\u001b[A\n",
      "805it [09:22,  1.45it/s]\u001b[A\n",
      "806it [09:23,  1.45it/s]\u001b[A\n",
      "807it [09:23,  1.45it/s]\u001b[A\n",
      "808it [09:24,  1.45it/s]\u001b[A\n",
      "809it [09:25,  1.45it/s]\u001b[A\n",
      "810it [09:25,  1.45it/s]\u001b[A\n",
      "811it [09:26,  1.45it/s]\u001b[A\n",
      "812it [09:27,  1.45it/s]\u001b[A\n",
      "813it [09:28,  1.45it/s]\u001b[A\n",
      "814it [09:28,  1.45it/s]\u001b[A\n",
      "815it [09:29,  1.45it/s]\u001b[A\n",
      "816it [09:30,  1.45it/s]\u001b[A\n",
      "817it [09:30,  1.45it/s]\u001b[A\n",
      "818it [09:31,  1.45it/s]\u001b[A\n",
      "819it [09:32,  1.45it/s]\u001b[A\n",
      "820it [09:32,  1.45it/s]\u001b[A\n",
      "821it [09:33,  1.45it/s]\u001b[A\n",
      "822it [09:34,  1.45it/s]\u001b[A\n",
      "823it [09:34,  1.45it/s]\u001b[A\n",
      "824it [09:35,  1.45it/s]\u001b[A\n",
      "825it [09:36,  1.45it/s]\u001b[A\n",
      "826it [09:36,  1.45it/s]\u001b[A\n",
      "827it [09:37,  1.45it/s]\u001b[A\n",
      "828it [09:38,  1.45it/s]\u001b[A\n",
      "829it [09:39,  1.45it/s]\u001b[A\n",
      "830it [09:39,  1.45it/s]\u001b[A\n",
      "831it [09:40,  1.45it/s]\u001b[A\n",
      "832it [09:41,  1.45it/s]\u001b[A\n",
      "833it [09:41,  1.45it/s]\u001b[A\n",
      "834it [09:42,  1.45it/s]\u001b[A\n",
      "835it [09:43,  1.45it/s]\u001b[A\n",
      "836it [09:43,  1.45it/s]\u001b[A\n",
      "837it [09:44,  1.45it/s]\u001b[A\n",
      "838it [09:45,  1.45it/s]\u001b[A\n",
      "839it [09:45,  1.45it/s]\u001b[A\n",
      "840it [09:46,  1.45it/s]\u001b[A\n",
      "841it [09:47,  1.45it/s]\u001b[A\n",
      "842it [09:48,  1.45it/s]\u001b[A\n",
      "843it [09:48,  1.45it/s]\u001b[A\n",
      "844it [09:49,  1.45it/s]\u001b[A\n",
      "845it [09:50,  1.45it/s]\u001b[A\n",
      "846it [09:50,  1.45it/s]\u001b[A\n",
      "847it [09:51,  1.45it/s]\u001b[A\n",
      "848it [09:52,  1.45it/s]\u001b[A\n",
      "849it [09:52,  1.45it/s]\u001b[A\n",
      "850it [09:53,  1.45it/s]\u001b[A\n",
      "851it [09:54,  1.45it/s]\u001b[A\n",
      "852it [09:54,  1.45it/s]\u001b[A\n",
      "853it [09:55,  1.45it/s]\u001b[A\n",
      "854it [09:56,  1.45it/s]\u001b[A\n",
      "855it [09:56,  1.45it/s]\u001b[A\n",
      "856it [09:57,  1.45it/s]\u001b[A\n",
      "857it [09:58,  1.45it/s]\u001b[A\n",
      "858it [09:59,  1.45it/s]\u001b[A\n",
      "859it [09:59,  1.45it/s]\u001b[A\n",
      "860it [10:00,  1.45it/s]\u001b[A\n",
      "861it [10:01,  1.45it/s]\u001b[A\n",
      "862it [10:01,  1.45it/s]\u001b[A\n",
      "863it [10:02,  1.45it/s]\u001b[A\n",
      "864it [10:03,  1.45it/s]\u001b[A\n",
      "865it [10:03,  1.45it/s]\u001b[A\n",
      "866it [10:04,  1.45it/s]\u001b[A\n",
      "867it [10:05,  1.45it/s]\u001b[A\n",
      "868it [10:05,  1.45it/s]\u001b[A\n",
      "869it [10:06,  1.45it/s]\u001b[A\n",
      "870it [10:07,  1.45it/s]\u001b[A\n",
      "871it [10:07,  1.45it/s]\u001b[A\n",
      "872it [10:08,  1.45it/s]\u001b[A\n",
      "873it [10:09,  1.45it/s]\u001b[A\n",
      "874it [10:10,  1.45it/s]\u001b[A\n",
      "875it [10:10,  1.45it/s]\u001b[A\n",
      "876it [10:11,  1.45it/s]\u001b[A\n",
      "877it [10:12,  1.45it/s]\u001b[A\n",
      "878it [10:12,  1.45it/s]\u001b[A\n",
      "879it [10:13,  1.45it/s]\u001b[A\n",
      "880it [10:14,  1.45it/s]\u001b[A\n",
      "881it [10:14,  1.45it/s]\u001b[A\n",
      "882it [10:15,  1.45it/s]\u001b[A\n",
      "883it [10:16,  1.45it/s]\u001b[A\n",
      "884it [10:16,  1.45it/s]\u001b[A\n",
      "885it [10:17,  1.45it/s]\u001b[A\n",
      "886it [10:18,  1.45it/s]\u001b[A\n",
      "887it [10:19,  1.45it/s]\u001b[A\n",
      "888it [10:19,  1.45it/s]\u001b[A\n",
      "889it [10:20,  1.45it/s]\u001b[A\n",
      "890it [10:21,  1.45it/s]\u001b[A\n",
      "891it [10:21,  1.45it/s]\u001b[A\n",
      "892it [10:22,  1.45it/s]\u001b[A\n",
      "893it [10:23,  1.45it/s]\u001b[A\n",
      "894it [10:23,  1.45it/s]\u001b[A\n",
      "895it [10:24,  1.45it/s]\u001b[A\n",
      "896it [10:25,  1.45it/s]\u001b[A\n",
      "897it [10:25,  1.45it/s]\u001b[A\n",
      "898it [10:26,  1.45it/s]\u001b[A\n",
      "899it [10:27,  1.45it/s]\u001b[A\n",
      "900it [10:27,  1.45it/s]\u001b[A\n",
      "901it [10:28,  1.45it/s]\u001b[A\n",
      "902it [10:29,  1.45it/s]\u001b[A\n",
      "903it [10:30,  1.45it/s]\u001b[A\n",
      "904it [10:30,  1.45it/s]\u001b[A\n",
      "905it [10:31,  1.45it/s]\u001b[A\n",
      "906it [10:32,  1.45it/s]\u001b[A\n",
      "907it [10:32,  1.45it/s]\u001b[A\n",
      "908it [10:33,  1.45it/s]\u001b[A\n",
      "909it [10:34,  1.45it/s]\u001b[A\n",
      "910it [10:34,  1.45it/s]\u001b[A\n",
      "911it [10:35,  1.45it/s]\u001b[A\n",
      "912it [10:36,  1.45it/s]\u001b[A\n",
      "913it [10:36,  1.45it/s]\u001b[A\n",
      "914it [10:37,  1.45it/s]\u001b[A\n",
      "915it [10:38,  1.45it/s]\u001b[A\n",
      "916it [10:38,  1.45it/s]\u001b[A\n",
      "917it [10:39,  1.45it/s]\u001b[A\n",
      "918it [10:40,  1.45it/s]\u001b[A\n",
      "919it [10:41,  1.45it/s]\u001b[A\n",
      "920it [10:41,  1.45it/s]\u001b[A\n",
      "921it [10:42,  1.45it/s]\u001b[A\n",
      "922it [10:43,  1.45it/s]\u001b[A\n",
      "923it [10:43,  1.45it/s]\u001b[A\n",
      "924it [10:44,  1.45it/s]\u001b[A\n",
      "925it [10:45,  1.45it/s]\u001b[A\n",
      "926it [10:45,  1.45it/s]\u001b[A\n",
      "927it [10:46,  1.45it/s]\u001b[A\n",
      "928it [10:47,  1.45it/s]\u001b[A\n",
      "929it [10:47,  1.45it/s]\u001b[A\n",
      "930it [10:48,  1.45it/s]\u001b[A\n",
      "931it [10:49,  1.45it/s]\u001b[A\n",
      "932it [10:50,  1.45it/s]\u001b[A\n",
      "933it [10:50,  1.45it/s]\u001b[A\n",
      "934it [10:51,  1.45it/s]\u001b[A\n",
      "935it [10:52,  1.45it/s]\u001b[A\n",
      "936it [10:52,  1.45it/s]\u001b[A\n",
      "937it [10:53,  1.45it/s]\u001b[A\n",
      "938it [10:54,  1.45it/s]\u001b[A\n",
      "939it [10:54,  1.45it/s]\u001b[A\n",
      "940it [10:55,  1.45it/s]\u001b[A\n",
      "941it [10:56,  1.45it/s]\u001b[A\n",
      "942it [10:56,  1.45it/s]\u001b[A\n",
      "943it [10:57,  1.45it/s]\u001b[A\n",
      "944it [10:58,  1.45it/s]\u001b[A\n",
      "945it [10:58,  1.45it/s]\u001b[A\n",
      "946it [10:59,  1.45it/s]\u001b[A\n",
      "947it [11:00,  1.45it/s]\u001b[A\n",
      "948it [11:01,  1.45it/s]\u001b[A\n",
      "949it [11:01,  1.45it/s]\u001b[A\n",
      "950it [11:02,  1.45it/s]\u001b[A\n",
      "951it [11:03,  1.45it/s]\u001b[A\n",
      "952it [11:03,  1.45it/s]\u001b[A\n",
      "953it [11:04,  1.45it/s]\u001b[A\n",
      "954it [11:05,  1.45it/s]\u001b[A\n",
      "955it [11:05,  1.45it/s]\u001b[A\n",
      "956it [11:06,  1.45it/s]\u001b[A\n",
      "957it [11:07,  1.45it/s]\u001b[A\n",
      "958it [11:07,  1.45it/s]\u001b[A\n",
      "959it [11:08,  1.45it/s]\u001b[A\n",
      "960it [11:09,  1.45it/s]\u001b[A\n",
      "961it [11:09,  1.45it/s]\u001b[A\n",
      "962it [11:10,  1.45it/s]\u001b[A\n",
      "963it [11:11,  1.45it/s]\u001b[A\n",
      "964it [11:12,  1.45it/s]\u001b[A\n",
      "965it [11:12,  1.45it/s]\u001b[A\n",
      "966it [11:13,  1.45it/s]\u001b[A\n",
      "967it [11:14,  1.45it/s]\u001b[A\n",
      "968it [11:14,  1.45it/s]\u001b[A\n",
      "969it [11:15,  1.45it/s]\u001b[A\n",
      "970it [11:16,  1.45it/s]\u001b[A\n",
      "971it [11:16,  1.45it/s]\u001b[A\n",
      "972it [11:17,  1.45it/s]\u001b[A\n",
      "973it [11:18,  1.45it/s]\u001b[A\n",
      "974it [11:18,  1.45it/s]\u001b[A\n",
      "975it [11:19,  1.45it/s]\u001b[A\n",
      "976it [11:20,  1.45it/s]\u001b[A\n",
      "977it [11:21,  1.45it/s]\u001b[A\n",
      "978it [11:21,  1.45it/s]\u001b[A\n",
      "979it [11:22,  1.45it/s]\u001b[A\n",
      "980it [11:23,  1.45it/s]\u001b[A\n",
      "981it [11:23,  1.45it/s]\u001b[A\n",
      "982it [11:24,  1.45it/s]\u001b[A\n",
      "983it [11:25,  1.45it/s]\u001b[A\n",
      "984it [11:25,  1.45it/s]\u001b[A\n",
      "985it [11:26,  1.45it/s]\u001b[A\n",
      "986it [11:27,  1.45it/s]\u001b[A\n",
      "987it [11:27,  1.45it/s]\u001b[A\n",
      "988it [11:28,  1.45it/s]\u001b[A\n",
      "989it [11:29,  1.45it/s]\u001b[A\n",
      "990it [11:29,  1.45it/s]\u001b[A\n",
      "991it [11:30,  1.45it/s]\u001b[A\n",
      "992it [11:31,  1.45it/s]\u001b[A\n",
      "993it [11:32,  1.45it/s]\u001b[A\n",
      "994it [11:32,  1.45it/s]\u001b[A\n",
      "995it [11:33,  1.45it/s]\u001b[A\n",
      "996it [11:34,  1.45it/s]\u001b[A\n",
      "997it [11:34,  1.45it/s]\u001b[A\n",
      "998it [11:35,  1.45it/s]\u001b[A\n",
      "999it [11:36,  1.45it/s]\u001b[A\n",
      "1000it [11:36,  1.45it/s]\u001b[A\n",
      "1001it [11:37,  1.45it/s]\u001b[A\n",
      "1002it [11:38,  1.45it/s]\u001b[A\n",
      "1003it [11:38,  1.45it/s]\u001b[A\n",
      "1004it [11:39,  1.45it/s]\u001b[A\n",
      "1005it [11:40,  1.45it/s]\u001b[A\n",
      "1006it [11:40,  1.45it/s]\u001b[A\n",
      "1007it [11:41,  1.45it/s]\u001b[A\n",
      "1008it [11:42,  1.45it/s]\u001b[A\n",
      "1009it [11:43,  1.45it/s]\u001b[A\n",
      "1010it [11:43,  1.45it/s]\u001b[A\n",
      "1011it [11:44,  1.45it/s]\u001b[A\n",
      "1012it [11:45,  1.45it/s]\u001b[A\n",
      "1013it [11:45,  1.45it/s]\u001b[A\n",
      "1014it [11:46,  1.45it/s]\u001b[A\n",
      "1015it [11:47,  1.45it/s]\u001b[A\n",
      "1016it [11:47,  1.45it/s]\u001b[A\n",
      "1017it [11:48,  1.45it/s]\u001b[A\n",
      "1018it [11:49,  1.45it/s]\u001b[A\n",
      "1019it [11:49,  1.45it/s]\u001b[A\n",
      "1020it [11:50,  1.45it/s]\u001b[A\n",
      "1021it [11:51,  1.45it/s]\u001b[A\n",
      "1022it [11:52,  1.45it/s]\u001b[A\n",
      "1023it [11:52,  1.45it/s]\u001b[A\n",
      "1024it [11:53,  1.45it/s]\u001b[A\n",
      "1025it [11:54,  1.45it/s]\u001b[A\n",
      "1026it [11:54,  1.45it/s]\u001b[A\n",
      "1027it [11:55,  1.45it/s]\u001b[A\n",
      "1028it [11:56,  1.45it/s]\u001b[A\n",
      "1029it [11:56,  1.45it/s]\u001b[A\n",
      "1030it [11:57,  1.45it/s]\u001b[A\n",
      "1031it [11:58,  1.45it/s]\u001b[A\n",
      "1032it [11:58,  1.45it/s]\u001b[A\n",
      "1033it [11:59,  1.45it/s]\u001b[A\n",
      "1034it [12:00,  1.45it/s]\u001b[A\n",
      "1035it [12:00,  1.45it/s]\u001b[A\n",
      "1036it [12:01,  1.45it/s]\u001b[A\n",
      "1037it [12:02,  1.45it/s]\u001b[A\n",
      "1038it [12:03,  1.45it/s]\u001b[A\n",
      "1039it [12:03,  1.45it/s]\u001b[A\n",
      "1040it [12:04,  1.45it/s]\u001b[A\n",
      "1041it [12:05,  1.45it/s]\u001b[A\n",
      "1042it [12:05,  1.45it/s]\u001b[A\n",
      "1043it [12:06,  1.45it/s]\u001b[A\n",
      "1044it [12:07,  1.45it/s]\u001b[A\n",
      "1045it [12:07,  1.45it/s]\u001b[A\n",
      "1046it [12:08,  1.45it/s]\u001b[A\n",
      "1047it [12:09,  1.45it/s]\u001b[A\n",
      "1048it [12:09,  1.45it/s]\u001b[A\n",
      "1049it [12:10,  1.45it/s]\u001b[A\n",
      "1050it [12:11,  1.45it/s]\u001b[A\n",
      "1051it [12:11,  1.45it/s]\u001b[A\n",
      "1052it [12:12,  1.45it/s]\u001b[A\n",
      "1053it [12:13,  1.45it/s]\u001b[A\n",
      "1054it [12:14,  1.45it/s]\u001b[A\n",
      "1055it [12:14,  1.45it/s]\u001b[A\n",
      "1056it [12:15,  1.45it/s]\u001b[A\n",
      "1057it [12:16,  1.45it/s]\u001b[A\n",
      "1058it [12:16,  1.45it/s]\u001b[A\n",
      "1059it [12:17,  1.45it/s]\u001b[A\n",
      "1060it [12:18,  1.45it/s]\u001b[A\n",
      "1061it [12:18,  1.45it/s]\u001b[A\n",
      "1062it [12:19,  1.45it/s]\u001b[A\n",
      "1063it [12:20,  1.45it/s]\u001b[A\n",
      "1064it [12:20,  1.45it/s]\u001b[A\n",
      "1065it [12:21,  1.45it/s]\u001b[A\n",
      "1066it [12:22,  1.45it/s]\u001b[A\n",
      "1067it [12:23,  1.45it/s]\u001b[A\n",
      "1068it [12:23,  1.45it/s]\u001b[A\n",
      "1069it [12:24,  1.45it/s]\u001b[A\n",
      "1070it [12:25,  1.45it/s]\u001b[A\n",
      "1071it [12:25,  1.45it/s]\u001b[A\n",
      "1072it [12:26,  1.45it/s]\u001b[A\n",
      "1073it [12:27,  1.45it/s]\u001b[A\n",
      "1074it [12:27,  1.45it/s]\u001b[A\n",
      "1075it [12:28,  1.45it/s]\u001b[A\n",
      "1076it [12:29,  1.45it/s]\u001b[A\n",
      "1077it [12:29,  1.45it/s]\u001b[A\n",
      "1078it [12:30,  1.45it/s]\u001b[A\n",
      "1079it [12:31,  1.45it/s]\u001b[A\n",
      "1080it [12:31,  1.45it/s]\u001b[A\n",
      "1081it [12:32,  1.45it/s]\u001b[A\n",
      "1082it [12:33,  1.45it/s]\u001b[A\n",
      "1083it [12:34,  1.45it/s]\u001b[A\n",
      "1084it [12:34,  1.45it/s]\u001b[A\n",
      "1085it [12:35,  1.45it/s]\u001b[A\n",
      "1086it [12:36,  1.45it/s]\u001b[A\n",
      "1087it [12:36,  1.45it/s]\u001b[A\n",
      "1088it [12:37,  1.45it/s]\u001b[A\n",
      "1089it [12:38,  1.45it/s]\u001b[A\n",
      "1090it [12:38,  1.45it/s]\u001b[A\n",
      "1091it [12:39,  1.45it/s]\u001b[A\n",
      "1092it [12:40,  1.45it/s]\u001b[A\n",
      "1093it [12:40,  1.45it/s]\u001b[A\n",
      "1094it [12:41,  1.45it/s]\u001b[A\n",
      "1095it [12:42,  1.45it/s]\u001b[A\n",
      "1096it [12:42,  1.45it/s]\u001b[A\n",
      "1097it [12:43,  1.45it/s]\u001b[A\n",
      "1098it [12:44,  1.45it/s]\u001b[A\n",
      "1099it [12:45,  1.45it/s]\u001b[A\n",
      "1100it [12:45,  1.45it/s]\u001b[A\n",
      "1101it [12:46,  1.45it/s]\u001b[A\n",
      "1102it [12:47,  1.45it/s]\u001b[A\n",
      "1103it [12:47,  1.45it/s]\u001b[A\n",
      "1104it [12:48,  1.45it/s]\u001b[A\n",
      "1105it [12:49,  1.45it/s]\u001b[A\n",
      "1106it [12:49,  1.45it/s]\u001b[A\n",
      "1107it [12:50,  1.45it/s]\u001b[A\n",
      "1108it [12:51,  1.45it/s]\u001b[A\n",
      "1109it [12:51,  1.45it/s]\u001b[A\n",
      "1110it [12:52,  1.45it/s]\u001b[A\n",
      "1111it [12:53,  1.45it/s]\u001b[A\n",
      "1112it [12:54,  1.45it/s]\u001b[A\n",
      "1113it [12:54,  1.45it/s]\u001b[A\n",
      "1114it [12:55,  1.45it/s]\u001b[A\n",
      "1115it [12:56,  1.45it/s]\u001b[A\n",
      "1116it [12:56,  1.45it/s]\u001b[A\n",
      "1117it [12:57,  1.45it/s]\u001b[A\n",
      "1118it [12:58,  1.45it/s]\u001b[A\n",
      "1119it [12:58,  1.45it/s]\u001b[A\n",
      "1120it [12:59,  1.45it/s]\u001b[A\n",
      "1121it [13:00,  1.45it/s]\u001b[A\n",
      "1122it [13:00,  1.45it/s]\u001b[A\n",
      "1123it [13:01,  1.45it/s]\u001b[A\n",
      "1124it [13:02,  1.45it/s]\u001b[A\n",
      "1125it [13:02,  1.45it/s]\u001b[A\n",
      "1126it [13:03,  1.45it/s]\u001b[A\n",
      "1127it [13:04,  1.45it/s]\u001b[A\n",
      "1128it [13:05,  1.45it/s]\u001b[A\n",
      "1129it [13:05,  1.45it/s]\u001b[A\n",
      "1130it [13:06,  1.45it/s]\u001b[A\n",
      "1131it [13:07,  1.45it/s]\u001b[A\n",
      "1132it [13:07,  1.45it/s]\u001b[A\n",
      "1133it [13:08,  1.45it/s]\u001b[A\n",
      "1134it [13:09,  1.45it/s]\u001b[A\n",
      "1135it [13:09,  1.45it/s]\u001b[A\n",
      "1136it [13:10,  1.45it/s]\u001b[A\n",
      "1137it [13:11,  1.45it/s]\u001b[A\n",
      "1138it [13:11,  1.45it/s]\u001b[A\n",
      "1139it [13:12,  1.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1140it [13:13,  1.45it/s]\u001b[A\n",
      "1141it [13:13,  1.45it/s]\u001b[A\n",
      "1142it [13:14,  1.45it/s]\u001b[A\n",
      "1143it [13:15,  1.45it/s]\u001b[A\n",
      "1144it [13:16,  1.45it/s]\u001b[A\n",
      "1145it [13:16,  1.45it/s]\u001b[A\n",
      "1146it [13:17,  1.45it/s]\u001b[A\n",
      "1147it [13:18,  1.45it/s]\u001b[A\n",
      "1148it [13:18,  1.45it/s]\u001b[A\n",
      "1149it [13:19,  1.45it/s]\u001b[A\n",
      "1150it [13:20,  1.45it/s]\u001b[A\n",
      "1151it [13:20,  1.45it/s]\u001b[A\n",
      "1152it [13:21,  1.45it/s]\u001b[A\n",
      "1153it [13:22,  1.45it/s]\u001b[A\n",
      "1154it [13:22,  1.45it/s]\u001b[A\n",
      "1155it [13:23,  1.45it/s]\u001b[A\n",
      "1156it [13:24,  1.45it/s]\u001b[A\n",
      "1157it [13:24,  1.45it/s]\u001b[A\n",
      "1158it [13:25,  1.45it/s]\u001b[A\n",
      "1159it [13:26,  1.45it/s]\u001b[A\n",
      "1160it [13:27,  1.45it/s]\u001b[A\n",
      "1161it [13:27,  1.45it/s]\u001b[A\n",
      "1162it [13:28,  1.45it/s]\u001b[A\n",
      "1163it [13:29,  1.45it/s]\u001b[A\n",
      "1164it [13:29,  1.45it/s]\u001b[A\n",
      "1165it [13:30,  1.45it/s]\u001b[A\n",
      "1166it [13:31,  1.45it/s]\u001b[A\n",
      "1167it [13:31,  1.45it/s]\u001b[A\n",
      "1168it [13:32,  1.45it/s]\u001b[A\n",
      "1169it [13:33,  1.45it/s]\u001b[A\n",
      "1170it [13:33,  1.45it/s]\u001b[A\n",
      "1171it [13:34,  1.45it/s]\u001b[A\n",
      "1172it [13:35,  1.45it/s]\u001b[A\n",
      "1173it [13:36,  1.45it/s]\u001b[A\n",
      "1174it [13:36,  1.45it/s]\u001b[A\n",
      "1175it [13:37,  1.45it/s]\u001b[A\n",
      "1176it [13:38,  1.45it/s]\u001b[A\n",
      "1177it [13:38,  1.45it/s]\u001b[A\n",
      "1178it [13:39,  1.45it/s]\u001b[A\n",
      "1179it [13:40,  1.45it/s]\u001b[A\n",
      "1180it [13:40,  1.45it/s]\u001b[A\n",
      "1181it [13:41,  1.45it/s]\u001b[A\n",
      "1182it [13:42,  1.45it/s]\u001b[A\n",
      "1183it [13:42,  1.45it/s]\u001b[A\n",
      "1184it [13:43,  1.45it/s]\u001b[A\n",
      "1185it [13:44,  1.45it/s]\u001b[A\n",
      "1186it [13:44,  1.45it/s]\u001b[A\n",
      "1187it [13:45,  1.45it/s]\u001b[A\n",
      "1188it [13:46,  1.45it/s]\u001b[A\n",
      "1189it [13:47,  1.45it/s]\u001b[A\n",
      "1190it [13:47,  1.45it/s]\u001b[A\n",
      "1191it [13:48,  1.45it/s]\u001b[A\n",
      "1192it [13:49,  1.45it/s]\u001b[A\n",
      "1193it [13:49,  1.45it/s]\u001b[A\n",
      "1194it [13:50,  1.45it/s]\u001b[A\n",
      "1195it [13:51,  1.45it/s]\u001b[A\n",
      "1196it [13:51,  1.45it/s]\u001b[A\n",
      "1197it [13:52,  1.45it/s]\u001b[A\n",
      "1198it [13:53,  1.45it/s]\u001b[A\n",
      "1199it [13:53,  1.45it/s]\u001b[A\n",
      "1200it [13:54,  1.45it/s]\u001b[A\n",
      "1201it [13:55,  1.45it/s]\u001b[A\n",
      "1202it [13:55,  1.45it/s]\u001b[A\n",
      "1203it [13:56,  1.45it/s]\u001b[A\n",
      "1204it [13:57,  1.45it/s]\u001b[A\n",
      "1205it [13:58,  1.45it/s]\u001b[A\n",
      "1206it [13:58,  1.45it/s]\u001b[A\n",
      "1207it [13:59,  1.45it/s]\u001b[A\n",
      "1208it [14:00,  1.45it/s]\u001b[A\n",
      "1209it [14:00,  1.45it/s]\u001b[A\n",
      "1210it [14:01,  1.45it/s]\u001b[A\n",
      "1211it [14:02,  1.45it/s]\u001b[A\n",
      "1212it [14:02,  1.45it/s]\u001b[A\n",
      "1213it [14:03,  1.45it/s]\u001b[A\n",
      "1214it [14:04,  1.45it/s]\u001b[A\n",
      "1215it [14:04,  1.45it/s]\u001b[A\n",
      "1216it [14:05,  1.45it/s]\u001b[A\n",
      "1217it [14:06,  1.45it/s]\u001b[A\n",
      "1218it [14:07,  1.45it/s]\u001b[A\n",
      "1219it [14:07,  1.45it/s]\u001b[A\n",
      "1220it [14:08,  1.45it/s]\u001b[A\n",
      "1221it [14:09,  1.45it/s]\u001b[A\n",
      "1222it [14:09,  1.45it/s]\u001b[A\n",
      "1223it [14:10,  1.45it/s]\u001b[A\n",
      "1224it [14:11,  1.45it/s]\u001b[A\n",
      "1225it [14:11,  1.45it/s]\u001b[A\n",
      "1226it [14:12,  1.45it/s]\u001b[A\n",
      "1227it [14:13,  1.45it/s]\u001b[A\n",
      "1228it [14:13,  1.45it/s]\u001b[A\n",
      "1229it [14:14,  1.45it/s]\u001b[A\n",
      "1230it [14:15,  1.45it/s]\u001b[A\n",
      "1231it [14:15,  1.45it/s]\u001b[A\n",
      "1232it [14:16,  1.45it/s]\u001b[A\n",
      "1233it [14:17,  1.45it/s]\u001b[A\n",
      "1234it [14:18,  1.45it/s]\u001b[A\n",
      "1235it [14:18,  1.45it/s]\u001b[A\n",
      "1236it [14:19,  1.45it/s]\u001b[A\n",
      "1237it [14:20,  1.45it/s]\u001b[A\n",
      "1238it [14:20,  1.45it/s]\u001b[A\n",
      "1239it [14:21,  1.45it/s]\u001b[A\n",
      "1240it [14:22,  1.45it/s]\u001b[A\n",
      "1241it [14:22,  1.45it/s]\u001b[A\n",
      "1242it [14:23,  1.45it/s]\u001b[A\n",
      "1243it [14:24,  1.45it/s]\u001b[A\n",
      "1244it [14:24,  1.45it/s]\u001b[A\n",
      "1245it [14:25,  1.45it/s]\u001b[A\n",
      "1246it [14:26,  1.45it/s]\u001b[A\n",
      "1247it [14:26,  1.45it/s]\u001b[A\n",
      "1248it [14:27,  1.45it/s]\u001b[A\n",
      "1249it [14:28,  1.45it/s]\u001b[A\n",
      "1250it [14:29,  1.45it/s]\u001b[A\n",
      "1251it [14:29,  1.45it/s]\u001b[A\n",
      "1252it [14:30,  1.45it/s]\u001b[A\n",
      "1253it [14:31,  1.45it/s]\u001b[A\n",
      "1254it [14:31,  1.45it/s]\u001b[A\n",
      "1255it [14:32,  1.45it/s]\u001b[A\n",
      "1256it [14:33,  1.45it/s]\u001b[A\n",
      "1257it [14:33,  1.45it/s]\u001b[A\n",
      "1258it [14:34,  1.45it/s]\u001b[A\n",
      "1259it [14:35,  1.45it/s]\u001b[A\n",
      "1260it [14:35,  1.45it/s]\u001b[A\n",
      "1261it [14:36,  1.45it/s]\u001b[A\n",
      "1262it [14:37,  1.45it/s]\u001b[A\n",
      "1263it [14:38,  1.45it/s]\u001b[A\n",
      "1264it [14:38,  1.45it/s]\u001b[A\n",
      "1265it [14:39,  1.45it/s]\u001b[A\n",
      "1266it [14:40,  1.45it/s]\u001b[A\n",
      "1267it [14:40,  1.45it/s]\u001b[A\n",
      "1268it [14:41,  1.45it/s]\u001b[A\n",
      "1269it [14:42,  1.45it/s]\u001b[A\n",
      "1270it [14:42,  1.45it/s]\u001b[A\n",
      "1271it [14:43,  1.45it/s]\u001b[A\n",
      "1272it [14:44,  1.45it/s]\u001b[A\n",
      "1273it [14:44,  1.45it/s]\u001b[A\n",
      "1274it [14:45,  1.45it/s]\u001b[A\n",
      "1275it [14:46,  1.45it/s]\u001b[A\n",
      "1276it [14:46,  1.45it/s]\u001b[A\n",
      "1277it [14:47,  1.45it/s]\u001b[A\n",
      "1278it [14:48,  1.45it/s]\u001b[A\n",
      "1279it [14:49,  1.45it/s]\u001b[A\n",
      "1280it [14:49,  1.45it/s]\u001b[A\n",
      "1281it [14:50,  1.45it/s]\u001b[A\n",
      "1282it [14:51,  1.45it/s]\u001b[A\n",
      "1283it [14:51,  1.45it/s]\u001b[A\n",
      "1284it [14:52,  1.45it/s]\u001b[A\n",
      "1285it [14:53,  1.45it/s]\u001b[A\n",
      "1286it [14:53,  1.45it/s]\u001b[A\n",
      "1287it [14:54,  1.45it/s]\u001b[A\n",
      "1288it [14:55,  1.45it/s]\u001b[A\n",
      "1289it [14:55,  1.45it/s]\u001b[A\n",
      "1290it [14:56,  1.45it/s]\u001b[A\n",
      "1291it [14:57,  1.45it/s]\u001b[A\n",
      "1292it [14:57,  1.45it/s]\u001b[A\n",
      "1293it [14:58,  1.45it/s]\u001b[A\n",
      "1294it [14:59,  1.45it/s]\u001b[A\n",
      "1295it [15:00,  1.45it/s]\u001b[A\n",
      "1296it [15:00,  1.45it/s]\u001b[A\n",
      "1297it [15:01,  1.45it/s]\u001b[A\n",
      "1298it [15:02,  1.45it/s]\u001b[A\n",
      "1299it [15:02,  1.45it/s]\u001b[A\n",
      "1300it [15:03,  1.45it/s]\u001b[A\n",
      "1301it [15:04,  1.45it/s]\u001b[A\n",
      "1302it [15:04,  1.45it/s]\u001b[A\n",
      "1303it [15:05,  1.45it/s]\u001b[A\n",
      "1304it [15:06,  1.45it/s]\u001b[A\n",
      "1305it [15:06,  1.45it/s]\u001b[A\n",
      "1306it [15:07,  1.45it/s]\u001b[A\n",
      "1307it [15:08,  1.45it/s]\u001b[A\n",
      "1308it [15:08,  1.45it/s]\u001b[A\n",
      "1309it [15:09,  1.45it/s]\u001b[A\n",
      "1310it [15:10,  1.45it/s]\u001b[A\n",
      "1311it [15:11,  1.45it/s]\u001b[A\n",
      "1312it [15:11,  1.45it/s]\u001b[A\n",
      "1313it [15:12,  1.45it/s]\u001b[A\n",
      "1314it [15:13,  1.45it/s]\u001b[A\n",
      "1315it [15:13,  1.45it/s]\u001b[A\n",
      "1316it [15:14,  1.45it/s]\u001b[A\n",
      "1317it [15:15,  1.45it/s]\u001b[A\n",
      "1318it [15:15,  1.45it/s]\u001b[A\n",
      "1319it [15:16,  1.45it/s]\u001b[A\n",
      "1320it [15:17,  1.45it/s]\u001b[A\n",
      "1321it [15:17,  1.45it/s]\u001b[A\n",
      "1322it [15:18,  1.45it/s]\u001b[A\n",
      "1323it [15:19,  1.45it/s]\u001b[A\n",
      "1324it [15:20,  1.45it/s]\u001b[A\n",
      "1325it [15:20,  1.45it/s]\u001b[A\n",
      "1326it [15:21,  1.45it/s]\u001b[A\n",
      "1327it [15:22,  1.45it/s]\u001b[A\n",
      "1328it [15:22,  1.45it/s]\u001b[A\n",
      "1329it [15:23,  1.45it/s]\u001b[A\n",
      "1330it [15:24,  1.45it/s]\u001b[A\n",
      "1331it [15:24,  1.45it/s]\u001b[A\n",
      "1332it [15:25,  1.45it/s]\u001b[A\n",
      "1333it [15:26,  1.45it/s]\u001b[A\n",
      "1334it [15:26,  1.45it/s]\u001b[A\n",
      "1335it [15:27,  1.45it/s]\u001b[A\n",
      "1336it [15:28,  1.45it/s]\u001b[A\n",
      "1337it [15:28,  1.45it/s]\u001b[A\n",
      "1338it [15:29,  1.45it/s]\u001b[A\n",
      "1339it [15:30,  1.45it/s]\u001b[A\n",
      "1340it [15:31,  1.45it/s]\u001b[A\n",
      "1341it [15:31,  1.45it/s]\u001b[A\n",
      "1342it [15:32,  1.45it/s]\u001b[A\n",
      "1343it [15:33,  1.45it/s]\u001b[A\n",
      "1344it [15:33,  1.45it/s]\u001b[A\n",
      "1345it [15:34,  1.45it/s]\u001b[A\n",
      "1346it [15:35,  1.45it/s]\u001b[A\n",
      "1347it [15:35,  1.45it/s]\u001b[A\n",
      "1348it [15:36,  1.45it/s]\u001b[A\n",
      "1349it [15:37,  1.45it/s]\u001b[A\n",
      "1350it [15:37,  1.45it/s]\u001b[A\n",
      "1351it [15:38,  1.45it/s]\u001b[A\n",
      "1352it [15:39,  1.45it/s]\u001b[A\n",
      "1353it [15:39,  1.45it/s]\u001b[A\n",
      "1354it [15:40,  1.45it/s]\u001b[A\n",
      "1355it [15:41,  1.45it/s]\u001b[A\n",
      "1356it [15:42,  1.45it/s]\u001b[A\n",
      "1357it [15:42,  1.45it/s]\u001b[A\n",
      "1358it [15:43,  1.45it/s]\u001b[A\n",
      "1359it [15:44,  1.45it/s]\u001b[A\n",
      "1360it [15:44,  1.45it/s]\u001b[A\n",
      "1361it [15:45,  1.45it/s]\u001b[A\n",
      "1362it [15:46,  1.45it/s]\u001b[A\n",
      "1363it [15:46,  1.45it/s]\u001b[A\n",
      "1364it [15:47,  1.45it/s]\u001b[A\n",
      "1365it [15:48,  1.45it/s]\u001b[A\n",
      "1366it [15:48,  1.45it/s]\u001b[A\n",
      "1367it [15:49,  1.45it/s]\u001b[A\n",
      "1368it [15:50,  1.45it/s]\u001b[A\n",
      "1369it [15:51,  1.45it/s]\u001b[A\n",
      "1370it [15:51,  1.45it/s]\u001b[A\n",
      "1371it [15:52,  1.45it/s]\u001b[A\n",
      "1372it [15:53,  1.45it/s]\u001b[A\n",
      "1373it [15:53,  1.45it/s]\u001b[A\n",
      "1374it [15:54,  1.45it/s]\u001b[A\n",
      "1375it [15:55,  1.45it/s]\u001b[A\n",
      "1376it [15:55,  1.45it/s]\u001b[A\n",
      "1377it [15:56,  1.45it/s]\u001b[A\n",
      "1378it [15:57,  1.45it/s]\u001b[A\n",
      "1379it [15:57,  1.45it/s]\u001b[A\n",
      "1380it [15:58,  1.45it/s]\u001b[A\n",
      "1381it [15:59,  1.45it/s]\u001b[A\n",
      "1382it [15:59,  1.45it/s]\u001b[A\n",
      "1383it [16:00,  1.45it/s]\u001b[A\n",
      "1384it [16:01,  1.45it/s]\u001b[A\n",
      "1385it [16:02,  1.45it/s]\u001b[A\n",
      "1386it [16:02,  1.45it/s]\u001b[A\n",
      "1387it [16:03,  1.45it/s]\u001b[A\n",
      "1388it [16:04,  1.45it/s]\u001b[A\n",
      "1389it [16:04,  1.45it/s]\u001b[A\n",
      "1390it [16:05,  1.45it/s]\u001b[A\n",
      "1391it [16:06,  1.45it/s]\u001b[A\n",
      "1392it [16:06,  1.45it/s]\u001b[A\n",
      "1393it [16:07,  1.45it/s]\u001b[A\n",
      "1394it [16:08,  1.45it/s]\u001b[A\n",
      "1395it [16:08,  1.45it/s]\u001b[A\n",
      "1396it [16:09,  1.45it/s]\u001b[A\n",
      "1397it [16:10,  1.45it/s]\u001b[A\n",
      "1398it [16:10,  1.45it/s]\u001b[A\n",
      "1399it [16:11,  1.45it/s]\u001b[A\n",
      "1400it [16:12,  1.45it/s]\u001b[A\n",
      "1401it [16:13,  1.45it/s]\u001b[A\n",
      "1402it [16:13,  1.45it/s]\u001b[A\n",
      "1403it [16:14,  1.45it/s]\u001b[A\n",
      "1404it [16:15,  1.45it/s]\u001b[A\n",
      "1405it [16:15,  1.45it/s]\u001b[A\n",
      "1406it [16:16,  1.45it/s]\u001b[A\n",
      "1407it [16:17,  1.45it/s]\u001b[A\n",
      "1408it [16:17,  1.45it/s]\u001b[A\n",
      "1409it [16:18,  1.45it/s]\u001b[A\n",
      "1410it [16:19,  1.45it/s]\u001b[A\n",
      "1411it [16:19,  1.45it/s]\u001b[A\n",
      "1412it [16:20,  1.45it/s]\u001b[A\n",
      "1413it [16:21,  1.45it/s]\u001b[A\n",
      "1414it [16:22,  1.45it/s]\u001b[A\n",
      "1415it [16:22,  1.45it/s]\u001b[A\n",
      "1416it [16:23,  1.45it/s]\u001b[A\n",
      "1417it [16:24,  1.45it/s]\u001b[A\n",
      "1418it [16:24,  1.45it/s]\u001b[A\n",
      "1419it [16:25,  1.45it/s]\u001b[A\n",
      "1420it [16:26,  1.45it/s]\u001b[A\n",
      "1421it [16:26,  1.45it/s]\u001b[A\n",
      "1422it [16:27,  1.45it/s]\u001b[A\n",
      "1423it [16:28,  1.45it/s]\u001b[A\n",
      "1424it [16:28,  1.45it/s]\u001b[A\n",
      "1425it [16:29,  1.45it/s]\u001b[A\n",
      "1426it [16:30,  1.45it/s]\u001b[A\n",
      "1427it [16:30,  1.45it/s]\u001b[A\n",
      "1428it [16:31,  1.45it/s]\u001b[A\n",
      "1429it [16:32,  1.45it/s]\u001b[A\n",
      "1430it [16:33,  1.45it/s]\u001b[A\n",
      "1431it [16:33,  1.45it/s]\u001b[A\n",
      "1432it [16:34,  1.45it/s]\u001b[A\n",
      "1433it [16:35,  1.45it/s]\u001b[A\n",
      "1434it [16:35,  1.45it/s]\u001b[A\n",
      "1435it [16:36,  1.45it/s]\u001b[A\n",
      "1436it [16:37,  1.45it/s]\u001b[A\n",
      "1437it [16:37,  1.45it/s]\u001b[A\n",
      "1438it [16:38,  1.45it/s]\u001b[A\n",
      "1439it [16:39,  1.45it/s]\u001b[A\n",
      "1440it [16:39,  1.45it/s]\u001b[A\n",
      "1441it [16:40,  1.45it/s]\u001b[A\n",
      "1442it [16:41,  1.45it/s]\u001b[A\n",
      "1443it [16:41,  1.45it/s]\u001b[A\n",
      "1444it [16:42,  1.45it/s]\u001b[A\n",
      "1445it [16:43,  1.45it/s]\u001b[A\n",
      "1446it [16:44,  1.45it/s]\u001b[A\n",
      "1447it [16:44,  1.45it/s]\u001b[A\n",
      "1448it [16:45,  1.45it/s]\u001b[A\n",
      "1449it [16:46,  1.45it/s]\u001b[A\n",
      "1450it [16:46,  1.45it/s]\u001b[A\n",
      "1451it [16:47,  1.45it/s]\u001b[A\n",
      "1452it [16:48,  1.45it/s]\u001b[A\n",
      "1453it [16:48,  1.45it/s]\u001b[A\n",
      "1454it [16:49,  1.45it/s]\u001b[A\n",
      "1455it [16:50,  1.45it/s]\u001b[A\n",
      "1456it [16:50,  1.45it/s]\u001b[A\n",
      "1457it [16:51,  1.45it/s]\u001b[A\n",
      "1458it [16:52,  1.45it/s]\u001b[A\n",
      "1459it [16:53,  1.45it/s]\u001b[A\n",
      "1460it [16:53,  1.45it/s]\u001b[A\n",
      "1461it [16:54,  1.45it/s]\u001b[A\n",
      "1462it [16:55,  1.45it/s]\u001b[A\n",
      "1463it [16:55,  1.45it/s]\u001b[A\n",
      "1464it [16:56,  1.45it/s]\u001b[A\n",
      "1465it [16:57,  1.45it/s]\u001b[A\n",
      "1466it [16:57,  1.45it/s]\u001b[A\n",
      "1467it [16:58,  1.45it/s]\u001b[A\n",
      "1468it [16:59,  1.45it/s]\u001b[A\n",
      "1469it [16:59,  1.45it/s]\u001b[A\n",
      "1470it [17:00,  1.45it/s]\u001b[A\n",
      "1471it [17:01,  1.45it/s]\u001b[A\n",
      "1472it [17:01,  1.45it/s]\u001b[A\n",
      "1473it [17:02,  1.45it/s]\u001b[A\n",
      "1474it [17:03,  1.45it/s]\u001b[A\n",
      "1475it [17:04,  1.45it/s]\u001b[A\n",
      "1476it [17:04,  1.45it/s]\u001b[A\n",
      "1477it [17:05,  1.45it/s]\u001b[A\n",
      "1478it [17:06,  1.45it/s]\u001b[A\n",
      "1479it [17:06,  1.45it/s]\u001b[A\n",
      "1480it [17:07,  1.45it/s]\u001b[A\n",
      "1481it [17:08,  1.45it/s]\u001b[A\n",
      "1482it [17:08,  1.45it/s]\u001b[A\n",
      "1483it [17:09,  1.45it/s]\u001b[A\n",
      "1484it [17:10,  1.45it/s]\u001b[A\n",
      "1485it [17:10,  1.45it/s]\u001b[A\n",
      "1486it [17:11,  1.45it/s]\u001b[A\n",
      "1487it [17:12,  1.45it/s]\u001b[A\n",
      "1488it [17:12,  1.45it/s]\u001b[A\n",
      "1489it [17:13,  1.45it/s]\u001b[A\n",
      "1490it [17:14,  1.45it/s]\u001b[A\n",
      "1491it [17:15,  1.45it/s]\u001b[A\n",
      "1492it [17:15,  1.45it/s]\u001b[A\n",
      "1493it [17:16,  1.45it/s]\u001b[A\n",
      "1494it [17:17,  1.45it/s]\u001b[A\n",
      "1495it [17:17,  1.45it/s]\u001b[A\n",
      "1496it [17:18,  1.45it/s]\u001b[A\n",
      "1497it [17:19,  1.45it/s]\u001b[A\n",
      "1498it [17:19,  1.45it/s]\u001b[A\n",
      "1499it [17:20,  1.45it/s]\u001b[A\n",
      "1500it [17:21,  1.45it/s]\u001b[A\n",
      "1501it [17:21,  1.45it/s]\u001b[A\n",
      "1502it [17:22,  1.45it/s]\u001b[A\n",
      "1503it [17:23,  1.45it/s]\u001b[A\n",
      "1504it [17:24,  1.45it/s]\u001b[A\n",
      "1505it [17:24,  1.45it/s]\u001b[A\n",
      "1506it [17:25,  1.45it/s]\u001b[A\n",
      "1507it [17:26,  1.45it/s]\u001b[A\n",
      "1508it [17:26,  1.45it/s]\u001b[A\n",
      "1509it [17:27,  1.45it/s]\u001b[A\n",
      "1510it [17:28,  1.45it/s]\u001b[A\n",
      "1511it [17:28,  1.45it/s]\u001b[A\n",
      "1512it [17:29,  1.45it/s]\u001b[A\n",
      "1513it [17:30,  1.45it/s]\u001b[A\n",
      "1514it [17:30,  1.45it/s]\u001b[A\n",
      "1515it [17:31,  1.45it/s]\u001b[A\n",
      "1516it [17:32,  1.45it/s]\u001b[A\n",
      "1517it [17:32,  1.45it/s]\u001b[A\n",
      "1518it [17:33,  1.45it/s]\u001b[A\n",
      "1519it [17:34,  1.45it/s]\u001b[A\n",
      "1520it [17:35,  1.45it/s]\u001b[A\n",
      "1521it [17:35,  1.45it/s]\u001b[A\n",
      "1522it [17:36,  1.45it/s]\u001b[A\n",
      "1523it [17:37,  1.45it/s]\u001b[A\n",
      "1524it [17:37,  1.45it/s]\u001b[A\n",
      "1525it [17:38,  1.45it/s]\u001b[A\n",
      "1526it [17:39,  1.45it/s]\u001b[A\n",
      "1527it [17:39,  1.45it/s]\u001b[A\n",
      "1528it [17:40,  1.45it/s]\u001b[A\n",
      "1529it [17:41,  1.45it/s]\u001b[A\n",
      "1530it [17:41,  1.45it/s]\u001b[A\n",
      "1531it [17:42,  1.45it/s]\u001b[A\n",
      "1532it [17:43,  1.45it/s]\u001b[A\n",
      "1533it [17:43,  1.45it/s]\u001b[A\n",
      "1534it [17:44,  1.45it/s]\u001b[A\n",
      "1535it [17:45,  1.45it/s]\u001b[A\n",
      "1536it [17:46,  1.45it/s]\u001b[A\n",
      "1537it [17:46,  1.45it/s]\u001b[A\n",
      "1538it [17:47,  1.45it/s]\u001b[A\n",
      "1539it [17:48,  1.45it/s]\u001b[A\n",
      "1540it [17:48,  1.45it/s]\u001b[A\n",
      "1541it [17:49,  1.45it/s]\u001b[A\n",
      "1542it [17:50,  1.45it/s]\u001b[A\n",
      "1543it [17:50,  1.45it/s]\u001b[A\n",
      "1544it [17:51,  1.45it/s]\u001b[A\n",
      "1545it [17:52,  1.45it/s]\u001b[A\n",
      "1546it [17:52,  1.45it/s]\u001b[A\n",
      "1547it [17:53,  1.45it/s]\u001b[A\n",
      "1548it [17:54,  1.45it/s]\u001b[A\n",
      "1549it [17:55,  1.45it/s]\u001b[A\n",
      "1550it [17:55,  1.45it/s]\u001b[A\n",
      "1551it [17:56,  1.45it/s]\u001b[A\n",
      "1552it [17:57,  1.45it/s]\u001b[A\n",
      "1553it [17:57,  1.45it/s]\u001b[A\n",
      "1554it [17:58,  1.45it/s]\u001b[A\n",
      "1555it [17:59,  1.45it/s]\u001b[A\n",
      "1556it [17:59,  1.45it/s]\u001b[A\n",
      "1557it [18:00,  1.45it/s]\u001b[A\n",
      "1558it [18:01,  1.45it/s]\u001b[A\n",
      "1559it [18:01,  1.45it/s]\u001b[A\n",
      "1560it [18:02,  1.45it/s]\u001b[A\n",
      "1561it [18:03,  1.45it/s]\u001b[A\n",
      "1562it [18:03,  1.45it/s]\u001b[A\n",
      "1563it [18:04,  1.45it/s]\u001b[A\n",
      "1564it [18:05,  1.45it/s]\u001b[A\n",
      "1565it [18:06,  1.45it/s]\u001b[A\n",
      "1566it [18:06,  1.45it/s]\u001b[A\n",
      "1567it [18:07,  1.45it/s]\u001b[A\n",
      "1568it [18:08,  1.45it/s]\u001b[A\n",
      "1569it [18:08,  1.45it/s]\u001b[A\n",
      "1570it [18:09,  1.45it/s]\u001b[A\n",
      "1571it [18:10,  1.45it/s]\u001b[A\n",
      "1572it [18:10,  1.45it/s]\u001b[A\n",
      "1573it [18:11,  1.45it/s]\u001b[A\n",
      "1574it [18:12,  1.45it/s]\u001b[A\n",
      "1575it [18:12,  1.45it/s]\u001b[A\n",
      "1576it [18:13,  1.45it/s]\u001b[A\n",
      "1577it [18:14,  1.45it/s]\u001b[A\n",
      "1578it [18:14,  1.45it/s]\u001b[A\n",
      "1579it [18:15,  1.45it/s]\u001b[A\n",
      "1580it [18:16,  1.45it/s]\u001b[A\n",
      "1581it [18:17,  1.45it/s]\u001b[A\n",
      "1582it [18:17,  1.45it/s]\u001b[A\n",
      "1583it [18:18,  1.45it/s]\u001b[A\n",
      "1584it [18:19,  1.45it/s]\u001b[A\n",
      "1585it [18:19,  1.45it/s]\u001b[A\n",
      "1586it [18:20,  1.45it/s]\u001b[A\n",
      "1587it [18:21,  1.45it/s]\u001b[A\n",
      "1588it [18:21,  1.45it/s]\u001b[A\n",
      "1589it [18:22,  1.45it/s]\u001b[A\n",
      "1590it [18:23,  1.45it/s]\u001b[A\n",
      "1591it [18:23,  1.45it/s]\u001b[A\n",
      "1592it [18:24,  1.45it/s]\u001b[A\n",
      "1593it [18:25,  1.45it/s]\u001b[A\n",
      "1594it [18:26,  1.45it/s]\u001b[A\n",
      "1595it [18:26,  1.45it/s]\u001b[A\n",
      "1596it [18:27,  1.45it/s]\u001b[A\n",
      "1597it [18:28,  1.45it/s]\u001b[A\n",
      "1598it [18:28,  1.45it/s]\u001b[A\n",
      "1599it [18:29,  1.45it/s]\u001b[A\n",
      "1600it [18:30,  1.45it/s]\u001b[A\n",
      "1601it [18:30,  1.45it/s]\u001b[A\n",
      "1602it [18:31,  1.45it/s]\u001b[A\n",
      "1603it [18:32,  1.45it/s]\u001b[A\n",
      "1604it [18:32,  1.45it/s]\u001b[A\n",
      "1605it [18:33,  1.45it/s]\u001b[A\n",
      "1606it [18:34,  1.45it/s]\u001b[A\n",
      "1607it [18:34,  1.45it/s]\u001b[A\n",
      "1608it [18:35,  1.45it/s]\u001b[A\n",
      "1609it [18:36,  1.45it/s]\u001b[A\n",
      "1610it [18:37,  1.45it/s]\u001b[A\n",
      "1611it [18:37,  1.45it/s]\u001b[A\n",
      "1612it [18:38,  1.45it/s]\u001b[A\n",
      "1613it [18:39,  1.45it/s]\u001b[A\n",
      "1614it [18:39,  1.45it/s]\u001b[A\n",
      "1615it [18:40,  1.45it/s]\u001b[A\n",
      "1616it [18:41,  1.45it/s]\u001b[A\n",
      "1617it [18:41,  1.45it/s]\u001b[A\n",
      "1618it [18:42,  1.45it/s]\u001b[A\n",
      "1619it [18:43,  1.45it/s]\u001b[A\n",
      "1620it [18:43,  1.45it/s]\u001b[A\n",
      "1621it [18:44,  1.45it/s]\u001b[A\n",
      "1622it [18:45,  1.45it/s]\u001b[A\n",
      "1623it [18:46,  1.45it/s]\u001b[A\n",
      "1624it [18:46,  1.45it/s]\u001b[A\n",
      "1625it [18:47,  1.45it/s]\u001b[A\n",
      "1626it [18:48,  1.45it/s]\u001b[A\n",
      "1627it [18:48,  1.45it/s]\u001b[A\n",
      "1628it [18:49,  1.45it/s]\u001b[A\n",
      "1629it [18:50,  1.45it/s]\u001b[A\n",
      "1630it [18:50,  1.45it/s]\u001b[A\n",
      "1631it [18:51,  1.45it/s]\u001b[A\n",
      "1632it [18:52,  1.45it/s]\u001b[A\n",
      "1633it [18:52,  1.45it/s]\u001b[A\n",
      "1634it [18:53,  1.45it/s]\u001b[A\n",
      "1635it [18:54,  1.45it/s]\u001b[A\n",
      "1636it [18:54,  1.45it/s]\u001b[A\n",
      "1637it [18:55,  1.45it/s]\u001b[A\n",
      "1638it [18:56,  1.45it/s]\u001b[A\n",
      "1639it [18:57,  1.45it/s]\u001b[A\n",
      "1640it [18:57,  1.45it/s]\u001b[A\n",
      "1641it [18:58,  1.45it/s]\u001b[A\n",
      "1642it [18:59,  1.45it/s]\u001b[A\n",
      "1643it [18:59,  1.45it/s]\u001b[A\n",
      "1644it [19:00,  1.45it/s]\u001b[A\n",
      "1645it [19:01,  1.45it/s]\u001b[A\n",
      "1646it [19:01,  1.45it/s]\u001b[A\n",
      "1647it [19:02,  1.45it/s]\u001b[A\n",
      "1648it [19:03,  1.45it/s]\u001b[A\n",
      "1649it [19:03,  1.45it/s]\u001b[A\n",
      "1650it [19:04,  1.45it/s]\u001b[A\n",
      "1651it [19:05,  1.45it/s]\u001b[A\n",
      "1652it [19:05,  1.45it/s]\u001b[A\n",
      "1653it [19:06,  1.45it/s]\u001b[A\n",
      "1654it [19:07,  1.45it/s]\u001b[A\n",
      "1655it [19:08,  1.45it/s]\u001b[A\n",
      "1656it [19:08,  1.45it/s]\u001b[A\n",
      "1657it [19:09,  1.45it/s]\u001b[A\n",
      "1658it [19:10,  1.45it/s]\u001b[A\n",
      "1659it [19:10,  1.45it/s]\u001b[A\n",
      "1660it [19:11,  1.45it/s]\u001b[A\n",
      "1661it [19:12,  1.45it/s]\u001b[A\n",
      "1662it [19:12,  1.45it/s]\u001b[A\n",
      "1663it [19:13,  1.45it/s]\u001b[A\n",
      "1664it [19:14,  1.45it/s]\u001b[A\n",
      "1665it [19:14,  1.45it/s]\u001b[A\n",
      "1666it [19:15,  1.45it/s]\u001b[A\n",
      "1667it [19:16,  1.45it/s]\u001b[A\n",
      "1668it [19:16,  1.45it/s]\u001b[A\n",
      "1669it [19:17,  1.45it/s]\u001b[A\n",
      "1670it [19:18,  1.45it/s]\u001b[A\n",
      "1671it [19:19,  1.45it/s]\u001b[A\n",
      "1672it [19:19,  1.45it/s]\u001b[A\n",
      "1673it [19:20,  1.45it/s]\u001b[A\n",
      "1674it [19:21,  1.45it/s]\u001b[A\n",
      "1675it [19:21,  1.45it/s]\u001b[A\n",
      "1676it [19:22,  1.45it/s]\u001b[A\n",
      "1677it [19:23,  1.45it/s]\u001b[A\n",
      "1678it [19:23,  1.45it/s]\u001b[A\n",
      "1679it [19:24,  1.45it/s]\u001b[A\n",
      "1680it [19:25,  1.45it/s]\u001b[A\n",
      "1681it [19:25,  1.45it/s]\u001b[A\n",
      "1682it [19:26,  1.45it/s]\u001b[A\n",
      "1683it [19:27,  1.45it/s]\u001b[A\n",
      "1684it [19:28,  1.45it/s]\u001b[A\n",
      "1685it [19:28,  1.45it/s]\u001b[A\n",
      "1686it [19:29,  1.45it/s]\u001b[A\n",
      "1687it [19:30,  1.45it/s]\u001b[A\n",
      "1688it [19:30,  1.45it/s]\u001b[A\n",
      "1689it [19:31,  1.45it/s]\u001b[A\n",
      "1690it [19:32,  1.45it/s]\u001b[A\n",
      "1691it [19:32,  1.45it/s]\u001b[A\n",
      "1692it [19:33,  1.45it/s]\u001b[A\n",
      "1693it [19:34,  1.45it/s]\u001b[A\n",
      "1694it [19:34,  1.45it/s]\u001b[A\n",
      "1695it [19:35,  1.45it/s]\u001b[A\n",
      "1696it [19:36,  1.45it/s]\u001b[A\n",
      "1697it [19:36,  1.45it/s]\u001b[A\n",
      "1698it [19:37,  1.45it/s]\u001b[A\n",
      "1699it [19:38,  1.45it/s]\u001b[A\n",
      "1700it [19:39,  1.45it/s]\u001b[A\n",
      "1701it [19:39,  1.45it/s]\u001b[A\n",
      "1702it [19:40,  1.45it/s]\u001b[A\n",
      "1703it [19:41,  1.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1704it [19:41,  1.45it/s]\u001b[A\n",
      "1705it [19:42,  1.45it/s]\u001b[A\n",
      "1706it [19:43,  1.45it/s]\u001b[A\n",
      "1707it [19:43,  1.45it/s]\u001b[A\n",
      "1708it [19:44,  1.45it/s]\u001b[A\n",
      "1709it [19:45,  1.45it/s]\u001b[A\n",
      "1710it [19:45,  1.45it/s]\u001b[A\n",
      "1711it [19:46,  1.45it/s]\u001b[A\n",
      "1712it [19:47,  1.45it/s]\u001b[A\n",
      "1713it [19:47,  1.45it/s]\u001b[A\n",
      "1714it [19:48,  1.45it/s]\u001b[A\n",
      "1715it [19:49,  1.45it/s]\u001b[A\n",
      "1716it [19:50,  1.45it/s]\u001b[A\n",
      "1717it [19:50,  1.45it/s]\u001b[A\n",
      "1718it [19:51,  1.45it/s]\u001b[A\n",
      "1719it [19:52,  1.45it/s]\u001b[A\n",
      "1720it [19:52,  1.45it/s]\u001b[A\n",
      "1721it [19:53,  1.45it/s]\u001b[A\n",
      "1722it [19:54,  1.45it/s]\u001b[A\n",
      "1723it [19:54,  1.45it/s]\u001b[A\n",
      "1724it [19:55,  1.45it/s]\u001b[A\n",
      "1725it [19:56,  1.45it/s]\u001b[A\n",
      "1726it [19:56,  1.45it/s]\u001b[A\n",
      "1727it [19:57,  1.45it/s]\u001b[A\n",
      "1728it [19:58,  1.45it/s]\u001b[A\n",
      "1729it [19:59,  1.45it/s]\u001b[A\n",
      "1730it [19:59,  1.45it/s]\u001b[A\n",
      "1731it [20:00,  1.45it/s]\u001b[A\n",
      "1732it [20:01,  1.45it/s]\u001b[A\n",
      "1733it [20:01,  1.45it/s]\u001b[A\n",
      "1734it [20:02,  1.45it/s]\u001b[A\n",
      "1735it [20:03,  1.45it/s]\u001b[A\n",
      "1736it [20:03,  1.45it/s]\u001b[A\n",
      "1737it [20:04,  1.45it/s]\u001b[A\n",
      "1738it [20:05,  1.45it/s]\u001b[A\n",
      "1739it [20:05,  1.45it/s]\u001b[A\n",
      "1740it [20:06,  1.45it/s]\u001b[A\n",
      "1741it [20:07,  1.45it/s]\u001b[A\n",
      "1742it [20:07,  1.45it/s]\u001b[A\n",
      "1743it [20:08,  1.45it/s]\u001b[A\n",
      "1744it [20:09,  1.45it/s]\u001b[A\n",
      "1745it [20:10,  1.45it/s]\u001b[A\n",
      "1746it [20:10,  1.45it/s]\u001b[A\n",
      "1747it [20:11,  1.45it/s]\u001b[A\n",
      "1748it [20:12,  1.45it/s]\u001b[A\n",
      "1749it [20:12,  1.45it/s]\u001b[A\n",
      "1750it [20:13,  1.45it/s]\u001b[A\n",
      "1751it [20:14,  1.45it/s]\u001b[A\n",
      "1752it [20:14,  1.45it/s]\u001b[A\n",
      "1753it [20:15,  1.45it/s]\u001b[A\n",
      "1754it [20:16,  1.45it/s]\u001b[A\n",
      "1755it [20:16,  1.45it/s]\u001b[A\n",
      "1756it [20:17,  1.45it/s]\u001b[A\n",
      "1757it [20:18,  1.45it/s]\u001b[A\n",
      "1758it [20:18,  1.45it/s]\u001b[A\n",
      "1759it [20:19,  1.45it/s]\u001b[A\n",
      "1760it [20:20,  1.45it/s]\u001b[A\n",
      "1761it [20:21,  1.45it/s]\u001b[A\n",
      "1762it [20:21,  1.45it/s]\u001b[A\n",
      "1763it [20:22,  1.45it/s]\u001b[A\n",
      "1764it [20:23,  1.45it/s]\u001b[A\n",
      "1765it [20:23,  1.45it/s]\u001b[A\n",
      "1766it [20:24,  1.45it/s]\u001b[A\n",
      "1767it [20:25,  1.45it/s]\u001b[A\n",
      "1768it [20:25,  1.45it/s]\u001b[A\n",
      "1769it [20:26,  1.45it/s]\u001b[A\n",
      "1770it [20:27,  1.45it/s]\u001b[A\n",
      "1771it [20:27,  1.45it/s]\u001b[A\n",
      "1772it [20:28,  1.45it/s]\u001b[A\n",
      "1773it [20:29,  1.45it/s]\u001b[A\n",
      "1774it [20:30,  1.45it/s]\u001b[A\n",
      "1775it [20:30,  1.45it/s]\u001b[A\n",
      "1776it [20:31,  1.45it/s]\u001b[A\n",
      "1777it [20:32,  1.45it/s]\u001b[A\n",
      "1778it [20:32,  1.45it/s]\u001b[A\n",
      "1779it [20:33,  1.45it/s]\u001b[A\n",
      "1780it [20:34,  1.45it/s]\u001b[A\n",
      "1781it [20:34,  1.45it/s]\u001b[A\n",
      "1782it [20:35,  1.45it/s]\u001b[A\n",
      "1783it [20:36,  1.45it/s]\u001b[A\n",
      "1784it [20:36,  1.45it/s]\u001b[A\n",
      "1785it [20:37,  1.45it/s]\u001b[A\n",
      "1786it [20:38,  1.45it/s]\u001b[A\n",
      "1787it [20:38,  1.45it/s]\u001b[A\n",
      "1788it [20:39,  1.45it/s]\u001b[A\n",
      "1789it [20:40,  1.45it/s]\u001b[A\n",
      "1790it [20:41,  1.45it/s]\u001b[A\n",
      "1791it [20:41,  1.45it/s]\u001b[A\n",
      "1792it [20:42,  1.45it/s]\u001b[A\n",
      "1793it [20:43,  1.45it/s]\u001b[A\n",
      "1794it [20:43,  1.45it/s]\u001b[A\n",
      "1795it [20:44,  1.45it/s]\u001b[A\n",
      "1796it [20:45,  1.45it/s]\u001b[A\n",
      "1797it [20:45,  1.45it/s]\u001b[A\n",
      "1798it [20:46,  1.45it/s]\u001b[A\n",
      "1799it [20:47,  1.45it/s]\u001b[A\n",
      "1800it [20:47,  1.45it/s]\u001b[A\n",
      "1801it [20:48,  1.45it/s]\u001b[A\n",
      "1802it [20:49,  1.45it/s]\u001b[A\n",
      "1803it [20:49,  1.45it/s]\u001b[A\n",
      "1804it [20:50,  1.45it/s]\u001b[A\n",
      "1805it [20:51,  1.45it/s]\u001b[A\n",
      "1806it [20:52,  1.45it/s]\u001b[A\n",
      "1807it [20:52,  1.45it/s]\u001b[A\n",
      "1808it [20:53,  1.45it/s]\u001b[A\n",
      "1809it [20:54,  1.45it/s]\u001b[A\n",
      "1810it [20:54,  1.45it/s]\u001b[A\n",
      "1811it [20:55,  1.45it/s]\u001b[A\n",
      "1812it [20:56,  1.45it/s]\u001b[A\n",
      "1813it [20:56,  1.45it/s]\u001b[A\n",
      "1814it [20:57,  1.45it/s]\u001b[A\n",
      "1815it [20:58,  1.45it/s]\u001b[A\n",
      "1816it [20:58,  1.45it/s]\u001b[A\n",
      "1817it [20:59,  1.45it/s]\u001b[A\n",
      "1818it [21:00,  1.45it/s]\u001b[A\n",
      "1819it [21:00,  1.45it/s]\u001b[A\n",
      "1820it [21:01,  1.45it/s]\u001b[A\n",
      "1821it [21:02,  1.45it/s]\u001b[A\n",
      "1822it [21:03,  1.45it/s]\u001b[A\n",
      "1823it [21:03,  1.45it/s]\u001b[A\n",
      "1824it [21:04,  1.45it/s]\u001b[A\n",
      "1825it [21:05,  1.45it/s]\u001b[A\n",
      "1826it [21:05,  1.45it/s]\u001b[A\n",
      "1827it [21:06,  1.45it/s]\u001b[A\n",
      "1828it [21:07,  1.45it/s]\u001b[A\n",
      "1829it [21:07,  1.45it/s]\u001b[A\n",
      "1830it [21:08,  1.45it/s]\u001b[A\n",
      "1831it [21:09,  1.45it/s]\u001b[A\n",
      "1832it [21:09,  1.45it/s]\u001b[A\n",
      "1833it [21:10,  1.45it/s]\u001b[A\n",
      "1834it [21:11,  1.45it/s]\u001b[A\n",
      "1835it [21:12,  1.45it/s]\u001b[A\n",
      "1836it [21:12,  1.45it/s]\u001b[A\n",
      "1837it [21:13,  1.45it/s]\u001b[A\n",
      "1838it [21:14,  1.45it/s]\u001b[A\n",
      "1839it [21:14,  1.45it/s]\u001b[A\n",
      "1840it [21:15,  1.45it/s]\u001b[A\n",
      "1841it [21:16,  1.45it/s]\u001b[A\n",
      "1842it [21:16,  1.45it/s]\u001b[A\n",
      "1843it [21:17,  1.45it/s]\u001b[A\n",
      "1844it [21:18,  1.45it/s]\u001b[A\n",
      "1845it [21:18,  1.45it/s]\u001b[A\n",
      "1846it [21:19,  1.45it/s]\u001b[A\n",
      "1847it [21:20,  1.45it/s]\u001b[A\n",
      "1848it [21:20,  1.45it/s]\u001b[A\n",
      "1849it [21:21,  1.45it/s]\u001b[A\n",
      "1850it [21:22,  1.45it/s]\u001b[A\n",
      "1851it [21:23,  1.45it/s]\u001b[A\n",
      "1852it [21:23,  1.45it/s]\u001b[A\n",
      "1853it [21:24,  1.45it/s]\u001b[A\n",
      "1854it [21:25,  1.45it/s]\u001b[A\n",
      "1855it [21:25,  1.45it/s]\u001b[A\n",
      "1856it [21:26,  1.45it/s]\u001b[A\n",
      "1857it [21:27,  1.45it/s]\u001b[A\n",
      "1858it [21:27,  1.45it/s]\u001b[A\n",
      "1859it [21:28,  1.45it/s]\u001b[A\n",
      "1860it [21:29,  1.45it/s]\u001b[A\n",
      "1861it [21:29,  1.45it/s]\u001b[A\n",
      "1862it [21:30,  1.45it/s]\u001b[A\n",
      "1863it [21:31,  1.45it/s]\u001b[A\n",
      "1864it [21:31,  1.45it/s]\u001b[A\n",
      "1865it [21:32,  1.45it/s]\u001b[A\n",
      "1866it [21:33,  1.45it/s]\u001b[A\n",
      "1867it [21:34,  1.45it/s]\u001b[A\n",
      "1868it [21:34,  1.45it/s]\u001b[A\n",
      "1869it [21:35,  1.45it/s]\u001b[A\n",
      "1870it [21:36,  1.45it/s]\u001b[A\n",
      "1871it [21:36,  1.45it/s]\u001b[A\n",
      "1872it [21:37,  1.45it/s]\u001b[A\n",
      "1873it [21:38,  1.45it/s]\u001b[A\n",
      "1874it [21:38,  1.45it/s]\u001b[A\n",
      "1875it [21:39,  1.45it/s]\u001b[A\n",
      "1876it [21:40,  1.45it/s]\u001b[A\n",
      "1877it [21:40,  1.45it/s]\u001b[A\n",
      "1878it [21:41,  1.45it/s]\u001b[A\n",
      "1879it [21:42,  1.45it/s]\u001b[A\n",
      "1880it [21:43,  1.45it/s]\u001b[A\n",
      "1881it [21:43,  1.45it/s]\u001b[A\n",
      "1882it [21:44,  1.45it/s]\u001b[A\n",
      "1883it [21:45,  1.45it/s]\u001b[A\n",
      "1884it [21:45,  1.45it/s]\u001b[A\n",
      "1885it [21:46,  1.45it/s]\u001b[A\n",
      "1886it [21:47,  1.45it/s]\u001b[A\n",
      "1887it [21:47,  1.45it/s]\u001b[A\n",
      "1888it [21:48,  1.45it/s]\u001b[A\n",
      "1889it [21:49,  1.45it/s]\u001b[A\n",
      "1890it [21:49,  1.45it/s]\u001b[A\n",
      "1891it [21:50,  1.45it/s]\u001b[A\n",
      "1892it [21:51,  1.45it/s]\u001b[A\n",
      "1893it [21:51,  1.45it/s]\u001b[A\n",
      "1894it [21:52,  1.45it/s]\u001b[A\n",
      "1895it [21:53,  1.45it/s]\u001b[A\n",
      "1896it [21:54,  1.45it/s]\u001b[A\n",
      "1897it [21:54,  1.45it/s]\u001b[A\n",
      "1898it [21:55,  1.45it/s]\u001b[A\n",
      "1899it [21:56,  1.45it/s]\u001b[A\n",
      "1900it [21:56,  1.45it/s]\u001b[A\n",
      "1901it [21:57,  1.45it/s]\u001b[A\n",
      "1902it [21:58,  1.45it/s]\u001b[A\n",
      "1903it [21:58,  1.45it/s]\u001b[A\n",
      "1904it [21:59,  1.45it/s]\u001b[A\n",
      "1905it [22:00,  1.45it/s]\u001b[A\n",
      "1906it [22:00,  1.45it/s]\u001b[A\n",
      "1907it [22:01,  1.45it/s]\u001b[A\n",
      "1908it [22:02,  1.45it/s]\u001b[A\n",
      "1909it [22:03,  1.45it/s]\u001b[A\n",
      "1910it [22:03,  1.45it/s]\u001b[A\n",
      "1911it [22:04,  1.45it/s]\u001b[A\n",
      "1912it [22:05,  1.45it/s]\u001b[A\n",
      "1913it [22:05,  1.45it/s]\u001b[A\n",
      "1914it [22:06,  1.45it/s]\u001b[A\n",
      "1915it [22:07,  1.45it/s]\u001b[A\n",
      "1916it [22:07,  1.45it/s]\u001b[A\n",
      "1917it [22:08,  1.45it/s]\u001b[A\n",
      "1918it [22:09,  1.45it/s]\u001b[A\n",
      "1919it [22:09,  1.45it/s]\u001b[A\n",
      "1920it [22:10,  1.45it/s]\u001b[A\n",
      "1921it [22:11,  1.45it/s]\u001b[A\n",
      "1922it [22:11,  1.45it/s]\u001b[A\n",
      "1923it [22:12,  1.45it/s]\u001b[A\n",
      "1924it [22:13,  1.45it/s]\u001b[A\n",
      "1925it [22:14,  1.45it/s]\u001b[A\n",
      "1926it [22:14,  1.45it/s]\u001b[A\n",
      "1927it [22:15,  1.45it/s]\u001b[A\n",
      "1928it [22:16,  1.45it/s]\u001b[A\n",
      "1929it [22:16,  1.45it/s]\u001b[A\n",
      "1930it [22:17,  1.45it/s]\u001b[A\n",
      "1931it [22:18,  1.45it/s]\u001b[A\n",
      "1932it [22:18,  1.45it/s]\u001b[A\n",
      "1933it [22:19,  1.45it/s]\u001b[A\n",
      "1934it [22:20,  1.45it/s]\u001b[A\n",
      "1935it [22:20,  1.45it/s]\u001b[A\n",
      "1936it [22:21,  1.45it/s]\u001b[A\n",
      "1937it [22:22,  1.45it/s]\u001b[A\n",
      "1938it [22:22,  1.45it/s]\u001b[A\n",
      "1939it [22:23,  1.45it/s]\u001b[A\n",
      "1940it [22:24,  1.45it/s]\u001b[A\n",
      "1941it [22:25,  1.45it/s]\u001b[A\n",
      "1942it [22:25,  1.45it/s]\u001b[A\n",
      "1943it [22:26,  1.45it/s]\u001b[A\n",
      "1944it [22:27,  1.45it/s]\u001b[A\n",
      "1945it [22:27,  1.45it/s]\u001b[A\n",
      "1946it [22:28,  1.45it/s]\u001b[A\n",
      "1947it [22:29,  1.45it/s]\u001b[A\n",
      "1948it [22:29,  1.45it/s]\u001b[A\n",
      "1949it [22:30,  1.45it/s]\u001b[A\n",
      "1950it [22:31,  1.45it/s]\u001b[A\n",
      "1951it [22:31,  1.45it/s]\u001b[A\n",
      "1952it [22:32,  1.45it/s]\u001b[A\n",
      "1953it [22:33,  1.45it/s]\u001b[A\n",
      "1954it [22:33,  1.45it/s]\u001b[A\n",
      "1955it [22:34,  1.45it/s]\u001b[A\n",
      "1956it [22:35,  1.45it/s]\u001b[A\n",
      "1957it [22:36,  1.45it/s]\u001b[A\n",
      "1958it [22:36,  1.45it/s]\u001b[A\n",
      "1959it [22:37,  1.45it/s]\u001b[A\n",
      "1960it [22:38,  1.45it/s]\u001b[A\n",
      "1961it [22:38,  1.45it/s]\u001b[A\n",
      "1962it [22:39,  1.45it/s]\u001b[A\n",
      "1963it [22:40,  1.45it/s]\u001b[A\n",
      "1964it [22:40,  1.45it/s]\u001b[A\n",
      "1965it [22:41,  1.45it/s]\u001b[A\n",
      "1966it [22:42,  1.45it/s]\u001b[A\n",
      "1967it [22:42,  1.45it/s]\u001b[A\n",
      "1968it [22:43,  1.45it/s]\u001b[A\n",
      "1969it [22:44,  1.45it/s]\u001b[A\n",
      "1970it [22:45,  1.45it/s]\u001b[A\n",
      "1971it [22:45,  1.45it/s]\u001b[A\n",
      "1972it [22:46,  1.45it/s]\u001b[A\n",
      "1973it [22:47,  1.45it/s]\u001b[A\n",
      "1974it [22:47,  1.45it/s]\u001b[A\n",
      "1975it [22:48,  1.45it/s]\u001b[A\n",
      "1976it [22:49,  1.45it/s]\u001b[A\n",
      "1977it [22:49,  1.45it/s]\u001b[A\n",
      "1978it [22:50,  1.45it/s]\u001b[A\n",
      "1979it [22:51,  1.45it/s]\u001b[A\n",
      "1980it [22:51,  1.45it/s]\u001b[A\n",
      "1981it [22:52,  1.45it/s]\u001b[A\n",
      "1982it [22:53,  1.45it/s]\u001b[A\n",
      "1983it [22:53,  1.45it/s]\u001b[A\n",
      "1984it [22:54,  1.45it/s]\u001b[A\n",
      "1985it [22:55,  1.45it/s]\u001b[A\n",
      "1986it [22:56,  1.45it/s]\u001b[A\n",
      "1987it [22:56,  1.45it/s]\u001b[A\n",
      "1988it [22:57,  1.45it/s]\u001b[A\n",
      "1989it [22:58,  1.45it/s]\u001b[A\n",
      "1990it [22:58,  1.45it/s]\u001b[A\n",
      "1991it [22:59,  1.45it/s]\u001b[A\n",
      "1992it [23:00,  1.45it/s]\u001b[A\n",
      "1993it [23:00,  1.45it/s]\u001b[A\n",
      "1994it [23:01,  1.45it/s]\u001b[A\n",
      "1995it [23:02,  1.45it/s]\u001b[A\n",
      "1996it [23:02,  1.45it/s]\u001b[A\n",
      "1997it [23:03,  1.45it/s]\u001b[A\n",
      "1998it [23:04,  1.45it/s]\u001b[A\n",
      "1999it [23:04,  1.45it/s]\u001b[A\n",
      "2000it [23:05,  1.45it/s]\u001b[A\n",
      "2001it [23:06,  1.45it/s]\u001b[A\n",
      "2002it [23:07,  1.45it/s]\u001b[A\n",
      "2003it [23:07,  1.45it/s]\u001b[A\n",
      "2004it [23:08,  1.45it/s]\u001b[A\n",
      "2005it [23:09,  1.45it/s]\u001b[A\n",
      "2006it [23:09,  1.45it/s]\u001b[A\n",
      "2007it [23:10,  1.45it/s]\u001b[A\n",
      "2008it [23:11,  1.45it/s]\u001b[A\n",
      "2009it [23:11,  1.45it/s]\u001b[A\n",
      "2010it [23:12,  1.45it/s]\u001b[A\n",
      "2011it [23:13,  1.45it/s]\u001b[A\n",
      "2012it [23:13,  1.45it/s]\u001b[A\n",
      "2013it [23:14,  1.45it/s]\u001b[A\n",
      "2014it [23:15,  1.45it/s]\u001b[A\n",
      "2015it [23:16,  1.45it/s]\u001b[A\n",
      "2016it [23:16,  1.45it/s]\u001b[A\n",
      "2017it [23:17,  1.45it/s]\u001b[A\n",
      "2018it [23:18,  1.45it/s]\u001b[A\n",
      "2019it [23:18,  1.45it/s]\u001b[A\n",
      "2020it [23:19,  1.45it/s]\u001b[A\n",
      "2021it [23:20,  1.45it/s]\u001b[A\n",
      "2022it [23:20,  1.45it/s]\u001b[A\n",
      "2023it [23:21,  1.45it/s]\u001b[A\n",
      "2024it [23:22,  1.45it/s]\u001b[A\n",
      "2025it [23:22,  1.45it/s]\u001b[A\n",
      "2026it [23:23,  1.45it/s]\u001b[A\n",
      "2027it [23:24,  1.45it/s]\u001b[A\n",
      "2028it [23:24,  1.45it/s]\u001b[A\n",
      "2029it [23:25,  1.45it/s]\u001b[A\n",
      "2030it [23:26,  1.45it/s]\u001b[A\n",
      "2031it [23:27,  1.45it/s]\u001b[A\n",
      "2032it [23:27,  1.45it/s]\u001b[A\n",
      "2033it [23:28,  1.45it/s]\u001b[A\n",
      "2034it [23:29,  1.45it/s]\u001b[A\n",
      "2035it [23:29,  1.45it/s]\u001b[A\n",
      "2036it [23:30,  1.45it/s]\u001b[A\n",
      "2037it [23:31,  1.45it/s]\u001b[A\n",
      "2038it [23:31,  1.45it/s]\u001b[A\n",
      "2039it [23:32,  1.45it/s]\u001b[A\n",
      "2040it [23:33,  1.45it/s]\u001b[A\n",
      "2041it [23:33,  1.45it/s]\u001b[A\n",
      "2042it [23:34,  1.45it/s]\u001b[A\n",
      "2043it [23:35,  1.45it/s]\u001b[A\n",
      "2044it [23:35,  1.45it/s]\u001b[A\n",
      "2045it [23:36,  1.45it/s]\u001b[A\n",
      "2046it [23:37,  1.45it/s]\u001b[A\n",
      "2047it [23:38,  1.45it/s]\u001b[A\n",
      "2048it [23:38,  1.45it/s]\u001b[A\n",
      "2049it [23:39,  1.45it/s]\u001b[A\n",
      "2050it [23:40,  1.45it/s]\u001b[A\n",
      "2051it [23:40,  1.45it/s]\u001b[A\n",
      "2052it [23:41,  1.45it/s]\u001b[A\n",
      "2053it [23:42,  1.45it/s]\u001b[A\n",
      "2054it [23:42,  1.45it/s]\u001b[A\n",
      "2055it [23:43,  1.45it/s]\u001b[A\n",
      "2056it [23:44,  1.45it/s]\u001b[A\n",
      "2057it [23:44,  1.45it/s]\u001b[A\n",
      "2058it [23:45,  1.45it/s]\u001b[A\n",
      "2059it [23:46,  1.45it/s]\u001b[A\n",
      "2060it [23:47,  1.45it/s]\u001b[A\n",
      "2061it [23:47,  1.45it/s]\u001b[A\n",
      "2062it [23:48,  1.45it/s]\u001b[A\n",
      "2063it [23:49,  1.45it/s]\u001b[A\n",
      "2064it [23:49,  1.45it/s]\u001b[A\n",
      "2065it [23:50,  1.45it/s]\u001b[A\n",
      "2066it [23:51,  1.45it/s]\u001b[A\n",
      "2067it [23:51,  1.45it/s]\u001b[A\n",
      "2068it [23:52,  1.45it/s]\u001b[A\n",
      "2069it [23:53,  1.45it/s]\u001b[A\n",
      "2070it [23:53,  1.45it/s]\u001b[A\n",
      "2071it [23:54,  1.45it/s]\u001b[A\n",
      "2072it [23:55,  1.45it/s]\u001b[A\n",
      "2073it [23:55,  1.45it/s]\u001b[A\n",
      "2074it [23:56,  1.45it/s]\u001b[A\n",
      "2075it [23:57,  1.45it/s]\u001b[A\n",
      "2076it [23:58,  1.45it/s]\u001b[A\n",
      "2077it [23:58,  1.45it/s]\u001b[A\n",
      "2078it [23:59,  1.45it/s]\u001b[A\n",
      "2079it [24:00,  1.45it/s]\u001b[A\n",
      "2080it [24:00,  1.45it/s]\u001b[A\n",
      "2081it [24:01,  1.45it/s]\u001b[A\n",
      "2082it [24:02,  1.45it/s]\u001b[A\n",
      "2083it [24:02,  1.45it/s]\u001b[A\n",
      "2084it [24:03,  1.45it/s]\u001b[A\n",
      "2085it [24:04,  1.45it/s]\u001b[A\n",
      "2086it [24:04,  1.45it/s]\u001b[A\n",
      "2087it [24:05,  1.45it/s]\u001b[A\n",
      "2088it [24:06,  1.45it/s]\u001b[A\n",
      "2089it [24:06,  1.45it/s]\u001b[A\n",
      "2090it [24:07,  1.45it/s]\u001b[A\n",
      "2091it [24:08,  1.45it/s]\u001b[A\n",
      "2092it [24:09,  1.45it/s]\u001b[A\n",
      "2093it [24:09,  1.45it/s]\u001b[A\n",
      "2094it [24:10,  1.45it/s]\u001b[A\n",
      "2095it [24:11,  1.45it/s]\u001b[A\n",
      "2096it [24:11,  1.45it/s]\u001b[A\n",
      "2097it [24:12,  1.45it/s]\u001b[A\n",
      "2098it [24:13,  1.45it/s]\u001b[A\n",
      "2099it [24:13,  1.45it/s]\u001b[A\n",
      "2100it [24:14,  1.45it/s]\u001b[A\n",
      "2101it [24:15,  1.45it/s]\u001b[A\n",
      "2102it [24:15,  1.45it/s]\u001b[A\n",
      "2103it [24:16,  1.45it/s]\u001b[A\n",
      "2104it [24:17,  1.45it/s]\u001b[A\n",
      "2105it [24:18,  1.45it/s]\u001b[A\n",
      "2106it [24:18,  1.45it/s]\u001b[A\n",
      "2107it [24:19,  1.45it/s]\u001b[A\n",
      "2108it [24:20,  1.45it/s]\u001b[A\n",
      "2109it [24:20,  1.45it/s]\u001b[A\n",
      "2110it [24:21,  1.45it/s]\u001b[A\n",
      "2111it [24:22,  1.45it/s]\u001b[A\n",
      "2112it [24:22,  1.45it/s]\u001b[A\n",
      "2113it [24:23,  1.45it/s]\u001b[A\n",
      "2114it [24:24,  1.45it/s]\u001b[A\n",
      "2115it [24:24,  1.45it/s]\u001b[A\n",
      "2116it [24:25,  1.45it/s]\u001b[A\n",
      "2117it [24:26,  1.45it/s]\u001b[A\n",
      "2118it [24:26,  1.45it/s]\u001b[A\n",
      "2119it [24:27,  1.45it/s]\u001b[A\n",
      "2120it [24:28,  1.45it/s]\u001b[A\n",
      "2121it [24:29,  1.45it/s]\u001b[A\n",
      "2122it [24:29,  1.45it/s]\u001b[A\n",
      "2123it [24:30,  1.45it/s]\u001b[A\n",
      "2124it [24:31,  1.45it/s]\u001b[A\n",
      "2125it [24:31,  1.45it/s]\u001b[A\n",
      "2126it [24:32,  1.45it/s]\u001b[A\n",
      "2127it [24:33,  1.45it/s]\u001b[A\n",
      "2128it [24:33,  1.45it/s]\u001b[A\n",
      "2129it [24:34,  1.45it/s]\u001b[A\n",
      "2130it [24:35,  1.45it/s]\u001b[A\n",
      "2131it [24:35,  1.45it/s]\u001b[A\n",
      "2132it [24:36,  1.45it/s]\u001b[A\n",
      "2133it [24:37,  1.45it/s]\u001b[A\n",
      "2134it [24:37,  1.45it/s]\u001b[A\n",
      "2135it [24:38,  1.45it/s]\u001b[A\n",
      "2136it [24:39,  1.45it/s]\u001b[A\n",
      "2137it [24:40,  1.45it/s]\u001b[A\n",
      "2138it [24:40,  1.45it/s]\u001b[A\n",
      "2139it [24:41,  1.45it/s]\u001b[A\n",
      "2140it [24:42,  1.45it/s]\u001b[A\n",
      "2141it [24:42,  1.45it/s]\u001b[A\n",
      "2142it [24:43,  1.45it/s]\u001b[A\n",
      "2143it [24:44,  1.45it/s]\u001b[A\n",
      "2144it [24:44,  1.45it/s]\u001b[A\n",
      "2145it [24:45,  1.45it/s]\u001b[A\n",
      "2146it [24:46,  1.45it/s]\u001b[A\n",
      "2147it [24:46,  1.45it/s]\u001b[A\n",
      "2148it [24:47,  1.45it/s]\u001b[A\n",
      "2149it [24:48,  1.45it/s]\u001b[A\n",
      "2150it [24:48,  1.45it/s]\u001b[A\n",
      "2151it [24:49,  1.45it/s]\u001b[A\n",
      "2152it [24:50,  1.45it/s]\u001b[A\n",
      "2153it [24:51,  1.45it/s]\u001b[A\n",
      "2154it [24:51,  1.45it/s]\u001b[A\n",
      "2155it [24:52,  1.45it/s]\u001b[A\n",
      "2156it [24:53,  1.45it/s]\u001b[A\n",
      "2157it [24:53,  1.45it/s]\u001b[A\n",
      "2158it [24:54,  1.45it/s]\u001b[A\n",
      "2159it [24:55,  1.45it/s]\u001b[A\n",
      "2160it [24:55,  1.45it/s]\u001b[A\n",
      "2161it [24:56,  1.45it/s]\u001b[A\n",
      "2162it [24:57,  1.45it/s]\u001b[A\n",
      "2163it [24:57,  1.45it/s]\u001b[A\n",
      "2164it [24:58,  1.45it/s]\u001b[A\n",
      "2165it [24:59,  1.45it/s]\u001b[A\n",
      "2166it [25:00,  1.45it/s]\u001b[A\n",
      "2167it [25:00,  1.45it/s]\u001b[A\n",
      "2168it [25:01,  1.45it/s]\u001b[A\n",
      "2169it [25:02,  1.45it/s]\u001b[A\n",
      "2170it [25:02,  1.45it/s]\u001b[A\n",
      "2171it [25:03,  1.45it/s]\u001b[A\n",
      "2172it [25:04,  1.45it/s]\u001b[A\n",
      "2173it [25:04,  1.45it/s]\u001b[A\n",
      "2174it [25:05,  1.45it/s]\u001b[A\n",
      "2175it [25:06,  1.45it/s]\u001b[A\n",
      "2176it [25:06,  1.45it/s]\u001b[A\n",
      "2177it [25:07,  1.45it/s]\u001b[A\n",
      "2178it [25:08,  1.45it/s]\u001b[A\n",
      "2179it [25:08,  1.45it/s]\u001b[A\n",
      "2180it [25:09,  1.45it/s]\u001b[A\n",
      "2181it [25:10,  1.45it/s]\u001b[A\n",
      "2182it [25:11,  1.45it/s]\u001b[A\n",
      "2183it [25:11,  1.45it/s]\u001b[A\n",
      "2184it [25:12,  1.45it/s]\u001b[A\n",
      "2185it [25:13,  1.45it/s]\u001b[A\n",
      "2186it [25:13,  1.45it/s]\u001b[A\n",
      "2187it [25:14,  1.45it/s]\u001b[A\n",
      "2188it [25:15,  1.45it/s]\u001b[A\n",
      "2189it [25:15,  1.45it/s]\u001b[A\n",
      "2190it [25:16,  1.45it/s]\u001b[A\n",
      "2191it [25:17,  1.45it/s]\u001b[A\n",
      "2192it [25:17,  1.45it/s]\u001b[A\n",
      "2193it [25:18,  1.45it/s]\u001b[A\n",
      "2194it [25:19,  1.45it/s]\u001b[A\n",
      "2195it [25:19,  1.45it/s]\u001b[A\n",
      "2196it [25:20,  1.45it/s]\u001b[A\n",
      "2197it [25:21,  1.45it/s]\u001b[A\n",
      "2198it [25:22,  1.45it/s]\u001b[A\n",
      "2199it [25:22,  1.45it/s]\u001b[A\n",
      "2200it [25:23,  1.45it/s]\u001b[A\n",
      "2201it [25:24,  1.45it/s]\u001b[A\n",
      "2202it [25:24,  1.45it/s]\u001b[A\n",
      "2203it [25:25,  1.45it/s]\u001b[A\n",
      "2204it [25:26,  1.45it/s]\u001b[A\n",
      "2205it [25:26,  1.45it/s]\u001b[A\n",
      "2206it [25:27,  1.45it/s]\u001b[A\n",
      "2207it [25:28,  1.45it/s]\u001b[A\n",
      "2208it [25:28,  1.45it/s]\u001b[A\n",
      "2209it [25:29,  1.45it/s]\u001b[A\n",
      "2210it [25:30,  1.45it/s]\u001b[A\n",
      "2211it [25:31,  1.45it/s]\u001b[A\n",
      "2212it [25:31,  1.45it/s]\u001b[A\n",
      "2213it [25:32,  1.45it/s]\u001b[A\n",
      "2214it [25:33,  1.45it/s]\u001b[A\n",
      "2215it [25:33,  1.45it/s]\u001b[A\n",
      "2216it [25:34,  1.45it/s]\u001b[A\n",
      "2217it [25:35,  1.45it/s]\u001b[A\n",
      "2218it [25:35,  1.45it/s]\u001b[A\n",
      "2219it [25:36,  1.45it/s]\u001b[A\n",
      "2220it [25:37,  1.45it/s]\u001b[A\n",
      "2221it [25:37,  1.45it/s]\u001b[A\n",
      "2222it [25:38,  1.45it/s]\u001b[A\n",
      "2223it [25:39,  1.45it/s]\u001b[A\n",
      "2224it [25:39,  1.45it/s]\u001b[A\n",
      "2225it [25:40,  1.45it/s]\u001b[A\n",
      "2226it [25:41,  1.45it/s]\u001b[A\n",
      "2227it [25:42,  1.45it/s]\u001b[A\n",
      "2228it [25:42,  1.45it/s]\u001b[A\n",
      "2229it [25:43,  1.45it/s]\u001b[A\n",
      "2230it [25:44,  1.45it/s]\u001b[A\n",
      "2231it [25:44,  1.45it/s]\u001b[A\n",
      "2232it [25:45,  1.45it/s]\u001b[A\n",
      "2233it [25:46,  1.45it/s]\u001b[A\n",
      "2234it [25:46,  1.45it/s]\u001b[A\n",
      "2235it [25:47,  1.45it/s]\u001b[A\n",
      "2236it [25:48,  1.45it/s]\u001b[A\n",
      "2237it [25:48,  1.45it/s]\u001b[A\n",
      "2238it [25:49,  1.45it/s]\u001b[A\n",
      "2239it [25:50,  1.45it/s]\u001b[A\n",
      "2240it [25:50,  1.45it/s]\u001b[A\n",
      "2241it [25:51,  1.45it/s]\u001b[A\n",
      "2242it [25:52,  1.45it/s]\u001b[A\n",
      "2243it [25:53,  1.45it/s]\u001b[A\n",
      "2244it [25:53,  1.45it/s]\u001b[A\n",
      "2245it [25:54,  1.45it/s]\u001b[A\n",
      "2246it [25:55,  1.45it/s]\u001b[A\n",
      "2247it [25:55,  1.45it/s]\u001b[A\n",
      "2248it [25:56,  1.45it/s]\u001b[A\n",
      "2249it [25:57,  1.45it/s]\u001b[A\n",
      "2250it [25:57,  1.45it/s]\u001b[A\n",
      "2251it [25:58,  1.45it/s]\u001b[A\n",
      "2252it [25:59,  1.45it/s]\u001b[A\n",
      "2253it [25:59,  1.45it/s]\u001b[A\n",
      "2254it [26:00,  1.45it/s]\u001b[A\n",
      "2255it [26:01,  1.45it/s]\u001b[A\n",
      "2256it [26:01,  1.45it/s]\u001b[A\n",
      "2257it [26:02,  1.45it/s]\u001b[A\n",
      "2258it [26:03,  1.45it/s]\u001b[A\n",
      "2259it [26:04,  1.45it/s]\u001b[A\n",
      "2260it [26:04,  1.45it/s]\u001b[A\n",
      "2261it [26:05,  1.45it/s]\u001b[A\n",
      "2262it [26:06,  1.45it/s]\u001b[A\n",
      "2263it [26:06,  1.45it/s]\u001b[A\n",
      "2264it [26:07,  1.45it/s]\u001b[A\n",
      "2265it [26:08,  1.45it/s]\u001b[A\n",
      "2266it [26:08,  1.45it/s]\u001b[A\n",
      "2267it [26:09,  1.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2268it [26:10,  1.45it/s]\u001b[A\n",
      "2269it [26:10,  1.45it/s]\u001b[A\n",
      "2270it [26:11,  1.45it/s]\u001b[A\n",
      "2271it [26:12,  1.45it/s]\u001b[A\n",
      "2272it [26:13,  1.45it/s]\u001b[A\n",
      "2273it [26:13,  1.45it/s]\u001b[A\n",
      "2274it [26:14,  1.45it/s]\u001b[A\n",
      "2275it [26:15,  1.45it/s]\u001b[A\n",
      "2276it [26:15,  1.45it/s]\u001b[A\n",
      "2277it [26:16,  1.45it/s]\u001b[A\n",
      "2278it [26:17,  1.45it/s]\u001b[A\n",
      "2279it [26:17,  1.45it/s]\u001b[A\n",
      "2280it [26:18,  1.45it/s]\u001b[A\n",
      "2281it [26:19,  1.45it/s]\u001b[A\n",
      "2282it [26:19,  1.45it/s]\u001b[A\n",
      "2283it [26:20,  1.45it/s]\u001b[A\n",
      "2284it [26:21,  1.45it/s]\u001b[A\n",
      "2285it [26:21,  1.45it/s]\u001b[A\n",
      "2286it [26:22,  1.45it/s]\u001b[A\n",
      "2287it [26:23,  1.45it/s]\u001b[A\n",
      "2288it [26:24,  1.45it/s]\u001b[A\n",
      "2289it [26:24,  1.45it/s]\u001b[A\n",
      "2290it [26:25,  1.45it/s]\u001b[A\n",
      "2291it [26:26,  1.45it/s]\u001b[A\n",
      "2292it [26:26,  1.45it/s]\u001b[A\n",
      "2293it [26:27,  1.45it/s]\u001b[A\n",
      "2294it [26:28,  1.45it/s]\u001b[A\n",
      "2295it [26:28,  1.45it/s]\u001b[A\n",
      "2296it [26:29,  1.45it/s]\u001b[A\n",
      "2297it [26:30,  1.45it/s]\u001b[A\n",
      "2298it [26:30,  1.45it/s]\u001b[A\n",
      "2299it [26:31,  1.45it/s]\u001b[A\n",
      "2300it [26:32,  1.45it/s]\u001b[A\n",
      "2301it [26:32,  1.45it/s]\u001b[A\n",
      "2302it [26:33,  1.45it/s]\u001b[A\n",
      "2303it [26:34,  1.45it/s]\u001b[A\n",
      "2304it [26:35,  1.45it/s]\u001b[A\n",
      "2305it [26:35,  1.45it/s]\u001b[A\n",
      "2306it [26:36,  1.45it/s]\u001b[A\n",
      "2307it [26:37,  1.45it/s]\u001b[A\n",
      "2308it [26:37,  1.45it/s]\u001b[A\n",
      "2309it [26:38,  1.45it/s]\u001b[A\n",
      "2310it [26:39,  1.45it/s]\u001b[A\n",
      "2311it [26:39,  1.45it/s]\u001b[A\n",
      "2312it [26:40,  1.45it/s]\u001b[A\n",
      "2313it [26:41,  1.45it/s]\u001b[A\n",
      "2314it [26:41,  1.45it/s]\u001b[A\n",
      "2315it [26:42,  1.45it/s]\u001b[A\n",
      "2316it [26:43,  1.45it/s]\u001b[A\n",
      "2317it [26:43,  1.45it/s]\u001b[A\n",
      "2318it [26:44,  1.45it/s]\u001b[A\n",
      "2319it [26:45,  1.45it/s]\u001b[A\n",
      "2320it [26:46,  1.45it/s]\u001b[A\n",
      "2321it [26:46,  1.45it/s]\u001b[A\n",
      "2322it [26:47,  1.45it/s]\u001b[A\n",
      "2323it [26:48,  1.45it/s]\u001b[A\n",
      "2324it [26:48,  1.45it/s]\u001b[A\n",
      "2325it [26:49,  1.45it/s]\u001b[A\n",
      "2326it [26:50,  1.45it/s]\u001b[A\n",
      "2327it [26:50,  1.45it/s]\u001b[A\n",
      "2328it [26:51,  1.45it/s]\u001b[A\n",
      "2329it [26:52,  1.45it/s]\u001b[A\n",
      "2330it [26:52,  1.45it/s]\u001b[A\n",
      "2331it [26:53,  1.45it/s]\u001b[A\n",
      "2332it [26:54,  1.45it/s]\u001b[A\n",
      "2333it [26:55,  1.45it/s]\u001b[A\n",
      "2334it [26:55,  1.45it/s]\u001b[A\n",
      "2335it [26:56,  1.45it/s]\u001b[A\n",
      "2336it [26:57,  1.45it/s]\u001b[A\n",
      "2337it [26:57,  1.45it/s]\u001b[A\n",
      "2338it [26:58,  1.45it/s]\u001b[A\n",
      "2339it [26:59,  1.45it/s]\u001b[A\n",
      "2340it [26:59,  1.45it/s]\u001b[A\n",
      "2341it [27:00,  1.45it/s]\u001b[A\n",
      "2342it [27:01,  1.45it/s]\u001b[A\n",
      "2343it [27:01,  1.45it/s]\u001b[A\n",
      "2344it [27:02,  1.45it/s]\u001b[A\n",
      "2345it [27:03,  1.45it/s]\u001b[A\n",
      "2346it [27:03,  1.45it/s]\u001b[A\n",
      "2347it [27:04,  1.45it/s]\u001b[A\n",
      "2348it [27:05,  1.45it/s]\u001b[A\n",
      "2349it [27:06,  1.45it/s]\u001b[A\n",
      "2350it [27:06,  1.45it/s]\u001b[A\n",
      "2351it [27:07,  1.45it/s]\u001b[A\n",
      "2352it [27:08,  1.45it/s]\u001b[A\n",
      "2353it [27:08,  1.45it/s]\u001b[A\n",
      "2354it [27:09,  1.45it/s]\u001b[A\n",
      "2355it [27:10,  1.45it/s]\u001b[A\n",
      "2356it [27:10,  1.45it/s]\u001b[A\n",
      "2357it [27:11,  1.45it/s]\u001b[A\n",
      "2358it [27:12,  1.45it/s]\u001b[A\n",
      "2359it [27:12,  1.45it/s]\u001b[A\n",
      "2360it [27:13,  1.45it/s]\u001b[A\n",
      "2361it [27:14,  1.45it/s]\u001b[A\n",
      "2362it [27:14,  1.45it/s]\u001b[A\n",
      "2363it [27:15,  1.45it/s]\u001b[A\n",
      "2364it [27:16,  1.45it/s]\u001b[A\n",
      "2365it [27:17,  1.45it/s]\u001b[A\n",
      "2366it [27:17,  1.45it/s]\u001b[A\n",
      "2367it [27:18,  1.45it/s]\u001b[A\n",
      "2368it [27:19,  1.45it/s]\u001b[A\n",
      "2369it [27:19,  1.45it/s]\u001b[A\n",
      "2370it [27:20,  1.45it/s]\u001b[A\n",
      "2371it [27:21,  1.45it/s]\u001b[A\n",
      "2372it [27:21,  1.45it/s]\u001b[A\n",
      "2373it [27:22,  1.45it/s]\u001b[A\n",
      "2374it [27:23,  1.45it/s]\u001b[A\n",
      "2375it [27:23,  1.45it/s]\u001b[A\n",
      "2376it [27:24,  1.45it/s]\u001b[A\n",
      "2377it [27:25,  1.45it/s]\u001b[A\n",
      "2378it [27:25,  1.45it/s]\u001b[A\n",
      "2379it [27:26,  1.45it/s]\u001b[A\n",
      "2380it [27:27,  1.45it/s]\u001b[A\n",
      "2381it [27:28,  1.45it/s]\u001b[A\n",
      "2382it [27:28,  1.45it/s]\u001b[A\n",
      "2383it [27:29,  1.45it/s]\u001b[A\n",
      "2384it [27:30,  1.45it/s]\u001b[A\n",
      "2385it [27:30,  1.45it/s]\u001b[A\n",
      "2386it [27:31,  1.45it/s]\u001b[A\n",
      "2387it [27:32,  1.45it/s]\u001b[A\n",
      "2388it [27:32,  1.45it/s]\u001b[A\n",
      "2389it [27:33,  1.45it/s]\u001b[A\n",
      "2390it [27:34,  1.45it/s]\u001b[A\n",
      "2391it [27:34,  1.45it/s]\u001b[A\n",
      "2392it [27:35,  1.45it/s]\u001b[A\n",
      "2393it [27:36,  1.45it/s]\u001b[A\n",
      "2394it [27:37,  1.45it/s]\u001b[A\n",
      "2395it [27:37,  1.45it/s]\u001b[A\n",
      "2396it [27:38,  1.45it/s]\u001b[A\n",
      "2397it [27:39,  1.45it/s]\u001b[A\n",
      "2398it [27:39,  1.45it/s]\u001b[A\n",
      "2399it [27:40,  1.45it/s]\u001b[A\n",
      "2400it [27:41,  1.45it/s]\u001b[A\n",
      "2401it [27:41,  1.45it/s]\u001b[A\n",
      "2402it [27:42,  1.45it/s]\u001b[A\n",
      "2403it [27:43,  1.45it/s]\u001b[A\n",
      "2404it [27:43,  1.45it/s]\u001b[A\n",
      "2405it [27:44,  1.45it/s]\u001b[A\n",
      "2406it [27:45,  1.45it/s]\u001b[A\n",
      "2407it [27:45,  1.45it/s]\u001b[A\n",
      "2408it [27:46,  1.45it/s]\u001b[A\n",
      "2409it [27:47,  1.45it/s]\u001b[A\n",
      "2410it [27:48,  1.45it/s]\u001b[A\n",
      "2411it [27:48,  1.45it/s]\u001b[A\n",
      "2412it [27:49,  1.45it/s]\u001b[A\n",
      "2413it [27:50,  1.45it/s]\u001b[A\n",
      "2414it [27:50,  1.45it/s]\u001b[A\n",
      "2415it [27:51,  1.45it/s]\u001b[A\n",
      "2416it [27:52,  1.45it/s]\u001b[A\n",
      "2417it [27:52,  1.45it/s]\u001b[A\n",
      "2418it [27:53,  1.45it/s]\u001b[A\n",
      "2419it [27:54,  1.45it/s]\u001b[A\n",
      "2420it [27:54,  1.45it/s]\u001b[A\n",
      "2421it [27:55,  1.45it/s]\u001b[A\n",
      "2422it [27:56,  1.45it/s]\u001b[A\n",
      "2423it [27:56,  1.45it/s]\u001b[A\n",
      "2424it [27:57,  1.45it/s]\u001b[A\n",
      "2425it [27:58,  1.45it/s]\u001b[A\n",
      "2426it [27:59,  1.45it/s]\u001b[A\n",
      "2427it [27:59,  1.45it/s]\u001b[A\n",
      "2428it [28:00,  1.45it/s]\u001b[A\n",
      "2429it [28:01,  1.45it/s]\u001b[A\n",
      "2430it [28:01,  1.45it/s]\u001b[A\n",
      "2431it [28:02,  1.45it/s]\u001b[A\n",
      "2432it [28:03,  1.45it/s]\u001b[A\n",
      "2433it [28:03,  1.45it/s]\u001b[A\n",
      "2434it [28:04,  1.45it/s]\u001b[A\n",
      "2435it [28:05,  1.45it/s]\u001b[A\n",
      "2436it [28:05,  1.45it/s]\u001b[A\n",
      "2437it [28:06,  1.45it/s]\u001b[A\n",
      "2438it [28:07,  1.45it/s]\u001b[A\n",
      "2439it [28:08,  1.45it/s]\u001b[A\n",
      "2440it [28:08,  1.45it/s]\u001b[A\n",
      "2441it [28:09,  1.45it/s]\u001b[A\n",
      "2442it [28:10,  1.45it/s]\u001b[A\n",
      "2443it [28:10,  1.45it/s]\u001b[A\n",
      "2444it [28:11,  1.45it/s]\u001b[A\n",
      "2445it [28:12,  1.45it/s]\u001b[A\n",
      "2446it [28:12,  1.45it/s]\u001b[A\n",
      "2447it [28:13,  1.45it/s]\u001b[A\n",
      "2448it [28:14,  1.45it/s]\u001b[A\n",
      "2449it [28:14,  1.45it/s]\u001b[A\n",
      "2450it [28:15,  1.45it/s]\u001b[A\n",
      "2451it [28:16,  1.45it/s]\u001b[A\n",
      "2452it [28:16,  1.45it/s]\u001b[A\n",
      "2453it [28:17,  1.45it/s]\u001b[A\n",
      "2454it [28:18,  1.45it/s]\u001b[A\n",
      "2455it [28:19,  1.45it/s]\u001b[A\n",
      "2456it [28:19,  1.45it/s]\u001b[A\n",
      "2457it [28:20,  1.45it/s]\u001b[A\n",
      "2458it [28:21,  1.45it/s]\u001b[A\n",
      "2459it [28:21,  1.45it/s]\u001b[A\n",
      "2460it [28:22,  1.45it/s]\u001b[A\n",
      "2461it [28:23,  1.45it/s]\u001b[A\n",
      "2462it [28:23,  1.45it/s]\u001b[A\n",
      "2463it [28:24,  1.45it/s]\u001b[A\n",
      "2464it [28:25,  1.45it/s]\u001b[A\n",
      "2465it [28:25,  1.45it/s]\u001b[A\n",
      "2466it [28:26,  1.45it/s]\u001b[A\n",
      "2467it [28:27,  1.45it/s]\u001b[A\n",
      "2468it [28:27,  1.45it/s]\u001b[A\n",
      "2469it [28:28,  1.45it/s]\u001b[A\n",
      "2470it [28:29,  1.45it/s]\u001b[A\n",
      "2471it [28:30,  1.45it/s]\u001b[A\n",
      "2472it [28:30,  1.45it/s]\u001b[A\n",
      "2473it [28:31,  1.45it/s]\u001b[A\n",
      "2474it [28:32,  1.45it/s]\u001b[A\n",
      "2475it [28:32,  1.45it/s]\u001b[A\n",
      "2476it [28:33,  1.45it/s]\u001b[A\n",
      "2477it [28:34,  1.45it/s]\u001b[A\n",
      "2478it [28:34,  1.45it/s]\u001b[A\n",
      "2479it [28:35,  1.45it/s]\u001b[A\n",
      "2480it [28:36,  1.45it/s]\u001b[A\n",
      "2481it [28:36,  1.45it/s]\u001b[A\n",
      "2482it [28:37,  1.45it/s]\u001b[A\n",
      "2483it [28:38,  1.45it/s]\u001b[A\n",
      "2484it [28:38,  1.45it/s]\u001b[A\n",
      "2485it [28:39,  1.45it/s]\u001b[A\n",
      "2486it [28:40,  1.45it/s]\u001b[A\n",
      "2487it [28:41,  1.45it/s]\u001b[A\n",
      "2488it [28:41,  1.45it/s]\u001b[A\n",
      "2489it [28:42,  1.45it/s]\u001b[A\n",
      "2490it [28:43,  1.45it/s]\u001b[A\n",
      "2491it [28:43,  1.45it/s]\u001b[A\n",
      "2492it [28:44,  1.45it/s]\u001b[A\n",
      "2493it [28:45,  1.45it/s]\u001b[A\n",
      "2494it [28:45,  1.45it/s]\u001b[A\n",
      "2495it [28:46,  1.45it/s]\u001b[A\n",
      "2496it [28:47,  1.45it/s]\u001b[A\n",
      "2497it [28:47,  1.45it/s]\u001b[A\n",
      "2498it [28:48,  1.45it/s]\u001b[A\n",
      "2499it [28:49,  1.45it/s]\u001b[A\n",
      "2500it [28:50,  1.45it/s]\u001b[A\n",
      "2501it [28:50,  1.45it/s]\u001b[A\n",
      "2502it [28:51,  1.45it/s]\u001b[A\n",
      "2503it [28:52,  1.45it/s]\u001b[A\n",
      "2504it [28:52,  1.45it/s]\u001b[A\n",
      "2505it [28:53,  1.45it/s]\u001b[A\n",
      "2506it [28:54,  1.45it/s]\u001b[A\n",
      "2507it [28:54,  1.45it/s]\u001b[A\n",
      "2508it [28:55,  1.45it/s]\u001b[A\n",
      "2509it [28:56,  1.45it/s]\u001b[A\n",
      "2510it [28:56,  1.45it/s]\u001b[A\n",
      "2511it [28:57,  1.45it/s]\u001b[A\n",
      "2512it [28:58,  1.45it/s]\u001b[A\n",
      "2513it [28:58,  1.45it/s]\u001b[A\n",
      "2514it [28:59,  1.45it/s]\u001b[A\n",
      "2515it [29:00,  1.45it/s]\u001b[A\n",
      "2516it [29:01,  1.45it/s]\u001b[A\n",
      "2517it [29:01,  1.45it/s]\u001b[A\n",
      "2518it [29:02,  1.45it/s]\u001b[A\n",
      "2519it [29:03,  1.46it/s]\u001b[A\n",
      "2520it [29:03,  1.45it/s]\u001b[A\n",
      "2521it [29:04,  1.45it/s]\u001b[A\n",
      "2522it [29:05,  1.45it/s]\u001b[A\n",
      "2523it [29:05,  1.45it/s]\u001b[A\n",
      "2524it [29:06,  1.45it/s]\u001b[A\n",
      "2525it [29:07,  1.45it/s]\u001b[A\n",
      "2526it [29:07,  1.45it/s]\u001b[A\n",
      "2527it [29:08,  1.45it/s]\u001b[A\n",
      "2528it [29:09,  1.45it/s]\u001b[A\n",
      "2529it [29:09,  1.45it/s]\u001b[A\n",
      "2530it [29:10,  1.45it/s]\u001b[A\n",
      "2531it [29:11,  1.45it/s]\u001b[A\n",
      "2532it [29:12,  1.45it/s]\u001b[A\n",
      "2533it [29:12,  1.45it/s]\u001b[A\n",
      "2534it [29:13,  1.45it/s]\u001b[A\n",
      "2535it [29:14,  1.45it/s]\u001b[A\n",
      "2536it [29:14,  1.45it/s]\u001b[A\n",
      "2537it [29:15,  1.45it/s]\u001b[A\n",
      "2538it [29:16,  1.45it/s]\u001b[A\n",
      "2539it [29:16,  1.45it/s]\u001b[A\n",
      "2540it [29:17,  1.45it/s]\u001b[A\n",
      "2541it [29:18,  1.45it/s]\u001b[A\n",
      "2542it [29:18,  1.45it/s]\u001b[A\n",
      "2543it [29:19,  1.45it/s]\u001b[A\n",
      "2544it [29:20,  1.45it/s]\u001b[A\n",
      "2545it [29:20,  1.45it/s]\u001b[A\n",
      "2546it [29:21,  1.45it/s]\u001b[A\n",
      "2547it [29:22,  1.45it/s]\u001b[A\n",
      "2548it [29:23,  1.45it/s]\u001b[A\n",
      "2549it [29:23,  1.45it/s]\u001b[A\n",
      "2550it [29:24,  1.45it/s]\u001b[A\n",
      "2551it [29:25,  1.45it/s]\u001b[A\n",
      "2552it [29:25,  1.45it/s]\u001b[A\n",
      "2553it [29:26,  1.45it/s]\u001b[A\n",
      "2554it [29:27,  1.45it/s]\u001b[A\n",
      "2555it [29:27,  1.45it/s]\u001b[A\n",
      "2556it [29:28,  1.45it/s]\u001b[A\n",
      "2557it [29:29,  1.45it/s]\u001b[A\n",
      "2558it [29:29,  1.45it/s]\u001b[A\n",
      "2559it [29:30,  1.45it/s]\u001b[A\n",
      "2560it [29:31,  1.45it/s]\u001b[A\n",
      "2561it [29:32,  1.45it/s]\u001b[A\n",
      "2562it [29:32,  1.45it/s]\u001b[A\n",
      "2563it [29:33,  1.45it/s]\u001b[A\n",
      "2564it [29:34,  1.45it/s]\u001b[A\n",
      "2565it [29:34,  1.45it/s]\u001b[A\n",
      "2566it [29:35,  1.45it/s]\u001b[A\n",
      "2567it [29:36,  1.45it/s]\u001b[A\n",
      "2568it [29:36,  1.45it/s]\u001b[A\n",
      "2569it [29:37,  1.45it/s]\u001b[A\n",
      "2570it [29:38,  1.45it/s]\u001b[A\n",
      "2571it [29:38,  1.45it/s]\u001b[A\n",
      "2572it [29:39,  1.45it/s]\u001b[A\n",
      "2573it [29:40,  1.45it/s]\u001b[A\n",
      "2574it [29:40,  1.45it/s]\u001b[A\n",
      "2575it [29:41,  1.45it/s]\u001b[A\n",
      "2576it [29:42,  1.45it/s]\u001b[A\n",
      "2577it [29:43,  1.45it/s]\u001b[A\n",
      "2578it [29:43,  1.45it/s]\u001b[A\n",
      "2579it [29:44,  1.45it/s]\u001b[A\n",
      "2580it [29:45,  1.45it/s]\u001b[A\n",
      "2581it [29:45,  1.45it/s]\u001b[A\n",
      "2582it [29:46,  1.45it/s]\u001b[A\n",
      "2583it [29:47,  1.45it/s]\u001b[A\n",
      "2584it [29:47,  1.45it/s]\u001b[A\n",
      "2585it [29:48,  1.45it/s]\u001b[A\n",
      "2586it [29:49,  1.45it/s]\u001b[A\n",
      "2587it [29:49,  1.45it/s]\u001b[A\n",
      "2588it [29:50,  1.45it/s]\u001b[A\n",
      "2589it [29:51,  1.45it/s]\u001b[A\n",
      "2590it [29:51,  1.45it/s]\u001b[A\n",
      "2591it [29:52,  1.45it/s]\u001b[A\n",
      "2592it [29:53,  1.45it/s]\u001b[A\n",
      "2593it [29:54,  1.45it/s]\u001b[A\n",
      "2594it [29:54,  1.45it/s]\u001b[A\n",
      "2595it [29:55,  1.45it/s]\u001b[A\n",
      "2596it [29:56,  1.45it/s]\u001b[A\n",
      "2597it [29:56,  1.45it/s]\u001b[A\n",
      "2598it [29:57,  1.45it/s]\u001b[A\n",
      "2599it [29:58,  1.45it/s]\u001b[A\n",
      "2600it [29:58,  1.45it/s]\u001b[A\n",
      "2601it [29:59,  1.45it/s]\u001b[A\n",
      "2602it [30:00,  1.45it/s]\u001b[A\n",
      "2603it [30:00,  1.45it/s]\u001b[A\n",
      "2604it [30:01,  1.45it/s]\u001b[A\n",
      "2605it [30:02,  1.45it/s]\u001b[A\n",
      "2606it [30:02,  1.45it/s]\u001b[A\n",
      "2607it [30:03,  1.45it/s]\u001b[A\n",
      "2608it [30:04,  1.45it/s]\u001b[A\n",
      "2609it [30:05,  1.45it/s]\u001b[A\n",
      "2610it [30:05,  1.45it/s]\u001b[A\n",
      "2611it [30:06,  1.45it/s]\u001b[A\n",
      "2612it [30:07,  1.45it/s]\u001b[A\n",
      "2613it [30:07,  1.45it/s]\u001b[A\n",
      "2614it [30:08,  1.45it/s]\u001b[A\n",
      "2615it [30:09,  1.45it/s]\u001b[A\n",
      "2616it [30:09,  1.45it/s]\u001b[A\n",
      "2617it [30:10,  1.45it/s]\u001b[A\n",
      "2618it [30:11,  1.45it/s]\u001b[A\n",
      "2619it [30:11,  1.45it/s]\u001b[A\n",
      "2620it [30:12,  1.45it/s]\u001b[A\n",
      "2621it [30:13,  1.45it/s]\u001b[A\n",
      "2622it [30:14,  1.45it/s]\u001b[A\n",
      "2623it [30:14,  1.45it/s]\u001b[A\n",
      "2624it [30:15,  1.45it/s]\u001b[A\n",
      "2625it [30:16,  1.45it/s]\u001b[A\n",
      "2626it [30:16,  1.45it/s]\u001b[A\n",
      "2627it [30:17,  1.45it/s]\u001b[A\n",
      "2628it [30:18,  1.45it/s]\u001b[A\n",
      "2629it [30:18,  1.45it/s]\u001b[A\n",
      "2630it [30:19,  1.45it/s]\u001b[A\n",
      "2631it [30:20,  1.45it/s]\u001b[A\n",
      "2632it [30:20,  1.45it/s]\u001b[A\n",
      "2633it [30:21,  1.45it/s]\u001b[A\n",
      "2634it [30:22,  1.45it/s]\u001b[A\n",
      "2635it [30:22,  1.45it/s]\u001b[A\n",
      "2636it [30:23,  1.45it/s]\u001b[A\n",
      "2637it [30:24,  1.46it/s]\u001b[A\n",
      "2638it [30:25,  1.45it/s]\u001b[A\n",
      "2639it [30:25,  1.45it/s]\u001b[A\n",
      "2640it [30:26,  1.45it/s]\u001b[A\n",
      "2641it [30:27,  1.45it/s]\u001b[A\n",
      "2642it [30:27,  1.45it/s]\u001b[A\n",
      "2643it [30:28,  1.45it/s]\u001b[A\n",
      "2644it [30:29,  1.45it/s]\u001b[A\n",
      "2645it [30:29,  1.45it/s]\u001b[A\n",
      "2646it [30:30,  1.45it/s]\u001b[A\n",
      "2647it [30:31,  1.45it/s]\u001b[A\n",
      "2648it [30:31,  1.45it/s]\u001b[A\n",
      "2649it [30:32,  1.45it/s]\u001b[A\n",
      "2650it [30:33,  1.45it/s]\u001b[A\n",
      "2651it [30:33,  1.45it/s]\u001b[A\n",
      "2652it [30:34,  1.45it/s]\u001b[A\n",
      "2653it [30:35,  1.45it/s]\u001b[A\n",
      "2654it [30:36,  1.45it/s]\u001b[A\n",
      "2655it [30:36,  1.45it/s]\u001b[A\n",
      "2656it [30:37,  1.45it/s]\u001b[A\n",
      "2657it [30:38,  1.45it/s]\u001b[A\n",
      "2658it [30:38,  1.45it/s]\u001b[A\n",
      "2659it [30:39,  1.45it/s]\u001b[A\n",
      "2660it [30:40,  1.45it/s]\u001b[A\n",
      "2661it [30:40,  1.45it/s]\u001b[A\n",
      "2662it [30:41,  1.45it/s]\u001b[A\n",
      "2663it [30:42,  1.45it/s]\u001b[A\n",
      "2664it [30:42,  1.45it/s]\u001b[A\n",
      "2665it [30:43,  1.45it/s]\u001b[A\n",
      "2666it [30:44,  1.45it/s]\u001b[A\n",
      "2667it [30:45,  1.45it/s]\u001b[A\n",
      "2668it [30:45,  1.45it/s]\u001b[A\n",
      "2669it [30:46,  1.45it/s]\u001b[A\n",
      "2670it [30:47,  1.45it/s]\u001b[A\n",
      "2671it [30:47,  1.45it/s]\u001b[A\n",
      "2672it [30:48,  1.45it/s]\u001b[A\n",
      "2673it [30:49,  1.45it/s]\u001b[A\n",
      "2674it [30:49,  1.45it/s]\u001b[A\n",
      "2675it [30:50,  1.45it/s]\u001b[A\n",
      "2676it [30:51,  1.45it/s]\u001b[A\n",
      "2677it [30:51,  1.45it/s]\u001b[A\n",
      "2678it [30:52,  1.45it/s]\u001b[A\n",
      "2679it [30:53,  1.45it/s]\u001b[A\n",
      "2680it [30:53,  1.45it/s]\u001b[A\n",
      "2681it [30:54,  1.45it/s]\u001b[A\n",
      "2682it [30:55,  1.45it/s]\u001b[A\n",
      "2683it [30:56,  1.45it/s]\u001b[A\n",
      "2684it [30:56,  1.45it/s]\u001b[A\n",
      "2685it [30:57,  1.45it/s]\u001b[A\n",
      "2686it [30:58,  1.45it/s]\u001b[A\n",
      "2687it [30:58,  1.45it/s]\u001b[A\n",
      "2688it [30:59,  1.45it/s]\u001b[A\n",
      "2689it [31:00,  1.45it/s]\u001b[A\n",
      "2690it [31:00,  1.45it/s]\u001b[A\n",
      "2691it [31:01,  1.45it/s]\u001b[A\n",
      "2692it [31:02,  1.45it/s]\u001b[A\n",
      "2693it [31:02,  1.45it/s]\u001b[A\n",
      "2694it [31:03,  1.45it/s]\u001b[A\n",
      "2695it [31:04,  1.45it/s]\u001b[A\n",
      "2696it [31:04,  1.45it/s]\u001b[A\n",
      "2697it [31:05,  1.45it/s]\u001b[A\n",
      "2698it [31:06,  1.45it/s]\u001b[A\n",
      "2699it [31:07,  1.45it/s]\u001b[A\n",
      "2700it [31:07,  1.45it/s]\u001b[A\n",
      "2701it [31:08,  1.45it/s]\u001b[A\n",
      "2702it [31:09,  1.45it/s]\u001b[A\n",
      "2703it [31:09,  1.45it/s]\u001b[A\n",
      "2704it [31:10,  1.45it/s]\u001b[A\n",
      "2705it [31:11,  1.45it/s]\u001b[A\n",
      "2706it [31:11,  1.45it/s]\u001b[A\n",
      "2707it [31:12,  1.45it/s]\u001b[A\n",
      "2708it [31:13,  1.45it/s]\u001b[A\n",
      "2709it [31:13,  1.45it/s]\u001b[A\n",
      "2710it [31:14,  1.45it/s]\u001b[A\n",
      "2711it [31:15,  1.45it/s]\u001b[A\n",
      "2712it [31:16,  1.45it/s]\u001b[A\n",
      "2713it [31:16,  1.45it/s]\u001b[A\n",
      "2714it [31:17,  1.45it/s]\u001b[A\n",
      "2715it [31:18,  1.45it/s]\u001b[A\n",
      "2716it [31:18,  1.45it/s]\u001b[A\n",
      "2717it [31:19,  1.45it/s]\u001b[A\n",
      "2718it [31:20,  1.45it/s]\u001b[A\n",
      "2719it [31:20,  1.45it/s]\u001b[A\n",
      "2720it [31:21,  1.45it/s]\u001b[A\n",
      "2721it [31:22,  1.45it/s]\u001b[A\n",
      "2722it [31:22,  1.45it/s]\u001b[A\n",
      "2723it [31:23,  1.45it/s]\u001b[A\n",
      "2724it [31:24,  1.45it/s]\u001b[A\n",
      "2725it [31:24,  1.45it/s]\u001b[A\n",
      "2726it [31:25,  1.45it/s]\u001b[A\n",
      "2727it [31:26,  1.45it/s]\u001b[A\n",
      "2728it [31:27,  1.45it/s]\u001b[A\n",
      "2729it [31:27,  1.45it/s]\u001b[A\n",
      "2730it [31:28,  1.45it/s]\u001b[A\n",
      "2731it [31:29,  1.45it/s]\u001b[A\n",
      "2732it [31:29,  1.45it/s]\u001b[A\n",
      "2733it [31:30,  1.45it/s]\u001b[A\n",
      "2734it [31:31,  1.45it/s]\u001b[A\n",
      "2735it [31:31,  1.45it/s]\u001b[A\n",
      "2736it [31:32,  1.45it/s]\u001b[A\n",
      "2737it [31:33,  1.45it/s]\u001b[A\n",
      "2738it [31:33,  1.45it/s]\u001b[A\n",
      "2739it [31:34,  1.45it/s]\u001b[A\n",
      "2740it [31:35,  1.45it/s]\u001b[A\n",
      "2741it [31:35,  1.45it/s]\u001b[A\n",
      "2742it [31:36,  1.45it/s]\u001b[A\n",
      "2743it [31:37,  1.45it/s]\u001b[A\n",
      "2744it [31:38,  1.45it/s]\u001b[A\n",
      "2745it [31:38,  1.45it/s]\u001b[A\n",
      "2746it [31:39,  1.45it/s]\u001b[A\n",
      "2747it [31:40,  1.45it/s]\u001b[A\n",
      "2748it [31:40,  1.45it/s]\u001b[A\n",
      "2749it [31:41,  1.45it/s]\u001b[A\n",
      "2750it [31:42,  1.45it/s]\u001b[A\n",
      "2751it [31:42,  1.45it/s]\u001b[A\n",
      "2752it [31:43,  1.45it/s]\u001b[A\n",
      "2753it [31:44,  1.45it/s]\u001b[A\n",
      "2754it [31:44,  1.45it/s]\u001b[A\n",
      "2755it [31:45,  1.45it/s]\u001b[A\n",
      "2756it [31:46,  1.45it/s]\u001b[A\n",
      "2757it [31:46,  1.45it/s]\u001b[A\n",
      "2758it [31:47,  1.45it/s]\u001b[A\n",
      "2759it [31:48,  1.45it/s]\u001b[A\n",
      "2760it [31:49,  1.45it/s]\u001b[A\n",
      "2761it [31:49,  1.45it/s]\u001b[A\n",
      "2762it [31:50,  1.45it/s]\u001b[A\n",
      "2763it [31:51,  1.45it/s]\u001b[A\n",
      "2764it [31:51,  1.45it/s]\u001b[A\n",
      "2765it [31:52,  1.45it/s]\u001b[A\n",
      "2766it [31:53,  1.45it/s]\u001b[A\n",
      "2767it [31:53,  1.45it/s]\u001b[A\n",
      "2768it [31:54,  1.45it/s]\u001b[A\n",
      "2769it [31:55,  1.45it/s]\u001b[A\n",
      "2770it [31:55,  1.45it/s]\u001b[A\n",
      "2771it [31:56,  1.45it/s]\u001b[A\n",
      "2772it [31:57,  1.45it/s]\u001b[A\n",
      "2773it [31:58,  1.45it/s]\u001b[A\n",
      "2774it [31:58,  1.45it/s]\u001b[A\n",
      "2775it [31:59,  1.45it/s]\u001b[A\n",
      "2776it [32:00,  1.45it/s]\u001b[A\n",
      "2777it [32:00,  1.45it/s]\u001b[A\n",
      "2778it [32:01,  1.45it/s]\u001b[A\n",
      "2779it [32:02,  1.45it/s]\u001b[A\n",
      "2780it [32:02,  1.45it/s]\u001b[A\n",
      "2781it [32:03,  1.45it/s]\u001b[A\n",
      "2782it [32:04,  1.45it/s]\u001b[A\n",
      "2783it [32:04,  1.45it/s]\u001b[A\n",
      "2784it [32:05,  1.45it/s]\u001b[A\n",
      "2785it [32:06,  1.45it/s]\u001b[A\n",
      "2786it [32:06,  1.45it/s]\u001b[A\n",
      "2787it [32:07,  1.45it/s]\u001b[A\n",
      "2788it [32:08,  1.45it/s]\u001b[A\n",
      "2789it [32:09,  1.45it/s]\u001b[A\n",
      "2790it [32:09,  1.45it/s]\u001b[A\n",
      "2791it [32:10,  1.45it/s]\u001b[A\n",
      "2792it [32:11,  1.45it/s]\u001b[A\n",
      "2793it [32:11,  1.45it/s]\u001b[A\n",
      "2794it [32:12,  1.45it/s]\u001b[A\n",
      "2795it [32:13,  1.45it/s]\u001b[A\n",
      "2796it [32:13,  1.45it/s]\u001b[A\n",
      "2797it [32:14,  1.45it/s]\u001b[A\n",
      "2798it [32:15,  1.45it/s]\u001b[A\n",
      "2799it [32:15,  1.45it/s]\u001b[A\n",
      "2800it [32:16,  1.45it/s]\u001b[A\n",
      "2801it [32:17,  1.45it/s]\u001b[A\n",
      "2802it [32:17,  1.45it/s]\u001b[A\n",
      "2803it [32:18,  1.45it/s]\u001b[A\n",
      "2804it [32:19,  1.45it/s]\u001b[A\n",
      "2805it [32:20,  1.45it/s]\u001b[A\n",
      "2806it [32:20,  1.45it/s]\u001b[A\n",
      "2807it [32:21,  1.45it/s]\u001b[A\n",
      "2808it [32:22,  1.45it/s]\u001b[A\n",
      "2809it [32:22,  1.45it/s]\u001b[A\n",
      "2810it [32:23,  1.45it/s]\u001b[A\n",
      "2811it [32:24,  1.45it/s]\u001b[A\n",
      "2812it [32:24,  1.45it/s]\u001b[A\n",
      "2813it [32:25,  1.45it/s]\u001b[A\n",
      "2814it [32:26,  1.45it/s]\u001b[A\n",
      "2815it [32:26,  1.45it/s]\u001b[A\n",
      "2816it [32:27,  1.45it/s]\u001b[A\n",
      "2817it [32:28,  1.45it/s]\u001b[A\n",
      "2818it [32:28,  1.45it/s]\u001b[A\n",
      "2819it [32:29,  1.45it/s]\u001b[A\n",
      "2820it [32:30,  1.45it/s]\u001b[A\n",
      "2821it [32:31,  1.45it/s]\u001b[A\n",
      "2822it [32:31,  1.45it/s]\u001b[A\n",
      "2823it [32:32,  1.45it/s]\u001b[A\n",
      "2824it [32:33,  1.45it/s]\u001b[A\n",
      "2825it [32:33,  1.45it/s]\u001b[A\n",
      "2826it [32:34,  1.45it/s]\u001b[A\n",
      "2827it [32:35,  1.45it/s]\u001b[A\n",
      "2828it [32:35,  1.45it/s]\u001b[A\n",
      "2829it [32:36,  1.45it/s]\u001b[A\n",
      "2830it [32:37,  1.45it/s]\u001b[A\n",
      "2831it [32:37,  1.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2832it [32:38,  1.45it/s]\u001b[A\n",
      "2833it [32:39,  1.45it/s]\u001b[A\n",
      "2834it [32:40,  1.45it/s]\u001b[A\n",
      "2835it [32:40,  1.45it/s]\u001b[A\n",
      "2836it [32:41,  1.45it/s]\u001b[A\n",
      "2837it [32:42,  1.45it/s]\u001b[A\n",
      "2838it [32:42,  1.45it/s]\u001b[A\n",
      "2839it [32:43,  1.45it/s]\u001b[A\n",
      "2840it [32:44,  1.45it/s]\u001b[A\n",
      "2841it [32:44,  1.45it/s]\u001b[A\n",
      "2842it [32:45,  1.45it/s]\u001b[A\n",
      "2843it [32:46,  1.45it/s]\u001b[A\n",
      "2844it [32:46,  1.45it/s]\u001b[A\n",
      "2845it [32:47,  1.45it/s]\u001b[A\n",
      "2846it [32:48,  1.45it/s]\u001b[A\n",
      "2847it [32:48,  1.45it/s]\u001b[A\n",
      "2848it [32:49,  1.45it/s]\u001b[A\n",
      "2849it [32:50,  1.45it/s]\u001b[A\n",
      "2850it [32:51,  1.45it/s]\u001b[A\n",
      "2851it [32:51,  1.45it/s]\u001b[A\n",
      "2852it [32:52,  1.45it/s]\u001b[A\n",
      "2853it [32:53,  1.45it/s]\u001b[A\n",
      "2854it [32:53,  1.45it/s]\u001b[A\n",
      "2855it [32:54,  1.45it/s]\u001b[A\n",
      "2856it [32:55,  1.45it/s]\u001b[A\n",
      "2857it [32:55,  1.45it/s]\u001b[A\n",
      "2858it [32:56,  1.45it/s]\u001b[A\n",
      "2859it [32:57,  1.45it/s]\u001b[A\n",
      "2860it [32:57,  1.45it/s]\u001b[A\n",
      "2861it [32:58,  1.45it/s]\u001b[A\n",
      "2862it [32:59,  1.45it/s]\u001b[A\n",
      "2863it [32:59,  1.45it/s]\u001b[A\n",
      "2864it [33:00,  1.45it/s]\u001b[A\n",
      "2865it [33:01,  1.45it/s]\u001b[A\n",
      "2866it [33:02,  1.45it/s]\u001b[A\n",
      "2867it [33:02,  1.45it/s]\u001b[A\n",
      "2868it [33:03,  1.45it/s]\u001b[A\n",
      "2869it [33:04,  1.45it/s]\u001b[A\n",
      "2870it [33:04,  1.45it/s]\u001b[A\n",
      "2871it [33:05,  1.45it/s]\u001b[A\n",
      "2872it [33:06,  1.45it/s]\u001b[A\n",
      "2873it [33:06,  1.45it/s]\u001b[A\n",
      "2874it [33:07,  1.45it/s]\u001b[A\n",
      "2875it [33:08,  1.45it/s]\u001b[A\n",
      "2876it [33:08,  1.45it/s]\u001b[A\n",
      "2877it [33:09,  1.45it/s]\u001b[A\n",
      "2878it [33:10,  1.45it/s]\u001b[A\n",
      "2879it [33:10,  1.45it/s]\u001b[A\n",
      "2880it [33:11,  1.45it/s]\u001b[A\n",
      "2881it [33:12,  1.45it/s]\u001b[A\n",
      "2882it [33:13,  1.45it/s]\u001b[A\n",
      "2883it [33:13,  1.45it/s]\u001b[A\n",
      "2884it [33:14,  1.45it/s]\u001b[A\n",
      "2885it [33:15,  1.45it/s]\u001b[A\n",
      "2886it [33:15,  1.45it/s]\u001b[A\n",
      "2887it [33:16,  1.45it/s]\u001b[A\n",
      "2888it [33:17,  1.45it/s]\u001b[A\n",
      "2889it [33:17,  1.45it/s]\u001b[A\n",
      "2890it [33:18,  1.45it/s]\u001b[A\n",
      "2891it [33:19,  1.45it/s]\u001b[A\n",
      "2892it [33:19,  1.45it/s]\u001b[A\n",
      "2893it [33:20,  1.45it/s]\u001b[A\n",
      "2894it [33:21,  1.45it/s]\u001b[A\n",
      "2895it [33:22,  1.45it/s]\u001b[A\n",
      "2896it [33:22,  1.45it/s]\u001b[A\n",
      "2897it [33:23,  1.45it/s]\u001b[A\n",
      "2898it [33:24,  1.45it/s]\u001b[A\n",
      "2899it [33:24,  1.45it/s]\u001b[A\n",
      "2900it [33:25,  1.45it/s]\u001b[A\n",
      "2901it [33:26,  1.45it/s]\u001b[A\n",
      "2902it [33:26,  1.45it/s]\u001b[A\n",
      "2903it [33:27,  1.45it/s]\u001b[A\n",
      "2904it [33:28,  1.45it/s]\u001b[A\n",
      "2905it [33:28,  1.45it/s]\u001b[A\n",
      "2906it [33:29,  1.45it/s]\u001b[A\n",
      "2907it [33:30,  1.45it/s]\u001b[A\n",
      "2908it [33:30,  1.45it/s]\u001b[A\n",
      "2909it [33:31,  1.45it/s]\u001b[A\n",
      "2910it [33:32,  1.45it/s]\u001b[A\n",
      "2911it [33:33,  1.45it/s]\u001b[A\n",
      "2912it [33:33,  1.45it/s]\u001b[A\n",
      "2913it [33:34,  1.45it/s]\u001b[A\n",
      "2914it [33:35,  1.45it/s]\u001b[A\n",
      "2915it [33:35,  1.45it/s]\u001b[A\n",
      "2916it [33:36,  1.45it/s]\u001b[A\n",
      "2917it [33:37,  1.45it/s]\u001b[A\n",
      "2918it [33:37,  1.45it/s]\u001b[A\n",
      "2919it [33:38,  1.45it/s]\u001b[A\n",
      "2920it [33:39,  1.45it/s]\u001b[A\n",
      "2921it [33:39,  1.45it/s]\u001b[A\n",
      "2922it [33:40,  1.45it/s]\u001b[A\n",
      "2923it [33:41,  1.45it/s]\u001b[A\n",
      "2924it [33:41,  1.45it/s]\u001b[A\n",
      "2925it [33:42,  1.45it/s]\u001b[A\n",
      "2926it [33:43,  1.45it/s]\u001b[A\n",
      "2927it [33:44,  1.45it/s]\u001b[A\n",
      "2928it [33:44,  1.45it/s]\u001b[A\n",
      "2929it [33:45,  1.45it/s]\u001b[A\n",
      "2930it [33:46,  1.45it/s]\u001b[A\n",
      "2931it [33:46,  1.45it/s]\u001b[A\n",
      "2932it [33:47,  1.45it/s]\u001b[A\n",
      "2933it [33:48,  1.45it/s]\u001b[A\n",
      "2934it [33:48,  1.45it/s]\u001b[A\n",
      "2935it [33:49,  1.45it/s]\u001b[A\n",
      "2936it [33:50,  1.45it/s]\u001b[A\n",
      "2937it [33:50,  1.45it/s]\u001b[A\n",
      "2938it [33:51,  1.45it/s]\u001b[A\n",
      "2939it [33:52,  1.45it/s]\u001b[A\n",
      "2940it [33:52,  1.45it/s]\u001b[A\n",
      "2941it [33:53,  1.45it/s]\u001b[A\n",
      "2942it [33:54,  1.45it/s]\u001b[A\n",
      "2943it [33:55,  1.45it/s]\u001b[A\n",
      "2944it [33:55,  1.45it/s]\u001b[A\n",
      "2945it [33:56,  1.45it/s]\u001b[A\n",
      "2946it [33:57,  1.45it/s]\u001b[A\n",
      "2947it [33:57,  1.45it/s]\u001b[A\n",
      "2948it [33:58,  1.45it/s]\u001b[A\n",
      "2949it [33:59,  1.45it/s]\u001b[A\n",
      "2950it [33:59,  1.45it/s]\u001b[A\n",
      "2951it [34:00,  1.45it/s]\u001b[A\n",
      "2952it [34:01,  1.45it/s]\u001b[A\n",
      "2953it [34:01,  1.45it/s]\u001b[A\n",
      "2954it [34:02,  1.45it/s]\u001b[A\n",
      "2955it [34:03,  1.45it/s]\u001b[A\n",
      "2956it [34:04,  1.45it/s]\u001b[A\n",
      "2957it [34:04,  1.45it/s]\u001b[A\n",
      "2958it [34:05,  1.45it/s]\u001b[A\n",
      "2959it [34:06,  1.45it/s]\u001b[A\n",
      "2960it [34:06,  1.45it/s]\u001b[A\n",
      "2961it [34:07,  1.45it/s]\u001b[A\n",
      "2962it [34:08,  1.45it/s]\u001b[A\n",
      "2963it [34:08,  1.45it/s]\u001b[A\n",
      "2964it [34:09,  1.45it/s]\u001b[A\n",
      "2965it [34:10,  1.45it/s]\u001b[A\n",
      "2966it [34:10,  1.45it/s]\u001b[A\n",
      "2967it [34:11,  1.45it/s]\u001b[A\n",
      "2968it [34:12,  1.45it/s]\u001b[A\n",
      "2969it [34:12,  1.45it/s]\u001b[A\n",
      "2970it [34:13,  1.45it/s]\u001b[A\n",
      "2971it [34:14,  1.45it/s]\u001b[A\n",
      "2972it [34:15,  1.45it/s]\u001b[A\n",
      "2973it [34:15,  1.45it/s]\u001b[A\n",
      "2974it [34:16,  1.45it/s]\u001b[A\n",
      "2975it [34:17,  1.45it/s]\u001b[A\n",
      "2976it [34:17,  1.45it/s]\u001b[A\n",
      "2977it [34:18,  1.45it/s]\u001b[A\n",
      "2978it [34:19,  1.45it/s]\u001b[A\n",
      "2979it [34:19,  1.45it/s]\u001b[A\n",
      "2980it [34:20,  1.45it/s]\u001b[A\n",
      "2981it [34:21,  1.45it/s]\u001b[A\n",
      "2982it [34:21,  1.45it/s]\u001b[A\n",
      "2983it [34:22,  1.45it/s]\u001b[A\n",
      "2984it [34:23,  1.45it/s]\u001b[A\n",
      "2985it [34:23,  1.45it/s]\u001b[A\n",
      "2986it [34:24,  1.45it/s]\u001b[A\n",
      "2987it [34:25,  1.45it/s]\u001b[A\n",
      "2988it [34:26,  1.45it/s]\u001b[A\n",
      "2989it [34:26,  1.45it/s]\u001b[A\n",
      "2990it [34:27,  1.45it/s]\u001b[A\n",
      "2991it [34:28,  1.45it/s]\u001b[A\n",
      "2992it [34:28,  1.45it/s]\u001b[A\n",
      "2993it [34:29,  1.45it/s]\u001b[A\n",
      "2994it [34:30,  1.45it/s]\u001b[A\n",
      "2995it [34:30,  1.45it/s]\u001b[A\n",
      "2996it [34:31,  1.45it/s]\u001b[A\n",
      "2997it [34:32,  1.45it/s]\u001b[A\n",
      "2998it [34:32,  1.45it/s]\u001b[A\n",
      "2999it [34:33,  1.45it/s]\u001b[A\n",
      "3000it [34:34,  1.45it/s]\u001b[A\n",
      "3001it [34:35,  1.45it/s]\u001b[A\n",
      "3002it [34:35,  1.45it/s]\u001b[A\n",
      "3003it [34:36,  1.45it/s]\u001b[A\n",
      "3004it [34:37,  1.45it/s]\u001b[A\n",
      "3005it [34:37,  1.45it/s]\u001b[A\n",
      "3006it [34:38,  1.45it/s]\u001b[A\n",
      "3007it [34:39,  1.45it/s]\u001b[A\n",
      "3008it [34:39,  1.45it/s]\u001b[A\n",
      "3009it [34:40,  1.45it/s]\u001b[A\n",
      "3010it [34:41,  1.45it/s]\u001b[A\n",
      "3011it [34:41,  1.45it/s]\u001b[A\n",
      "3012it [34:42,  1.45it/s]\u001b[A\n",
      "3013it [34:43,  1.45it/s]\u001b[A\n",
      "3014it [34:43,  1.45it/s]\u001b[A\n",
      "3015it [34:44,  1.45it/s]\u001b[A\n",
      "3016it [34:45,  1.45it/s]\u001b[A\n",
      "3017it [34:46,  1.45it/s]\u001b[A\n",
      "3018it [34:46,  1.45it/s]\u001b[A\n",
      "3019it [34:47,  1.45it/s]\u001b[A\n",
      "3020it [34:48,  1.45it/s]\u001b[A\n",
      "3021it [34:48,  1.45it/s]\u001b[A\n",
      "3022it [34:49,  1.45it/s]\u001b[A\n",
      "3023it [34:50,  1.45it/s]\u001b[A\n",
      "3024it [34:50,  1.45it/s]\u001b[A\n",
      "3025it [34:51,  1.45it/s]\u001b[A\n",
      "3026it [34:52,  1.45it/s]\u001b[A\n",
      "3027it [34:52,  1.45it/s]\u001b[A\n",
      "3028it [34:53,  1.45it/s]\u001b[A\n",
      "3029it [34:54,  1.45it/s]\u001b[A\n",
      "3030it [34:54,  1.45it/s]\u001b[A\n",
      "3031it [34:55,  1.45it/s]\u001b[A\n",
      "3032it [34:56,  1.45it/s]\u001b[A\n",
      "3033it [34:57,  1.45it/s]\u001b[A\n",
      "3034it [34:57,  1.45it/s]\u001b[A\n",
      "3035it [34:58,  1.45it/s]\u001b[A\n",
      "3036it [34:59,  1.45it/s]\u001b[A\n",
      "3037it [34:59,  1.45it/s]\u001b[A\n",
      "3038it [35:00,  1.45it/s]\u001b[A\n",
      "3039it [35:01,  1.45it/s]\u001b[A\n",
      "3040it [35:01,  1.45it/s]\u001b[A\n",
      "3041it [35:02,  1.45it/s]\u001b[A\n",
      "3042it [35:03,  1.45it/s]\u001b[A\n",
      "3043it [35:03,  1.45it/s]\u001b[A\n",
      "3044it [35:04,  1.45it/s]\u001b[A\n",
      "3045it [35:05,  1.45it/s]\u001b[A\n",
      "3046it [35:05,  1.45it/s]\u001b[A\n",
      "3047it [35:06,  1.45it/s]\u001b[A\n",
      "3048it [35:07,  1.45it/s]\u001b[A\n",
      "3049it [35:08,  1.45it/s]\u001b[A\n",
      "3050it [35:08,  1.45it/s]\u001b[A\n",
      "3051it [35:09,  1.45it/s]\u001b[A\n",
      "3052it [35:10,  1.45it/s]\u001b[A\n",
      "3053it [35:10,  1.45it/s]\u001b[A\n",
      "3054it [35:11,  1.45it/s]\u001b[A\n",
      "3055it [35:12,  1.45it/s]\u001b[A\n",
      "3056it [35:12,  1.45it/s]\u001b[A\n",
      "3057it [35:13,  1.45it/s]\u001b[A\n",
      "3058it [35:14,  1.45it/s]\u001b[A\n",
      "3059it [35:14,  1.45it/s]\u001b[A\n",
      "3060it [35:15,  1.45it/s]\u001b[A\n",
      "3061it [35:16,  1.45it/s]\u001b[A\n",
      "3062it [35:17,  1.45it/s]\u001b[A\n",
      "3063it [35:17,  1.45it/s]\u001b[A\n",
      "3064it [35:18,  1.45it/s]\u001b[A\n",
      "3065it [35:19,  1.45it/s]\u001b[A\n",
      "3066it [35:19,  1.45it/s]\u001b[A\n",
      "3067it [35:20,  1.45it/s]\u001b[A\n",
      "3068it [35:21,  1.45it/s]\u001b[A\n",
      "3069it [35:21,  1.45it/s]\u001b[A\n",
      "3070it [35:22,  1.45it/s]\u001b[A\n",
      "3071it [35:23,  1.45it/s]\u001b[A\n",
      "3072it [35:23,  1.45it/s]\u001b[A\n",
      "3073it [35:24,  1.45it/s]\u001b[A\n",
      "3074it [35:25,  1.45it/s]\u001b[A\n",
      "3075it [35:25,  1.45it/s]\u001b[A\n",
      "3076it [35:26,  1.45it/s]\u001b[A\n",
      "3077it [35:27,  1.45it/s]\u001b[A\n",
      "3078it [35:28,  1.45it/s]\u001b[A\n",
      "3079it [35:28,  1.45it/s]\u001b[A\n",
      "3080it [35:29,  1.45it/s]\u001b[A\n",
      "3081it [35:30,  1.45it/s]\u001b[A\n",
      "3082it [35:30,  1.45it/s]\u001b[A\n",
      "3083it [35:31,  1.45it/s]\u001b[A\n",
      "3084it [35:32,  1.45it/s]\u001b[A\n",
      "3085it [35:32,  1.45it/s]\u001b[A\n",
      "3086it [35:33,  1.46it/s]\u001b[A\n",
      "3087it [35:34,  1.45it/s]\u001b[A\n",
      "3088it [35:34,  1.45it/s]\u001b[A\n",
      "3089it [35:35,  1.45it/s]\u001b[A\n",
      "3090it [35:36,  1.45it/s]\u001b[A\n",
      "3091it [35:36,  1.45it/s]\u001b[A\n",
      "3092it [35:37,  1.45it/s]\u001b[A\n",
      "3093it [35:38,  1.45it/s]\u001b[A\n",
      "3094it [35:39,  1.45it/s]\u001b[A\n",
      "3095it [35:39,  1.45it/s]\u001b[A\n",
      "3096it [35:40,  1.45it/s]\u001b[A\n",
      "3097it [35:41,  1.45it/s]\u001b[A\n",
      "3098it [35:41,  1.45it/s]\u001b[A\n",
      "3099it [35:42,  1.45it/s]\u001b[A\n",
      "3100it [35:43,  1.45it/s]\u001b[A\n",
      "3101it [35:43,  1.45it/s]\u001b[A\n",
      "3102it [35:44,  1.45it/s]\u001b[A\n",
      "3103it [35:45,  1.45it/s]\u001b[A\n",
      "3104it [35:45,  1.45it/s]\u001b[A\n",
      "3105it [35:46,  1.45it/s]\u001b[A\n",
      "3106it [35:47,  1.45it/s]\u001b[A\n",
      "3107it [35:47,  1.45it/s]\u001b[A\n",
      "3108it [35:48,  1.45it/s]\u001b[A\n",
      "3109it [35:49,  1.45it/s]\u001b[A\n",
      "3110it [35:50,  1.45it/s]\u001b[A\n",
      "3111it [35:50,  1.45it/s]\u001b[A\n",
      "3112it [35:51,  1.45it/s]\u001b[A\n",
      "3113it [35:52,  1.45it/s]\u001b[A\n",
      "3114it [35:52,  1.45it/s]\u001b[A\n",
      "3115it [35:53,  1.45it/s]\u001b[A\n",
      "3116it [35:54,  1.45it/s]\u001b[A\n",
      "3117it [35:54,  1.45it/s]\u001b[A\n",
      "3118it [35:55,  1.45it/s]\u001b[A\n",
      "3119it [35:56,  1.45it/s]\u001b[A\n",
      "3120it [35:56,  1.45it/s]\u001b[A\n",
      "3121it [35:57,  1.45it/s]\u001b[A\n",
      "3122it [35:58,  1.45it/s]\u001b[A\n",
      "3123it [35:59,  1.45it/s]\u001b[A\n",
      "3124it [35:59,  1.45it/s]\u001b[A\n",
      "3125it [36:00,  1.45it/s]\u001b[A\n",
      "3126it [36:01,  1.45it/s]\u001b[A\n",
      "3127it [36:01,  1.45it/s]\u001b[A\n",
      "3128it [36:02,  1.45it/s]\u001b[A\n",
      "3129it [36:03,  1.45it/s]\u001b[A\n",
      "3130it [36:03,  1.45it/s]\u001b[A\n",
      "3131it [36:04,  1.45it/s]\u001b[A\n",
      "3132it [36:05,  1.45it/s]\u001b[A\n",
      "3133it [36:05,  1.45it/s]\u001b[A\n",
      "3134it [36:06,  1.45it/s]\u001b[A\n",
      "3135it [36:07,  1.45it/s]\u001b[A\n",
      "3136it [36:07,  1.45it/s]\u001b[A\n",
      "3137it [36:08,  1.45it/s]\u001b[A\n",
      "3138it [36:09,  1.45it/s]\u001b[A\n",
      "3139it [36:10,  1.45it/s]\u001b[A\n",
      "3140it [36:10,  1.45it/s]\u001b[A\n",
      "3141it [36:11,  1.45it/s]\u001b[A\n",
      "3142it [36:12,  1.45it/s]\u001b[A\n",
      "3143it [36:12,  1.45it/s]\u001b[A\n",
      "3144it [36:13,  1.45it/s]\u001b[A\n",
      "3145it [36:14,  1.45it/s]\u001b[A\n",
      "3146it [36:14,  1.45it/s]\u001b[A\n",
      "3147it [36:15,  1.45it/s]\u001b[A\n",
      "3148it [36:16,  1.45it/s]\u001b[A\n",
      "3149it [36:16,  1.45it/s]\u001b[A\n",
      "3150it [36:17,  1.45it/s]\u001b[A\n",
      "3151it [36:18,  1.45it/s]\u001b[A\n",
      "3152it [36:18,  1.45it/s]\u001b[A\n",
      "3153it [36:19,  1.45it/s]\u001b[A\n",
      "3154it [36:20,  1.45it/s]\u001b[A\n",
      "3155it [36:21,  1.45it/s]\u001b[A\n",
      "3156it [36:21,  1.45it/s]\u001b[A\n",
      "3157it [36:22,  1.45it/s]\u001b[A\n",
      "3158it [36:23,  1.45it/s]\u001b[A\n",
      "3159it [36:23,  1.45it/s]\u001b[A\n",
      "3160it [36:24,  1.45it/s]\u001b[A\n",
      "3161it [36:25,  1.45it/s]\u001b[A\n",
      "3162it [36:25,  1.45it/s]\u001b[A\n",
      "3163it [36:26,  1.45it/s]\u001b[A\n",
      "3164it [36:27,  1.45it/s]\u001b[A\n",
      "3165it [36:27,  1.45it/s]\u001b[A\n",
      "3166it [36:28,  1.45it/s]\u001b[A\n",
      "3167it [36:29,  1.45it/s]\u001b[A\n",
      "3168it [36:30,  1.45it/s]\u001b[A\n",
      "3169it [36:30,  1.45it/s]\u001b[A\n",
      "3170it [36:31,  1.45it/s]\u001b[A\n",
      "3171it [36:32,  1.45it/s]\u001b[A\n",
      "3172it [36:32,  1.45it/s]\u001b[A\n",
      "3173it [36:33,  1.45it/s]\u001b[A\n",
      "3174it [36:34,  1.45it/s]\u001b[A\n",
      "3175it [36:34,  1.45it/s]\u001b[A\n",
      "3176it [36:35,  1.45it/s]\u001b[A\n",
      "3177it [36:36,  1.45it/s]\u001b[A\n",
      "3178it [36:36,  1.45it/s]\u001b[A\n",
      "3179it [36:37,  1.45it/s]\u001b[A\n",
      "3180it [36:38,  1.45it/s]\u001b[A\n",
      "3181it [36:38,  1.45it/s]\u001b[A\n",
      "3182it [36:39,  1.45it/s]\u001b[A\n",
      "3183it [36:40,  1.45it/s]\u001b[A\n",
      "3184it [36:41,  1.45it/s]\u001b[A\n",
      "3185it [36:41,  1.45it/s]\u001b[A\n",
      "3186it [36:42,  1.45it/s]\u001b[A\n",
      "3187it [36:43,  1.45it/s]\u001b[A\n",
      "3188it [36:43,  1.45it/s]\u001b[A\n",
      "3189it [36:44,  1.45it/s]\u001b[A\n",
      "3190it [36:45,  1.45it/s]\u001b[A\n",
      "3191it [36:45,  1.45it/s]\u001b[A\n",
      "3192it [36:46,  1.45it/s]\u001b[A\n",
      "3193it [36:47,  1.45it/s]\u001b[A\n",
      "3194it [36:47,  1.45it/s]\u001b[A\n",
      "3195it [36:48,  1.45it/s]\u001b[A\n",
      "3196it [36:49,  1.45it/s]\u001b[A\n",
      "3197it [36:49,  1.45it/s]\u001b[A\n",
      "3198it [36:50,  1.45it/s]\u001b[A\n",
      "3199it [36:51,  1.45it/s]\u001b[A\n",
      "3200it [36:52,  1.45it/s]\u001b[A\n",
      "3201it [36:52,  1.45it/s]\u001b[A\n",
      "3202it [36:53,  1.45it/s]\u001b[A\n",
      "3203it [36:54,  1.45it/s]\u001b[A\n",
      "3204it [36:54,  1.45it/s]\u001b[A\n",
      "3205it [36:55,  1.45it/s]\u001b[A\n",
      "3206it [36:56,  1.45it/s]\u001b[A\n",
      "3207it [36:56,  1.45it/s]\u001b[A\n",
      "3208it [36:57,  1.45it/s]\u001b[A\n",
      "3209it [36:58,  1.45it/s]\u001b[A\n",
      "3210it [36:58,  1.45it/s]\u001b[A\n",
      "3211it [36:59,  1.45it/s]\u001b[A\n",
      "3212it [37:00,  1.45it/s]\u001b[A\n",
      "3213it [37:01,  1.45it/s]\u001b[A\n",
      "3214it [37:01,  1.45it/s]\u001b[A\n",
      "3215it [37:02,  1.45it/s]\u001b[A\n",
      "3216it [37:03,  1.45it/s]\u001b[A\n",
      "3217it [37:03,  1.45it/s]\u001b[A\n",
      "3218it [37:04,  1.45it/s]\u001b[A\n",
      "3219it [37:05,  1.45it/s]\u001b[A\n",
      "3220it [37:05,  1.45it/s]\u001b[A\n",
      "3221it [37:06,  1.45it/s]\u001b[A\n",
      "3222it [37:07,  1.45it/s]\u001b[A\n",
      "3223it [37:07,  1.45it/s]\u001b[A\n",
      "3224it [37:08,  1.45it/s]\u001b[A\n",
      "3225it [37:09,  1.45it/s]\u001b[A\n",
      "3226it [37:09,  1.45it/s]\u001b[A\n",
      "3227it [37:10,  1.45it/s]\u001b[A\n",
      "3228it [37:11,  1.45it/s]\u001b[A\n",
      "3229it [37:12,  1.45it/s]\u001b[A\n",
      "3230it [37:12,  1.45it/s]\u001b[A\n",
      "3231it [37:13,  1.45it/s]\u001b[A\n",
      "3232it [37:14,  1.45it/s]\u001b[A\n",
      "3233it [37:14,  1.45it/s]\u001b[A\n",
      "3234it [37:15,  1.45it/s]\u001b[A\n",
      "3235it [37:16,  1.45it/s]\u001b[A\n",
      "3236it [37:16,  1.45it/s]\u001b[A\n",
      "3237it [37:17,  1.45it/s]\u001b[A\n",
      "3238it [37:18,  1.45it/s]\u001b[A\n",
      "3239it [37:18,  1.45it/s]\u001b[A\n",
      "3240it [37:19,  1.45it/s]\u001b[A\n",
      "3241it [37:20,  1.45it/s]\u001b[A\n",
      "3242it [37:20,  1.45it/s]\u001b[A\n",
      "3243it [37:21,  1.45it/s]\u001b[A\n",
      "3244it [37:22,  1.45it/s]\u001b[A\n",
      "3245it [37:23,  1.45it/s]\u001b[A\n",
      "3246it [37:23,  1.45it/s]\u001b[A\n",
      "3247it [37:24,  1.45it/s]\u001b[A\n",
      "3248it [37:25,  1.45it/s]\u001b[A\n",
      "3249it [37:25,  1.45it/s]\u001b[A\n",
      "3250it [37:26,  1.45it/s]\u001b[A\n",
      "3251it [37:27,  1.45it/s]\u001b[A\n",
      "3252it [37:27,  1.45it/s]\u001b[A\n",
      "3253it [37:28,  1.45it/s]\u001b[A\n",
      "3254it [37:29,  1.45it/s]\u001b[A\n",
      "3255it [37:29,  1.45it/s]\u001b[A\n",
      "3256it [37:30,  1.45it/s]\u001b[A\n",
      "3257it [37:31,  1.45it/s]\u001b[A\n",
      "3258it [37:31,  1.45it/s]\u001b[A\n",
      "3259it [37:32,  1.45it/s]\u001b[A\n",
      "3260it [37:33,  1.45it/s]\u001b[A\n",
      "3261it [37:34,  1.45it/s]\u001b[A\n",
      "3262it [37:34,  1.45it/s]\u001b[A\n",
      "3263it [37:35,  1.45it/s]\u001b[A\n",
      "3264it [37:36,  1.45it/s]\u001b[A\n",
      "3265it [37:36,  1.45it/s]\u001b[A\n",
      "3266it [37:37,  1.45it/s]\u001b[A\n",
      "3267it [37:38,  1.45it/s]\u001b[A\n",
      "3268it [37:38,  1.45it/s]\u001b[A\n",
      "3269it [37:39,  1.45it/s]\u001b[A\n",
      "3270it [37:40,  1.45it/s]\u001b[A\n",
      "3271it [37:40,  1.45it/s]\u001b[A\n",
      "3272it [37:41,  1.45it/s]\u001b[A\n",
      "3273it [37:42,  1.45it/s]\u001b[A\n",
      "3274it [37:43,  1.45it/s]\u001b[A\n",
      "3275it [37:43,  1.45it/s]\u001b[A\n",
      "3276it [37:44,  1.45it/s]\u001b[A\n",
      "3277it [37:45,  1.45it/s]\u001b[A\n",
      "3278it [37:45,  1.45it/s]\u001b[A\n",
      "3279it [37:46,  1.45it/s]\u001b[A\n",
      "3280it [37:47,  1.45it/s]\u001b[A\n",
      "3281it [37:47,  1.45it/s]\u001b[A\n",
      "3282it [37:48,  1.45it/s]\u001b[A\n",
      "3283it [37:49,  1.45it/s]\u001b[A\n",
      "3284it [37:49,  1.45it/s]\u001b[A\n",
      "3285it [37:50,  1.45it/s]\u001b[A\n",
      "3286it [37:51,  1.45it/s]\u001b[A\n",
      "3287it [37:51,  1.45it/s]\u001b[A\n",
      "3288it [37:52,  1.45it/s]\u001b[A\n",
      "3289it [37:53,  1.45it/s]\u001b[A\n",
      "3290it [37:54,  1.45it/s]\u001b[A\n",
      "3291it [37:54,  1.45it/s]\u001b[A\n",
      "3292it [37:55,  1.45it/s]\u001b[A\n",
      "3293it [37:56,  1.45it/s]\u001b[A\n",
      "3294it [37:56,  1.45it/s]\u001b[A\n",
      "3295it [37:57,  1.45it/s]\u001b[A\n",
      "3296it [37:58,  1.45it/s]\u001b[A\n",
      "3297it [37:58,  1.45it/s]\u001b[A\n",
      "3298it [37:59,  1.45it/s]\u001b[A\n",
      "3299it [38:00,  1.45it/s]\u001b[A\n",
      "3300it [38:00,  1.45it/s]\u001b[A\n",
      "3301it [38:01,  1.45it/s]\u001b[A\n",
      "3302it [38:02,  1.45it/s]\u001b[A\n",
      "3303it [38:02,  1.45it/s]\u001b[A\n",
      "3304it [38:03,  1.45it/s]\u001b[A\n",
      "3305it [38:04,  1.45it/s]\u001b[A\n",
      "3306it [38:05,  1.45it/s]\u001b[A\n",
      "3307it [38:05,  1.45it/s]\u001b[A\n",
      "3308it [38:06,  1.45it/s]\u001b[A\n",
      "3309it [38:07,  1.45it/s]\u001b[A\n",
      "3310it [38:07,  1.45it/s]\u001b[A\n",
      "3311it [38:08,  1.45it/s]\u001b[A\n",
      "3312it [38:09,  1.45it/s]\u001b[A\n",
      "3313it [38:09,  1.45it/s]\u001b[A\n",
      "3314it [38:10,  1.45it/s]\u001b[A\n",
      "3315it [38:11,  1.45it/s]\u001b[A\n",
      "3316it [38:11,  1.45it/s]\u001b[A\n",
      "3317it [38:12,  1.45it/s]\u001b[A\n",
      "3318it [38:13,  1.45it/s]\u001b[A\n",
      "3319it [38:14,  1.45it/s]\u001b[A\n",
      "3320it [38:14,  1.45it/s]\u001b[A\n",
      "3321it [38:15,  1.45it/s]\u001b[A\n",
      "3322it [38:16,  1.45it/s]\u001b[A\n",
      "3323it [38:16,  1.45it/s]\u001b[A\n",
      "3324it [38:17,  1.45it/s]\u001b[A\n",
      "3325it [38:18,  1.45it/s]\u001b[A\n",
      "3326it [38:18,  1.45it/s]\u001b[A\n",
      "3327it [38:19,  1.45it/s]\u001b[A\n",
      "3328it [38:20,  1.45it/s]\u001b[A\n",
      "3329it [38:20,  1.45it/s]\u001b[A\n",
      "3330it [38:21,  1.45it/s]\u001b[A\n",
      "3331it [38:22,  1.45it/s]\u001b[A\n",
      "3332it [38:22,  1.45it/s]\u001b[A\n",
      "3333it [38:23,  1.45it/s]\u001b[A\n",
      "3334it [38:24,  1.45it/s]\u001b[A\n",
      "3335it [38:25,  1.45it/s]\u001b[A\n",
      "3336it [38:25,  1.45it/s]\u001b[A\n",
      "3337it [38:26,  1.45it/s]\u001b[A\n",
      "3338it [38:27,  1.45it/s]\u001b[A\n",
      "3339it [38:27,  1.45it/s]\u001b[A\n",
      "3340it [38:28,  1.45it/s]\u001b[A\n",
      "3341it [38:29,  1.45it/s]\u001b[A\n",
      "3342it [38:29,  1.45it/s]\u001b[A\n",
      "3343it [38:30,  1.45it/s]\u001b[A\n",
      "3344it [38:31,  1.45it/s]\u001b[A\n",
      "3345it [38:31,  1.45it/s]\u001b[A\n",
      "3346it [38:32,  1.45it/s]\u001b[A\n",
      "3347it [38:33,  1.45it/s]\u001b[A\n",
      "3348it [38:33,  1.45it/s]\u001b[A\n",
      "3349it [38:34,  1.45it/s]\u001b[A\n",
      "3350it [38:35,  1.45it/s]\u001b[A\n",
      "3351it [38:36,  1.45it/s]\u001b[A\n",
      "3352it [38:36,  1.45it/s]\u001b[A\n",
      "3353it [38:37,  1.45it/s]\u001b[A\n",
      "3354it [38:38,  1.45it/s]\u001b[A\n",
      "3355it [38:38,  1.45it/s]\u001b[A\n",
      "3356it [38:39,  1.45it/s]\u001b[A\n",
      "3357it [38:40,  1.45it/s]\u001b[A\n",
      "3358it [38:40,  1.45it/s]\u001b[A\n",
      "3359it [38:41,  1.45it/s]\u001b[A\n",
      "3360it [38:42,  1.45it/s]\u001b[A\n",
      "3361it [38:42,  1.45it/s]\u001b[A\n",
      "3362it [38:43,  1.45it/s]\u001b[A\n",
      "3363it [38:44,  1.45it/s]\u001b[A\n",
      "3364it [38:45,  1.45it/s]\u001b[A\n",
      "3365it [38:45,  1.45it/s]\u001b[A\n",
      "3366it [38:46,  1.45it/s]\u001b[A\n",
      "3367it [38:47,  1.45it/s]\u001b[A\n",
      "3368it [38:47,  1.45it/s]\u001b[A\n",
      "3369it [38:48,  1.45it/s]\u001b[A\n",
      "3370it [38:49,  1.45it/s]\u001b[A\n",
      "3371it [38:49,  1.45it/s]\u001b[A\n",
      "3372it [38:50,  1.45it/s]\u001b[A\n",
      "3373it [38:51,  1.45it/s]\u001b[A\n",
      "3374it [38:51,  1.45it/s]\u001b[A\n",
      "3375it [38:52,  1.45it/s]\u001b[A\n",
      "3376it [38:53,  1.45it/s]\u001b[A\n",
      "3377it [38:53,  1.45it/s]\u001b[A\n",
      "3378it [38:54,  1.45it/s]\u001b[A\n",
      "3379it [38:55,  1.45it/s]\u001b[A\n",
      "3380it [38:56,  1.45it/s]\u001b[A\n",
      "3381it [38:56,  1.45it/s]\u001b[A\n",
      "3382it [38:57,  1.45it/s]\u001b[A\n",
      "3383it [38:58,  1.45it/s]\u001b[A\n",
      "3384it [38:58,  1.45it/s]\u001b[A\n",
      "3385it [38:59,  1.45it/s]\u001b[A\n",
      "3386it [39:00,  1.45it/s]\u001b[A\n",
      "3387it [39:00,  1.45it/s]\u001b[A\n",
      "3388it [39:01,  1.45it/s]\u001b[A\n",
      "3389it [39:02,  1.45it/s]\u001b[A\n",
      "3390it [39:02,  1.45it/s]\u001b[A\n",
      "3391it [39:03,  1.45it/s]\u001b[A\n",
      "3392it [39:04,  1.45it/s]\u001b[A\n",
      "3393it [39:04,  1.45it/s]\u001b[A\n",
      "3394it [39:05,  1.45it/s]\u001b[A\n",
      "3395it [39:06,  1.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3396it [39:07,  1.45it/s]\u001b[A\n",
      "3397it [39:07,  1.45it/s]\u001b[A\n",
      "3398it [39:08,  1.45it/s]\u001b[A\n",
      "3399it [39:09,  1.45it/s]\u001b[A\n",
      "3400it [39:09,  1.45it/s]\u001b[A\n",
      "3401it [39:10,  1.45it/s]\u001b[A\n",
      "3402it [39:11,  1.45it/s]\u001b[A\n",
      "3403it [39:11,  1.45it/s]\u001b[A\n",
      "3404it [39:12,  1.45it/s]\u001b[A\n",
      "3405it [39:13,  1.45it/s]\u001b[A\n",
      "3406it [39:13,  1.45it/s]\u001b[A\n",
      "3407it [39:14,  1.45it/s]\u001b[A\n",
      "3408it [39:15,  1.45it/s]\u001b[A\n",
      "3409it [39:15,  1.45it/s]\u001b[A\n",
      "3410it [39:16,  1.45it/s]\u001b[A\n",
      "3411it [39:17,  1.45it/s]\u001b[A\n",
      "3412it [39:18,  1.45it/s]\u001b[A\n",
      "3413it [39:18,  1.45it/s]\u001b[A\n",
      "3414it [39:19,  1.45it/s]\u001b[A\n",
      "3415it [39:20,  1.45it/s]\u001b[A\n",
      "3416it [39:20,  1.45it/s]\u001b[A\n",
      "3417it [39:21,  1.45it/s]\u001b[A\n",
      "3418it [39:22,  1.45it/s]\u001b[A\n",
      "3419it [39:22,  1.45it/s]\u001b[A\n",
      "3420it [39:23,  1.45it/s]\u001b[A\n",
      "3421it [39:24,  1.45it/s]\u001b[A\n",
      "3422it [39:24,  1.45it/s]\u001b[A\n",
      "3423it [39:25,  1.45it/s]\u001b[A\n",
      "3424it [39:26,  1.45it/s]\u001b[A\n",
      "3425it [39:27,  1.45it/s]\u001b[A\n",
      "3426it [39:27,  1.45it/s]\u001b[A\n",
      "3427it [39:28,  1.45it/s]\u001b[A\n",
      "3428it [39:29,  1.45it/s]\u001b[A\n",
      "3429it [39:29,  1.45it/s]\u001b[A\n",
      "3430it [39:30,  1.45it/s]\u001b[A\n",
      "3431it [39:31,  1.45it/s]\u001b[A\n",
      "3432it [39:31,  1.45it/s]\u001b[A\n",
      "3433it [39:32,  1.45it/s]\u001b[A\n",
      "3434it [39:33,  1.45it/s]\u001b[A\n",
      "3435it [39:33,  1.45it/s]\u001b[A\n",
      "3436it [39:34,  1.45it/s]\u001b[A\n",
      "3437it [39:35,  1.45it/s]\u001b[A\n",
      "3438it [39:35,  1.45it/s]\u001b[A\n",
      "3439it [39:36,  1.45it/s]\u001b[A\n",
      "3440it [39:37,  1.45it/s]\u001b[A\n",
      "3441it [39:38,  1.45it/s]\u001b[A\n",
      "3442it [39:38,  1.45it/s]\u001b[A\n",
      "3443it [39:39,  1.45it/s]\u001b[A\n",
      "3444it [39:40,  1.45it/s]\u001b[A\n",
      "3445it [39:40,  1.45it/s]\u001b[A\n",
      "3446it [39:41,  1.45it/s]\u001b[A\n",
      "3447it [39:42,  1.45it/s]\u001b[A\n",
      "3448it [39:42,  1.45it/s]\u001b[A\n",
      "3449it [39:43,  1.45it/s]\u001b[A\n",
      "3450it [39:44,  1.45it/s]\u001b[A\n",
      "3451it [39:44,  1.45it/s]\u001b[A\n",
      "3452it [39:45,  1.45it/s]\u001b[A\n",
      "3453it [39:46,  1.45it/s]\u001b[A\n",
      "3454it [39:46,  1.45it/s]\u001b[A\n",
      "3455it [39:47,  1.45it/s]\u001b[A\n",
      "3456it [39:48,  1.45it/s]\u001b[A\n",
      "3457it [39:49,  1.45it/s]\u001b[A\n",
      "3458it [39:49,  1.45it/s]\u001b[A\n",
      "3459it [39:50,  1.45it/s]\u001b[A\n",
      "3460it [39:51,  1.45it/s]\u001b[A\n",
      "3461it [39:51,  1.45it/s]\u001b[A\n",
      "3462it [39:52,  1.45it/s]\u001b[A\n",
      "3463it [39:53,  1.45it/s]\u001b[A\n",
      "3464it [39:53,  1.45it/s]\u001b[A\n",
      "3465it [39:54,  1.45it/s]\u001b[A\n",
      "3466it [39:55,  1.45it/s]\u001b[A\n",
      "3467it [39:55,  1.45it/s]\u001b[A\n",
      "3468it [39:56,  1.45it/s]\u001b[A\n",
      "3469it [39:57,  1.45it/s]\u001b[A\n",
      "3470it [39:58,  1.45it/s]\u001b[A\n",
      "3471it [39:58,  1.45it/s]\u001b[A\n",
      "3472it [39:59,  1.45it/s]\u001b[A\n",
      "3473it [40:00,  1.45it/s]\u001b[A\n",
      "3474it [40:00,  1.45it/s]\u001b[A\n",
      "3475it [40:01,  1.45it/s]\u001b[A\n",
      "3476it [40:02,  1.45it/s]\u001b[A\n",
      "3477it [40:02,  1.45it/s]\u001b[A\n",
      "3478it [40:03,  1.45it/s]\u001b[A\n",
      "3479it [40:04,  1.45it/s]\u001b[A\n",
      "3480it [40:04,  1.45it/s]\u001b[A\n",
      "3481it [40:05,  1.45it/s]\u001b[A\n",
      "3482it [40:06,  1.45it/s]\u001b[A\n",
      "3483it [40:06,  1.45it/s]\u001b[A\n",
      "3484it [40:07,  1.45it/s]\u001b[A\n",
      "3485it [40:08,  1.45it/s]\u001b[A\n",
      "3486it [40:09,  1.45it/s]\u001b[A\n",
      "3487it [40:09,  1.45it/s]\u001b[A\n",
      "3488it [40:10,  1.45it/s]\u001b[A\n",
      "3489it [40:11,  1.45it/s]\u001b[A\n",
      "3490it [40:11,  1.45it/s]\u001b[A\n",
      "3491it [40:12,  1.45it/s]\u001b[A\n",
      "3492it [40:13,  1.45it/s]\u001b[A\n",
      "3493it [40:13,  1.45it/s]\u001b[A\n",
      "3494it [40:14,  1.45it/s]\u001b[A\n",
      "3495it [40:15,  1.45it/s]\u001b[A\n",
      "3496it [40:15,  1.45it/s]\u001b[A\n",
      "3497it [40:16,  1.45it/s]\u001b[A\n",
      "3498it [40:17,  1.45it/s]\u001b[A\n",
      "3499it [40:17,  1.45it/s]\u001b[A\n",
      "3500it [40:18,  1.45it/s]\u001b[A\n",
      "3501it [40:19,  1.45it/s]\u001b[A\n",
      "3502it [40:20,  1.45it/s]\u001b[A\n",
      "3503it [40:20,  1.45it/s]\u001b[A\n",
      "3504it [40:21,  1.45it/s]\u001b[A\n",
      "3505it [40:22,  1.45it/s]\u001b[A\n",
      "3506it [40:22,  1.45it/s]\u001b[A\n",
      "3507it [40:23,  1.45it/s]\u001b[A\n",
      "3508it [40:24,  1.45it/s]\u001b[A\n",
      "3509it [40:24,  1.45it/s]\u001b[A\n",
      "3510it [40:25,  1.45it/s]\u001b[A\n",
      "3511it [40:26,  1.45it/s]\u001b[A\n",
      "3512it [40:26,  1.45it/s]\u001b[A\n",
      "3513it [40:27,  1.45it/s]\u001b[A\n",
      "3514it [40:28,  1.45it/s]\u001b[A\n",
      "3515it [40:28,  1.45it/s]\u001b[A\n",
      "3516it [40:29,  1.45it/s]\u001b[A\n",
      "3517it [40:30,  1.45it/s]\u001b[A\n",
      "3518it [40:31,  1.45it/s]\u001b[A\n",
      "3519it [40:31,  1.45it/s]\u001b[A\n",
      "3520it [40:32,  1.45it/s]\u001b[A\n",
      "3521it [40:33,  1.45it/s]\u001b[A\n",
      "3522it [40:33,  1.45it/s]\u001b[A\n",
      "3523it [40:34,  1.45it/s]\u001b[A\n",
      "3524it [40:35,  1.45it/s]\u001b[A\n",
      "3525it [40:35,  1.45it/s]\u001b[A\n",
      "3526it [40:36,  1.45it/s]\u001b[A\n",
      "3527it [40:37,  1.45it/s]\u001b[A\n",
      "3528it [40:37,  1.45it/s]\u001b[A\n",
      "3529it [40:38,  1.45it/s]\u001b[A\n",
      "3530it [40:39,  1.45it/s]\u001b[A\n",
      "3531it [40:40,  1.45it/s]\u001b[A\n",
      "3532it [40:40,  1.45it/s]\u001b[A\n",
      "3533it [40:41,  1.45it/s]\u001b[A\n",
      "3534it [40:42,  1.45it/s]\u001b[A\n",
      "3535it [40:42,  1.45it/s]\u001b[A\n",
      "3536it [40:43,  1.45it/s]\u001b[A\n",
      "3537it [40:44,  1.45it/s]\u001b[A\n",
      "3538it [40:44,  1.45it/s]\u001b[A\n",
      "3539it [40:45,  1.45it/s]\u001b[A\n",
      "3540it [40:46,  1.45it/s]\u001b[A\n",
      "3541it [40:46,  1.45it/s]\u001b[A\n",
      "3542it [40:47,  1.45it/s]\u001b[A\n",
      "3543it [40:48,  1.45it/s]\u001b[A\n",
      "3544it [40:48,  1.45it/s]\u001b[A\n",
      "3545it [40:49,  1.45it/s]\u001b[A\n",
      "3546it [40:50,  1.45it/s]\u001b[A\n",
      "3547it [40:51,  1.45it/s]\u001b[A\n",
      "3548it [40:51,  1.45it/s]\u001b[A\n",
      "3549it [40:52,  1.45it/s]\u001b[A\n",
      "3550it [40:53,  1.45it/s]\u001b[A\n",
      "3551it [40:53,  1.45it/s]\u001b[A\n",
      "3552it [40:54,  1.45it/s]\u001b[A\n",
      "3553it [40:55,  1.45it/s]\u001b[A\n",
      "3554it [40:55,  1.45it/s]\u001b[A\n",
      "3555it [40:56,  1.45it/s]\u001b[A\n",
      "3556it [40:57,  1.45it/s]\u001b[A\n",
      "3557it [40:57,  1.45it/s]\u001b[A\n",
      "3558it [40:58,  1.45it/s]\u001b[A\n",
      "3559it [40:59,  1.45it/s]\u001b[A\n",
      "3560it [40:59,  1.45it/s]\u001b[A\n",
      "3561it [41:00,  1.45it/s]\u001b[A\n",
      "3562it [41:01,  1.45it/s]\u001b[A\n",
      "3563it [41:02,  1.45it/s]\u001b[A\n",
      "3564it [41:02,  1.45it/s]\u001b[A\n",
      "3565it [41:03,  1.45it/s]\u001b[A\n",
      "3566it [41:04,  1.45it/s]\u001b[A\n",
      "3567it [41:04,  1.45it/s]\u001b[A\n",
      "3568it [41:05,  1.45it/s]\u001b[A\n",
      "3569it [41:06,  1.45it/s]\u001b[A\n",
      "3570it [41:06,  1.45it/s]\u001b[A\n",
      "3571it [41:07,  1.45it/s]\u001b[A\n",
      "3572it [41:08,  1.45it/s]\u001b[A\n",
      "3573it [41:08,  1.45it/s]\u001b[A\n",
      "3574it [41:09,  1.45it/s]\u001b[A\n",
      "3575it [41:10,  1.45it/s]\u001b[A\n",
      "3576it [41:10,  1.45it/s]\u001b[A\n",
      "3577it [41:11,  1.45it/s]\u001b[A\n",
      "3578it [41:12,  1.45it/s]\u001b[A\n",
      "3579it [41:13,  1.45it/s]\u001b[A\n",
      "3580it [41:13,  1.45it/s]\u001b[A\n",
      "3581it [41:14,  1.45it/s]\u001b[A\n",
      "3582it [41:15,  1.45it/s]\u001b[A\n",
      "3583it [41:15,  1.45it/s]\u001b[A\n",
      "3584it [41:16,  1.45it/s]\u001b[A\n",
      "3585it [41:17,  1.45it/s]\u001b[A\n",
      "3586it [41:17,  1.45it/s]\u001b[A\n",
      "3587it [41:18,  1.45it/s]\u001b[A\n",
      "3588it [41:19,  1.45it/s]\u001b[A\n",
      "3589it [41:19,  1.45it/s]\u001b[A\n",
      "3590it [41:20,  1.45it/s]\u001b[A\n",
      "3591it [41:21,  1.45it/s]\u001b[A\n",
      "3592it [41:22,  1.45it/s]\u001b[A\n",
      "3593it [41:22,  1.45it/s]\u001b[A\n",
      "3594it [41:23,  1.45it/s]\u001b[A\n",
      "3595it [41:24,  1.45it/s]\u001b[A\n",
      "3596it [41:24,  1.45it/s]\u001b[A\n",
      "3597it [41:25,  1.45it/s]\u001b[A\n",
      "3598it [41:26,  1.45it/s]\u001b[A\n",
      "3599it [41:26,  1.45it/s]\u001b[A\n",
      "3600it [41:27,  1.45it/s]\u001b[A\n",
      "3601it [41:28,  1.45it/s]\u001b[A\n",
      "3602it [41:28,  1.45it/s]\u001b[A\n",
      "3603it [41:29,  1.45it/s]\u001b[A\n",
      "3604it [41:30,  1.45it/s]\u001b[A\n",
      "3605it [41:30,  1.45it/s]\u001b[A\n",
      "3606it [41:31,  1.45it/s]\u001b[A\n",
      "3607it [41:32,  1.45it/s]\u001b[A\n",
      "3608it [41:33,  1.45it/s]\u001b[A\n",
      "3609it [41:33,  1.45it/s]\u001b[A\n",
      "3610it [41:34,  1.45it/s]\u001b[A\n",
      "3611it [41:35,  1.45it/s]\u001b[A\n",
      "3612it [41:35,  1.45it/s]\u001b[A\n",
      "3613it [41:36,  1.45it/s]\u001b[A\n",
      "3614it [41:37,  1.45it/s]\u001b[A\n",
      "3615it [41:37,  1.45it/s]\u001b[A\n",
      "3616it [41:38,  1.45it/s]\u001b[A\n",
      "3617it [41:39,  1.45it/s]\u001b[A\n",
      "3618it [41:39,  1.45it/s]\u001b[A\n",
      "3619it [41:40,  1.45it/s]\u001b[A\n",
      "3620it [41:41,  1.45it/s]\u001b[A\n",
      "3621it [41:41,  1.45it/s]\u001b[A\n",
      "3622it [41:42,  1.45it/s]\u001b[A\n",
      "3623it [41:43,  1.45it/s]\u001b[A\n",
      "3624it [41:44,  1.45it/s]\u001b[A\n",
      "3625it [41:44,  1.45it/s]\u001b[A\n",
      "3626it [41:45,  1.45it/s]\u001b[A\n",
      "3627it [41:46,  1.45it/s]\u001b[A\n",
      "3628it [41:46,  1.45it/s]\u001b[A\n",
      "3629it [41:47,  1.45it/s]\u001b[A\n",
      "3630it [41:48,  1.45it/s]\u001b[A\n",
      "3631it [41:48,  1.45it/s]\u001b[A\n",
      "3632it [41:49,  1.45it/s]\u001b[A\n",
      "3633it [41:50,  1.45it/s]\u001b[A\n",
      "3634it [41:50,  1.45it/s]\u001b[A\n",
      "3635it [41:51,  1.45it/s]\u001b[A\n",
      "3636it [41:52,  1.45it/s]\u001b[A\n",
      "3637it [41:52,  1.45it/s]\u001b[A\n",
      "3638it [41:53,  1.45it/s]\u001b[A\n",
      "3639it [41:54,  1.45it/s]\u001b[A\n",
      "3640it [41:55,  1.45it/s]\u001b[A\n",
      "3641it [41:55,  1.45it/s]\u001b[A\n",
      "3642it [41:56,  1.45it/s]\u001b[A\n",
      "3643it [41:57,  1.45it/s]\u001b[A\n",
      "3644it [41:57,  1.45it/s]\u001b[A\n",
      "3645it [41:58,  1.45it/s]\u001b[A\n",
      "3646it [41:59,  1.45it/s]\u001b[A\n",
      "3647it [41:59,  1.45it/s]\u001b[A\n",
      "3648it [42:00,  1.45it/s]\u001b[A\n",
      "3649it [42:01,  1.45it/s]\u001b[A\n",
      "3650it [42:01,  1.45it/s]\u001b[A\n",
      "3651it [42:02,  1.45it/s]\u001b[A\n",
      "3652it [42:03,  1.45it/s]\u001b[A\n",
      "3653it [42:04,  1.45it/s]\u001b[A\n",
      "3654it [42:04,  1.45it/s]\u001b[A\n",
      "3655it [42:05,  1.45it/s]\u001b[A\n",
      "3656it [42:06,  1.45it/s]\u001b[A\n",
      "3657it [42:06,  1.45it/s]\u001b[A\n",
      "3658it [42:07,  1.45it/s]\u001b[A\n",
      "3659it [42:08,  1.45it/s]\u001b[A\n",
      "3660it [42:08,  1.45it/s]\u001b[A\n",
      "3661it [42:09,  1.45it/s]\u001b[A\n",
      "3662it [42:10,  1.45it/s]\u001b[A\n",
      "3663it [42:10,  1.45it/s]\u001b[A\n",
      "3664it [42:11,  1.45it/s]\u001b[A\n",
      "3665it [42:12,  1.45it/s]\u001b[A\n",
      "3666it [42:12,  1.45it/s]\u001b[A\n",
      "3667it [42:13,  1.45it/s]\u001b[A\n",
      "3668it [42:14,  1.45it/s]\u001b[A\n",
      "3669it [42:15,  1.45it/s]\u001b[A\n",
      "3670it [42:15,  1.45it/s]\u001b[A\n",
      "3671it [42:16,  1.45it/s]\u001b[A\n",
      "3672it [42:17,  1.45it/s]\u001b[A\n",
      "3673it [42:17,  1.45it/s]\u001b[A\n",
      "3674it [42:18,  1.45it/s]\u001b[A\n",
      "3675it [42:19,  1.45it/s]\u001b[A\n",
      "3676it [42:19,  1.45it/s]\u001b[A\n",
      "3677it [42:20,  1.45it/s]\u001b[A\n",
      "3678it [42:21,  1.45it/s]\u001b[A\n",
      "3679it [42:21,  1.45it/s]\u001b[A\n",
      "3680it [42:22,  1.45it/s]\u001b[A\n",
      "3681it [42:23,  1.45it/s]\u001b[A\n",
      "3682it [42:23,  1.45it/s]\u001b[A\n",
      "3683it [42:24,  1.45it/s]\u001b[A\n",
      "3684it [42:25,  1.45it/s]\u001b[A\n",
      "3685it [42:26,  1.45it/s]\u001b[A\n",
      "3686it [42:26,  1.45it/s]\u001b[A\n",
      "3687it [42:27,  1.45it/s]\u001b[A\n",
      "3688it [42:28,  1.45it/s]\u001b[A\n",
      "3689it [42:28,  1.45it/s]\u001b[A\n",
      "3690it [42:29,  1.45it/s]\u001b[A\n",
      "3691it [42:30,  1.45it/s]\u001b[A\n",
      "3692it [42:30,  1.45it/s]\u001b[A\n",
      "3693it [42:31,  1.45it/s]\u001b[A\n",
      "3694it [42:32,  1.45it/s]\u001b[A\n",
      "3695it [42:32,  1.45it/s]\u001b[A\n",
      "3696it [42:33,  1.45it/s]\u001b[A\n",
      "3697it [42:34,  1.45it/s]\u001b[A\n",
      "3698it [42:34,  1.45it/s]\u001b[A\n",
      "3699it [42:35,  1.45it/s]\u001b[A\n",
      "3700it [42:36,  1.45it/s]\u001b[A\n",
      "3701it [42:37,  1.45it/s]\u001b[A\n",
      "3702it [42:37,  1.45it/s]\u001b[A\n",
      "3703it [42:38,  1.45it/s]\u001b[A\n",
      "3704it [42:39,  1.45it/s]\u001b[A\n",
      "3705it [42:39,  1.45it/s]\u001b[A\n",
      "3706it [42:40,  1.45it/s]\u001b[A\n",
      "3707it [42:41,  1.45it/s]\u001b[A\n",
      "3708it [42:41,  1.45it/s]\u001b[A\n",
      "3709it [42:42,  1.45it/s]\u001b[A\n",
      "3710it [42:43,  1.45it/s]\u001b[A\n",
      "3711it [42:43,  1.45it/s]\u001b[A\n",
      "3712it [42:44,  1.45it/s]\u001b[A\n",
      "3713it [42:45,  1.45it/s]\u001b[A\n",
      "3714it [42:46,  1.45it/s]\u001b[A\n",
      "3715it [42:46,  1.45it/s]\u001b[A\n",
      "3716it [42:47,  1.45it/s]\u001b[A\n",
      "3717it [42:48,  1.45it/s]\u001b[A\n",
      "3718it [42:48,  1.45it/s]\u001b[A\n",
      "3719it [42:49,  1.45it/s]\u001b[A\n",
      "3720it [42:50,  1.45it/s]\u001b[A\n",
      "3721it [42:50,  1.45it/s]\u001b[A\n",
      "3722it [42:51,  1.45it/s]\u001b[A\n",
      "3723it [42:52,  1.45it/s]\u001b[A\n",
      "3724it [42:52,  1.45it/s]\u001b[A\n",
      "3725it [42:53,  1.45it/s]\u001b[A\n",
      "3726it [42:54,  1.45it/s]\u001b[A\n",
      "3727it [42:54,  1.45it/s]\u001b[A\n",
      "3728it [42:55,  1.45it/s]\u001b[A\n",
      "3729it [42:56,  1.45it/s]\u001b[A\n",
      "3730it [42:57,  1.45it/s]\u001b[A\n",
      "3731it [42:57,  1.45it/s]\u001b[A\n",
      "3732it [42:58,  1.45it/s]\u001b[A\n",
      "3733it [42:59,  1.45it/s]\u001b[A\n",
      "3734it [42:59,  1.45it/s]\u001b[A\n",
      "3735it [43:00,  1.45it/s]\u001b[A\n",
      "3736it [43:01,  1.45it/s]\u001b[A\n",
      "3737it [43:01,  1.45it/s]\u001b[A\n",
      "3738it [43:02,  1.45it/s]\u001b[A\n",
      "3739it [43:03,  1.45it/s]\u001b[A\n",
      "3740it [43:03,  1.45it/s]\u001b[A\n",
      "3741it [43:04,  1.45it/s]\u001b[A\n",
      "3742it [43:05,  1.45it/s]\u001b[A\n",
      "3743it [43:05,  1.45it/s]\u001b[A\n",
      "3744it [43:06,  1.45it/s]\u001b[A\n",
      "3745it [43:07,  1.45it/s]\u001b[A\n",
      "3746it [43:08,  1.45it/s]\u001b[A\n",
      "3747it [43:08,  1.45it/s]\u001b[A\n",
      "3748it [43:09,  1.45it/s]\u001b[A\n",
      "3749it [43:10,  1.45it/s]\u001b[A\n",
      "3750it [43:10,  1.45it/s]\u001b[A\n",
      "3751it [43:11,  1.45it/s]\u001b[A\n",
      "3752it [43:12,  1.45it/s]\u001b[A\n",
      "3753it [43:12,  1.45it/s]\u001b[A\n",
      "3754it [43:13,  1.45it/s]\u001b[A\n",
      "3755it [43:14,  1.45it/s]\u001b[A\n",
      "3756it [43:14,  1.45it/s]\u001b[A\n",
      "3757it [43:15,  1.45it/s]\u001b[A\n",
      "3758it [43:16,  1.45it/s]\u001b[A\n",
      "3759it [43:17,  1.45it/s]\u001b[A\n",
      "3760it [43:17,  1.45it/s]\u001b[A\n",
      "3761it [43:18,  1.45it/s]\u001b[A\n",
      "3762it [43:19,  1.45it/s]\u001b[A\n",
      "3763it [43:19,  1.45it/s]\u001b[A\n",
      "3764it [43:20,  1.45it/s]\u001b[A\n",
      "3765it [43:21,  1.45it/s]\u001b[A\n",
      "3766it [43:21,  1.45it/s]\u001b[A\n",
      "3767it [43:22,  1.45it/s]\u001b[A\n",
      "3768it [43:23,  1.45it/s]\u001b[A\n",
      "3769it [43:23,  1.45it/s]\u001b[A\n",
      "3770it [43:24,  1.45it/s]\u001b[A\n",
      "3771it [43:25,  1.45it/s]\u001b[A\n",
      "3772it [43:25,  1.45it/s]\u001b[A\n",
      "3773it [43:26,  1.45it/s]\u001b[A\n",
      "3774it [43:27,  1.45it/s]\u001b[A\n",
      "3775it [43:28,  1.45it/s]\u001b[A\n",
      "3776it [43:28,  1.45it/s]\u001b[A\n",
      "3777it [43:29,  1.45it/s]\u001b[A\n",
      "3778it [43:30,  1.45it/s]\u001b[A\n",
      "3779it [43:30,  1.45it/s]\u001b[A\n",
      "3780it [43:31,  1.45it/s]\u001b[A\n",
      "3781it [43:32,  1.45it/s]\u001b[A\n",
      "3782it [43:32,  1.45it/s]\u001b[A\n",
      "3783it [43:33,  1.45it/s]\u001b[A\n",
      "3784it [43:34,  1.45it/s]\u001b[A\n",
      "3785it [43:34,  1.45it/s]\u001b[A\n",
      "3786it [43:35,  1.45it/s]\u001b[A\n",
      "3787it [43:36,  1.45it/s]\u001b[A\n",
      "3788it [43:36,  1.45it/s]\u001b[A\n",
      "3789it [43:37,  1.45it/s]\u001b[A\n",
      "3790it [43:38,  1.45it/s]\u001b[A\n",
      "3791it [43:39,  1.45it/s]\u001b[A\n",
      "3792it [43:39,  1.45it/s]\u001b[A\n",
      "3793it [43:40,  1.45it/s]\u001b[A\n",
      "3794it [43:41,  1.45it/s]\u001b[A\n",
      "3795it [43:41,  1.45it/s]\u001b[A\n",
      "3796it [43:42,  1.45it/s]\u001b[A\n",
      "3797it [43:43,  1.45it/s]\u001b[A\n",
      "3798it [43:43,  1.45it/s]\u001b[A\n",
      "3799it [43:44,  1.45it/s]\u001b[A\n",
      "3800it [43:45,  1.45it/s]\u001b[A\n",
      "3801it [43:45,  1.45it/s]\u001b[A\n",
      "3802it [43:46,  1.45it/s]\u001b[A\n",
      "3803it [43:47,  1.45it/s]\u001b[A\n",
      "3804it [43:48,  1.45it/s]\u001b[A\n",
      "3805it [43:48,  1.45it/s]\u001b[A\n",
      "3806it [43:49,  1.45it/s]\u001b[A\n",
      "3807it [43:50,  1.45it/s]\u001b[A\n",
      "3808it [43:50,  1.45it/s]\u001b[A\n",
      "3809it [43:51,  1.45it/s]\u001b[A\n",
      "3810it [43:52,  1.45it/s]\u001b[A\n",
      "3811it [43:52,  1.45it/s]\u001b[A\n",
      "3812it [43:53,  1.45it/s]\u001b[A\n",
      "3813it [43:54,  1.45it/s]\u001b[A\n",
      "3814it [43:54,  1.45it/s]\u001b[A\n",
      "3815it [43:55,  1.45it/s]\u001b[A\n",
      "3816it [43:56,  1.45it/s]\u001b[A\n",
      "3817it [43:56,  1.45it/s]\u001b[A\n",
      "3818it [43:57,  1.45it/s]\u001b[A\n",
      "3819it [43:58,  1.45it/s]\u001b[A\n",
      "3820it [43:59,  1.45it/s]\u001b[A\n",
      "3821it [43:59,  1.45it/s]\u001b[A\n",
      "3822it [44:00,  1.45it/s]\u001b[A\n",
      "3823it [44:01,  1.45it/s]\u001b[A\n",
      "3824it [44:01,  1.45it/s]\u001b[A\n",
      "3825it [44:02,  1.45it/s]\u001b[A\n",
      "3826it [44:03,  1.45it/s]\u001b[A\n",
      "3827it [44:03,  1.45it/s]\u001b[A\n",
      "3828it [44:04,  1.45it/s]\u001b[A\n",
      "3829it [44:05,  1.45it/s]\u001b[A\n",
      "3830it [44:05,  1.45it/s]\u001b[A\n",
      "3831it [44:06,  1.45it/s]\u001b[A\n",
      "3832it [44:07,  1.45it/s]\u001b[A\n",
      "3833it [44:07,  1.45it/s]\u001b[A\n",
      "3834it [44:08,  1.45it/s]\u001b[A\n",
      "3835it [44:09,  1.45it/s]\u001b[A\n",
      "3836it [44:10,  1.45it/s]\u001b[A\n",
      "3837it [44:10,  1.45it/s]\u001b[A\n",
      "3838it [44:11,  1.45it/s]\u001b[A\n",
      "3839it [44:12,  1.45it/s]\u001b[A\n",
      "3840it [44:12,  1.45it/s]\u001b[A\n",
      "3841it [44:13,  1.45it/s]\u001b[A\n",
      "3842it [44:14,  1.45it/s]\u001b[A\n",
      "3843it [44:14,  1.45it/s]\u001b[A\n",
      "3844it [44:15,  1.45it/s]\u001b[A\n",
      "3845it [44:16,  1.45it/s]\u001b[A\n",
      "3846it [44:16,  1.45it/s]\u001b[A\n",
      "3847it [44:17,  1.45it/s]\u001b[A\n",
      "3848it [44:18,  1.45it/s]\u001b[A\n",
      "3849it [44:19,  1.45it/s]\u001b[A\n",
      "3850it [44:19,  1.45it/s]\u001b[A\n",
      "3851it [44:20,  1.45it/s]\u001b[A\n",
      "3852it [44:21,  1.45it/s]\u001b[A\n",
      "3853it [44:21,  1.45it/s]\u001b[A\n",
      "3854it [44:22,  1.45it/s]\u001b[A\n",
      "3855it [44:23,  1.45it/s]\u001b[A\n",
      "3856it [44:23,  1.45it/s]\u001b[A\n",
      "3857it [44:24,  1.45it/s]\u001b[A\n",
      "3858it [44:25,  1.45it/s]\u001b[A\n",
      "3859it [44:25,  1.45it/s]\u001b[A\n",
      "3860it [44:26,  1.45it/s]\u001b[A\n",
      "3861it [44:27,  1.45it/s]\u001b[A\n",
      "3862it [44:27,  1.45it/s]\u001b[A\n",
      "3863it [44:28,  1.45it/s]\u001b[A\n",
      "3864it [44:29,  1.45it/s]\u001b[A\n",
      "3865it [44:30,  1.45it/s]\u001b[A\n",
      "3866it [44:30,  1.45it/s]\u001b[A\n",
      "3867it [44:31,  1.45it/s]\u001b[A\n",
      "3868it [44:32,  1.45it/s]\u001b[A\n",
      "3869it [44:32,  1.45it/s]\u001b[A\n",
      "3870it [44:33,  1.45it/s]\u001b[A\n",
      "3871it [44:34,  1.45it/s]\u001b[A\n",
      "3872it [44:34,  1.45it/s]\u001b[A\n",
      "3873it [44:35,  1.45it/s]\u001b[A\n",
      "3874it [44:36,  1.45it/s]\u001b[A\n",
      "3875it [44:36,  1.45it/s]\u001b[A\n",
      "3876it [44:37,  1.45it/s]\u001b[A\n",
      "3877it [44:38,  1.45it/s]\u001b[A\n",
      "3878it [44:38,  1.45it/s]\u001b[A\n",
      "3879it [44:39,  1.45it/s]\u001b[A\n",
      "3880it [44:40,  1.45it/s]\u001b[A\n",
      "3881it [44:41,  1.45it/s]\u001b[A\n",
      "3882it [44:41,  1.45it/s]\u001b[A\n",
      "3883it [44:42,  1.45it/s]\u001b[A\n",
      "3884it [44:43,  1.45it/s]\u001b[A\n",
      "3885it [44:43,  1.45it/s]\u001b[A\n",
      "3886it [44:44,  1.45it/s]\u001b[A\n",
      "3887it [44:45,  1.45it/s]\u001b[A\n",
      "3888it [44:45,  1.45it/s]\u001b[A\n",
      "3889it [44:46,  1.45it/s]\u001b[A\n",
      "3890it [44:47,  1.45it/s]\u001b[A\n",
      "3891it [44:47,  1.45it/s]\u001b[A\n",
      "3892it [44:48,  1.45it/s]\u001b[A\n",
      "3893it [44:49,  1.45it/s]\u001b[A\n",
      "3894it [44:49,  1.45it/s]\u001b[A\n",
      "3895it [44:50,  1.45it/s]\u001b[A\n",
      "3896it [44:51,  1.45it/s]\u001b[A\n",
      "3897it [44:52,  1.45it/s]\u001b[A\n",
      "3898it [44:52,  1.45it/s]\u001b[A\n",
      "3899it [44:53,  1.45it/s]\u001b[A\n",
      "3900it [44:54,  1.45it/s]\u001b[A\n",
      "3901it [44:54,  1.45it/s]\u001b[A\n",
      "3902it [44:55,  1.45it/s]\u001b[A\n",
      "3903it [44:56,  1.45it/s]\u001b[A\n",
      "3904it [44:56,  1.45it/s]\u001b[A\n",
      "3905it [44:57,  1.45it/s]\u001b[A\n",
      "3906it [44:58,  1.45it/s]\u001b[A\n",
      "3907it [44:58,  1.45it/s]\u001b[A\n",
      "3908it [44:59,  1.45it/s]\u001b[A\n",
      "3909it [45:00,  1.45it/s]\u001b[A\n",
      "3910it [45:01,  1.45it/s]\u001b[A\n",
      "3911it [45:01,  1.45it/s]\u001b[A\n",
      "3912it [45:02,  1.45it/s]\u001b[A\n",
      "3913it [45:03,  1.45it/s]\u001b[A\n",
      "3914it [45:03,  1.45it/s]\u001b[A\n",
      "3915it [45:04,  1.45it/s]\u001b[A\n",
      "3916it [45:05,  1.45it/s]\u001b[A\n",
      "3917it [45:05,  1.45it/s]\u001b[A\n",
      "3918it [45:06,  1.45it/s]\u001b[A\n",
      "3919it [45:07,  1.45it/s]\u001b[A\n",
      "3920it [45:07,  1.45it/s]\u001b[A\n",
      "3921it [45:08,  1.45it/s]\u001b[A\n",
      "3922it [45:09,  1.45it/s]\u001b[A\n",
      "3923it [45:09,  1.45it/s]\u001b[A\n",
      "3924it [45:10,  1.45it/s]\u001b[A\n",
      "3925it [45:11,  1.45it/s]\u001b[A\n",
      "3926it [45:12,  1.45it/s]\u001b[A\n",
      "3927it [45:12,  1.45it/s]\u001b[A\n",
      "3928it [45:13,  1.45it/s]\u001b[A\n",
      "3929it [45:14,  1.45it/s]\u001b[A\n",
      "3930it [45:14,  1.45it/s]\u001b[A\n",
      "3931it [45:15,  1.45it/s]\u001b[A\n",
      "3932it [45:16,  1.45it/s]\u001b[A\n",
      "3933it [45:16,  1.45it/s]\u001b[A\n",
      "3934it [45:17,  1.45it/s]\u001b[A\n",
      "3935it [45:18,  1.45it/s]\u001b[A\n",
      "3936it [45:18,  1.45it/s]\u001b[A\n",
      "3937it [45:19,  1.45it/s]\u001b[A\n",
      "3938it [45:20,  1.45it/s]\u001b[A\n",
      "3939it [45:20,  1.45it/s]\u001b[A\n",
      "3940it [45:21,  1.45it/s]\u001b[A\n",
      "3941it [45:22,  1.45it/s]\u001b[A\n",
      "3942it [45:23,  1.45it/s]\u001b[A\n",
      "3943it [45:23,  1.45it/s]\u001b[A\n",
      "3944it [45:24,  1.45it/s]\u001b[A\n",
      "3945it [45:25,  1.45it/s]\u001b[A\n",
      "3946it [45:25,  1.45it/s]\u001b[A\n",
      "3947it [45:26,  1.45it/s]\u001b[A\n",
      "3948it [45:27,  1.45it/s]\u001b[A\n",
      "3949it [45:27,  1.45it/s]\u001b[A\n",
      "3950it [45:28,  1.45it/s]\u001b[A\n",
      "3951it [45:29,  1.45it/s]\u001b[A\n",
      "3952it [45:29,  1.45it/s]\u001b[A\n",
      "3953it [45:30,  1.45it/s]\u001b[A\n",
      "3954it [45:31,  1.45it/s]\u001b[A\n",
      "3955it [45:32,  1.45it/s]\u001b[A\n",
      "3956it [45:32,  1.45it/s]\u001b[A\n",
      "3957it [45:33,  1.45it/s]\u001b[A\n",
      "3958it [45:34,  1.45it/s]\u001b[A\n",
      "3959it [45:34,  1.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3960it [45:35,  1.45it/s]\u001b[A\n",
      "3961it [45:36,  1.45it/s]\u001b[A\n",
      "3962it [45:36,  1.45it/s]\u001b[A\n",
      "3963it [45:37,  1.45it/s]\u001b[A\n",
      "3964it [45:38,  1.45it/s]\u001b[A\n",
      "3965it [45:38,  1.45it/s]\u001b[A\n",
      "3966it [45:39,  1.45it/s]\u001b[A\n",
      "3967it [45:40,  1.45it/s]\u001b[A\n",
      "3968it [45:40,  1.45it/s]\u001b[A\n",
      "3969it [45:41,  1.45it/s]\u001b[A\n",
      "3970it [45:42,  1.45it/s]\u001b[A\n",
      "3971it [45:43,  1.45it/s]\u001b[A\n",
      "3972it [45:43,  1.45it/s]\u001b[A\n",
      "3973it [45:44,  1.45it/s]\u001b[A\n",
      "3974it [45:45,  1.45it/s]\u001b[A\n",
      "3975it [45:45,  1.45it/s]\u001b[A\n",
      "3976it [45:46,  1.45it/s]\u001b[A\n",
      "3977it [45:47,  1.45it/s]\u001b[A\n",
      "3978it [45:47,  1.45it/s]\u001b[A\n",
      "3979it [45:48,  1.45it/s]\u001b[A\n",
      "3980it [45:49,  1.45it/s]\u001b[A\n",
      "3981it [45:49,  1.45it/s]\u001b[A\n",
      "3982it [45:50,  1.45it/s]\u001b[A\n",
      "3983it [45:51,  1.45it/s]\u001b[A\n",
      "3984it [45:51,  1.45it/s]\u001b[A\n",
      "3985it [45:52,  1.45it/s]\u001b[A\n",
      "3986it [45:53,  1.45it/s]\u001b[A\n",
      "3987it [45:54,  1.46it/s]\u001b[A\n",
      "3988it [45:54,  1.45it/s]\u001b[A\n",
      "3989it [45:55,  1.45it/s]\u001b[A\n",
      "3990it [45:56,  1.45it/s]\u001b[A\n",
      "3991it [45:56,  1.45it/s]\u001b[A\n",
      "3992it [45:57,  1.45it/s]\u001b[A\n",
      "3993it [45:58,  1.45it/s]\u001b[A\n",
      "3994it [45:58,  1.45it/s]\u001b[A\n",
      "3995it [45:59,  1.45it/s]\u001b[A\n",
      "3996it [46:00,  1.45it/s]\u001b[A\n",
      "3997it [46:00,  1.45it/s]\u001b[A\n",
      "3998it [46:01,  1.45it/s]\u001b[A\n",
      "3999it [46:02,  1.45it/s]\u001b[A\n",
      "4000it [46:02,  1.45it/s]\u001b[A\n",
      "4001it [46:03,  1.45it/s]\u001b[A\n",
      "4002it [46:04,  1.45it/s]\u001b[A\n",
      "4003it [46:05,  1.45it/s]\u001b[A\n",
      "4004it [46:05,  1.45it/s]\u001b[A\n",
      "4005it [46:06,  1.45it/s]\u001b[A\n",
      "4006it [46:07,  1.45it/s]\u001b[A\n",
      "4007it [46:07,  1.45it/s]\u001b[A\n",
      "4008it [46:08,  1.45it/s]\u001b[A\n",
      "4009it [46:09,  1.45it/s]\u001b[A\n",
      "4010it [46:09,  1.45it/s]\u001b[A\n",
      "4011it [46:10,  1.45it/s]\u001b[A\n",
      "4012it [46:11,  1.45it/s]\u001b[A\n",
      "4013it [46:11,  1.45it/s]\u001b[A\n",
      "4014it [46:12,  1.45it/s]\u001b[A\n",
      "4015it [46:13,  1.45it/s]\u001b[A\n",
      "4016it [46:14,  1.45it/s]\u001b[A\n",
      "4017it [46:14,  1.45it/s]\u001b[A\n",
      "4018it [46:15,  1.45it/s]\u001b[A\n",
      "4019it [46:16,  1.45it/s]\u001b[A\n",
      "4020it [46:16,  1.45it/s]\u001b[A\n",
      "4021it [46:17,  1.45it/s]\u001b[A\n",
      "4022it [46:18,  1.45it/s]\u001b[A\n",
      "4023it [46:18,  1.45it/s]\u001b[A\n",
      "4024it [46:19,  1.45it/s]\u001b[A\n",
      "4025it [46:20,  1.45it/s]\u001b[A\n",
      "4026it [46:20,  1.45it/s]\u001b[A\n",
      "4027it [46:21,  1.45it/s]\u001b[A\n",
      "4028it [46:22,  1.45it/s]\u001b[A\n",
      "4029it [46:22,  1.45it/s]\u001b[A\n",
      "4030it [46:23,  1.45it/s]\u001b[A\n",
      "4031it [46:24,  1.45it/s]\u001b[A\n",
      "4032it [46:25,  1.45it/s]\u001b[A\n",
      "4033it [46:25,  1.45it/s]\u001b[A\n",
      "4034it [46:26,  1.45it/s]\u001b[A\n",
      "4035it [46:27,  1.45it/s]\u001b[A\n",
      "4036it [46:27,  1.45it/s]\u001b[A\n",
      "4037it [46:28,  1.45it/s]\u001b[A\n",
      "4038it [46:29,  1.45it/s]\u001b[A\n",
      "4039it [46:29,  1.45it/s]\u001b[A\n",
      "4040it [46:30,  1.45it/s]\u001b[A\n",
      "4041it [46:31,  1.45it/s]\u001b[A\n",
      "4042it [46:31,  1.45it/s]\u001b[A\n",
      "4043it [46:32,  1.45it/s]\u001b[A\n",
      "4044it [46:33,  1.45it/s]\u001b[A\n",
      "4045it [46:33,  1.45it/s]\u001b[A\n",
      "4046it [46:34,  1.46it/s]\u001b[A\n",
      "4047it [46:35,  1.45it/s]\u001b[A\n",
      "4048it [46:36,  1.45it/s]\u001b[A\n",
      "4049it [46:36,  1.45it/s]\u001b[A\n",
      "4050it [46:37,  1.45it/s]\u001b[A\n",
      "4051it [46:38,  1.45it/s]\u001b[A\n",
      "4052it [46:38,  1.45it/s]\u001b[A\n",
      "4053it [46:39,  1.45it/s]\u001b[A\n",
      "4054it [46:40,  1.45it/s]\u001b[A\n",
      "4055it [46:40,  1.45it/s]\u001b[A\n",
      "4056it [46:41,  1.45it/s]\u001b[A\n",
      "4057it [46:42,  1.45it/s]\u001b[A\n",
      "4058it [46:42,  1.45it/s]\u001b[A\n",
      "4059it [46:43,  1.45it/s]\u001b[A\n",
      "4060it [46:44,  1.45it/s]\u001b[A\n",
      "4061it [46:44,  1.45it/s]\u001b[A\n",
      "4062it [46:45,  1.45it/s]\u001b[A\n",
      "4063it [46:46,  1.45it/s]\u001b[A\n",
      "4064it [46:47,  1.45it/s]\u001b[A\n",
      "4065it [46:47,  1.45it/s]\u001b[A\n",
      "4066it [46:48,  1.45it/s]\u001b[A\n",
      "4067it [46:49,  1.45it/s]\u001b[A\n",
      "4068it [46:49,  1.45it/s]\u001b[A\n",
      "4069it [46:50,  1.45it/s]\u001b[A\n",
      "4070it [46:51,  1.45it/s]\u001b[A\n",
      "4071it [46:51,  1.45it/s]\u001b[A\n",
      "4072it [46:52,  1.45it/s]\u001b[A\n",
      "4073it [46:53,  1.45it/s]\u001b[A\n",
      "4074it [46:53,  1.45it/s]\u001b[A\n",
      "4075it [46:54,  1.45it/s]\u001b[A\n",
      "4076it [46:55,  1.45it/s]\u001b[A\n",
      "4077it [46:55,  1.45it/s]\u001b[A\n",
      "4078it [46:56,  1.45it/s]\u001b[A\n",
      "4079it [46:57,  1.45it/s]\u001b[A\n",
      "4080it [46:58,  1.45it/s]\u001b[A\n",
      "4081it [46:58,  1.45it/s]\u001b[A\n",
      "4082it [46:59,  1.45it/s]\u001b[A\n",
      "4083it [47:00,  1.45it/s]\u001b[A\n",
      "4084it [47:00,  1.45it/s]\u001b[A\n",
      "4085it [47:01,  1.45it/s]\u001b[A\n",
      "4086it [47:02,  1.45it/s]\u001b[A\n",
      "4087it [47:02,  1.45it/s]\u001b[A\n",
      "4088it [47:03,  1.45it/s]\u001b[A\n",
      "4089it [47:04,  1.45it/s]\u001b[A\n",
      "4090it [47:04,  1.45it/s]\u001b[A\n",
      "4091it [47:05,  1.45it/s]\u001b[A\n",
      "4092it [47:06,  1.45it/s]\u001b[A\n",
      "4093it [47:07,  1.45it/s]\u001b[A\n",
      "4094it [47:07,  1.45it/s]\u001b[A\n",
      "4095it [47:08,  1.45it/s]\u001b[A\n",
      "4096it [47:09,  1.45it/s]\u001b[A\n",
      "4097it [47:09,  1.45it/s]\u001b[A\n",
      "4098it [47:10,  1.45it/s]\u001b[A\n",
      "4099it [47:11,  1.45it/s]\u001b[A\n",
      "4100it [47:11,  1.45it/s]\u001b[A\n",
      "4101it [47:12,  1.45it/s]\u001b[A\n",
      "4102it [47:13,  1.45it/s]\u001b[A\n",
      "4103it [47:13,  1.45it/s]\u001b[A\n",
      "4104it [47:14,  1.45it/s]\u001b[A\n",
      "4105it [47:15,  1.45it/s]\u001b[A\n",
      "4106it [47:15,  1.45it/s]\u001b[A\n",
      "4107it [47:16,  1.45it/s]\u001b[A\n",
      "4108it [47:17,  1.45it/s]\u001b[A\n",
      "4109it [47:18,  1.45it/s]\u001b[A\n",
      "4110it [47:18,  1.45it/s]\u001b[A\n",
      "4111it [47:19,  1.45it/s]\u001b[A\n",
      "4112it [47:20,  1.45it/s]\u001b[A\n",
      "4113it [47:20,  1.45it/s]\u001b[A\n",
      "4114it [47:21,  1.45it/s]\u001b[A\n",
      "4115it [47:22,  1.45it/s]\u001b[A\n",
      "4116it [47:22,  1.45it/s]\u001b[A\n",
      "4117it [47:23,  1.45it/s]\u001b[A\n",
      "4118it [47:24,  1.45it/s]\u001b[A\n",
      "4119it [47:24,  1.45it/s]\u001b[A\n",
      "4120it [47:25,  1.45it/s]\u001b[A\n",
      "4121it [47:26,  1.45it/s]\u001b[A\n",
      "4122it [47:26,  1.45it/s]\u001b[A\n",
      "4123it [47:27,  1.45it/s]\u001b[A\n",
      "4124it [47:28,  1.45it/s]\u001b[A\n",
      "4125it [47:29,  1.45it/s]\u001b[A\n",
      "4126it [47:29,  1.45it/s]\u001b[A\n",
      "4127it [47:30,  1.45it/s]\u001b[A\n",
      "4128it [47:31,  1.45it/s]\u001b[A\n",
      "4129it [47:31,  1.45it/s]\u001b[A\n",
      "4130it [47:32,  1.45it/s]\u001b[A\n",
      "4131it [47:33,  1.45it/s]\u001b[A\n",
      "4132it [47:33,  1.45it/s]\u001b[A\n",
      "4133it [47:34,  1.45it/s]\u001b[A\n",
      "4134it [47:35,  1.45it/s]\u001b[A\n",
      "4135it [47:35,  1.45it/s]\u001b[A\n",
      "4136it [47:36,  1.45it/s]\u001b[A\n",
      "4137it [47:37,  1.45it/s]\u001b[A\n",
      "4138it [47:38,  1.45it/s]\u001b[A\n",
      "4139it [47:38,  1.45it/s]\u001b[A\n",
      "4140it [47:39,  1.45it/s]\u001b[A\n",
      "4141it [47:40,  1.45it/s]\u001b[A\n",
      "4142it [47:40,  1.45it/s]\u001b[A\n",
      "4143it [47:41,  1.45it/s]\u001b[A\n",
      "4144it [47:42,  1.45it/s]\u001b[A\n",
      "4145it [47:42,  1.45it/s]\u001b[A\n",
      "4146it [47:43,  1.45it/s]\u001b[A\n",
      "4147it [47:44,  1.45it/s]\u001b[A\n",
      "4148it [47:44,  1.45it/s]\u001b[A\n",
      "4149it [47:45,  1.45it/s]\u001b[A\n",
      "4150it [47:46,  1.45it/s]\u001b[A\n",
      "4151it [47:46,  1.45it/s]\u001b[A\n",
      "4152it [47:47,  1.45it/s]\u001b[A\n",
      "4153it [47:48,  1.45it/s]\u001b[A\n",
      "4154it [47:49,  1.45it/s]\u001b[A\n",
      "4155it [47:49,  1.45it/s]\u001b[A\n",
      "4156it [47:50,  1.45it/s]\u001b[A\n",
      "4157it [47:51,  1.45it/s]\u001b[A\n",
      "4158it [47:51,  1.45it/s]\u001b[A\n",
      "4159it [47:52,  1.45it/s]\u001b[A\n",
      "4160it [47:53,  1.45it/s]\u001b[A\n",
      "4161it [47:53,  1.45it/s]\u001b[A\n",
      "4162it [47:54,  1.45it/s]\u001b[A\n",
      "4163it [47:55,  1.45it/s]\u001b[A\n",
      "4164it [47:55,  1.45it/s]\u001b[A\n",
      "4165it [47:56,  1.45it/s]\u001b[A\n",
      "4166it [47:57,  1.45it/s]\u001b[A\n",
      "4167it [47:57,  1.45it/s]\u001b[A\n",
      "4168it [47:58,  1.45it/s]\u001b[A\n",
      "4169it [47:59,  1.45it/s]\u001b[A\n",
      "4170it [48:00,  1.45it/s]\u001b[A\n",
      "4171it [48:00,  1.45it/s]\u001b[A\n",
      "4172it [48:01,  1.45it/s]\u001b[A\n",
      "4173it [48:02,  1.45it/s]\u001b[A\n",
      "4174it [48:02,  1.45it/s]\u001b[A\n",
      "4175it [48:03,  1.45it/s]\u001b[A\n",
      "4176it [48:04,  1.45it/s]\u001b[A\n",
      "4177it [48:04,  1.45it/s]\u001b[A\n",
      "4178it [48:05,  1.45it/s]\u001b[A\n",
      "4179it [48:06,  1.45it/s]\u001b[A\n",
      "4180it [48:06,  1.45it/s]\u001b[A\n",
      "4181it [48:07,  1.45it/s]\u001b[A\n",
      "4182it [48:08,  1.45it/s]\u001b[A\n",
      "4183it [48:08,  1.45it/s]\u001b[A\n",
      "4184it [48:09,  1.45it/s]\u001b[A\n",
      "4185it [48:10,  1.45it/s]\u001b[A\n",
      "4186it [48:11,  1.45it/s]\u001b[A\n",
      "4187it [48:11,  1.45it/s]\u001b[A\n",
      "4188it [48:12,  1.45it/s]\u001b[A\n",
      "4189it [48:13,  1.45it/s]\u001b[A\n",
      "4190it [48:13,  1.45it/s]\u001b[A\n",
      "4191it [48:14,  1.45it/s]\u001b[A\n",
      "4192it [48:15,  1.45it/s]\u001b[A\n",
      "4193it [48:15,  1.45it/s]\u001b[A\n",
      "4194it [48:16,  1.45it/s]\u001b[A\n",
      "4195it [48:17,  1.45it/s]\u001b[A\n",
      "4196it [48:17,  1.45it/s]\u001b[A\n",
      "4197it [48:18,  1.45it/s]\u001b[A\n",
      "4198it [48:19,  1.45it/s]\u001b[A\n",
      "4199it [48:20,  1.45it/s]\u001b[A\n",
      "4200it [48:20,  1.45it/s]\u001b[A\n",
      "4201it [48:21,  1.45it/s]\u001b[A\n",
      "4202it [48:22,  1.45it/s]\u001b[A\n",
      "4203it [48:22,  1.45it/s]\u001b[A\n",
      "4204it [48:23,  1.45it/s]\u001b[A\n",
      "4205it [48:24,  1.45it/s]\u001b[A\n",
      "4206it [48:24,  1.45it/s]\u001b[A\n",
      "4207it [48:25,  1.45it/s]\u001b[A\n",
      "4208it [48:26,  1.45it/s]\u001b[A\n",
      "4209it [48:26,  1.45it/s]\u001b[A\n",
      "4210it [48:27,  1.45it/s]\u001b[A\n",
      "4211it [48:28,  1.45it/s]\u001b[A\n",
      "4212it [48:28,  1.45it/s]\u001b[A\n",
      "4213it [48:29,  1.45it/s]\u001b[A\n",
      "4214it [48:30,  1.45it/s]\u001b[A\n",
      "4215it [48:31,  1.45it/s]\u001b[A\n",
      "4216it [48:31,  1.45it/s]\u001b[A\n",
      "4217it [48:32,  1.45it/s]\u001b[A\n",
      "4218it [48:33,  1.45it/s]\u001b[A\n",
      "4219it [48:33,  1.45it/s]\u001b[A\n",
      "4220it [48:34,  1.45it/s]\u001b[A\n",
      "4221it [48:35,  1.45it/s]\u001b[A\n",
      "4222it [48:35,  1.45it/s]\u001b[A\n",
      "4223it [48:36,  1.45it/s]\u001b[A\n",
      "4224it [48:37,  1.45it/s]\u001b[A\n",
      "4225it [48:37,  1.45it/s]\u001b[A\n",
      "4226it [48:38,  1.45it/s]\u001b[A\n",
      "4227it [48:39,  1.45it/s]\u001b[A\n",
      "4228it [48:39,  1.45it/s]\u001b[A\n",
      "4229it [48:40,  1.45it/s]\u001b[A\n",
      "4230it [48:41,  1.45it/s]\u001b[A\n",
      "4231it [48:42,  1.45it/s]\u001b[A\n",
      "4232it [48:42,  1.45it/s]\u001b[A\n",
      "4233it [48:43,  1.45it/s]\u001b[A\n",
      "4234it [48:44,  1.45it/s]\u001b[A\n",
      "4235it [48:44,  1.45it/s]\u001b[A\n",
      "4236it [48:45,  1.45it/s]\u001b[A\n",
      "4237it [48:46,  1.45it/s]\u001b[A\n",
      "4238it [48:46,  1.45it/s]\u001b[A\n",
      "4239it [48:47,  1.45it/s]\u001b[A\n",
      "4240it [48:48,  1.45it/s]\u001b[A\n",
      "4241it [48:48,  1.45it/s]\u001b[A\n",
      "4242it [48:49,  1.45it/s]\u001b[A\n",
      "4243it [48:50,  1.45it/s]\u001b[A\n",
      "4244it [48:51,  1.45it/s]\u001b[A\n",
      "4245it [48:51,  1.45it/s]\u001b[A\n",
      "4246it [48:52,  1.45it/s]\u001b[A\n",
      "4247it [48:53,  1.45it/s]\u001b[A\n",
      "4248it [48:53,  1.45it/s]\u001b[A\n",
      "4249it [48:54,  1.45it/s]\u001b[A\n",
      "4250it [48:55,  1.45it/s]\u001b[A\n",
      "4251it [48:55,  1.45it/s]\u001b[A\n",
      "4252it [48:56,  1.45it/s]\u001b[A\n",
      "4253it [48:57,  1.45it/s]\u001b[A\n",
      "4254it [48:57,  1.45it/s]\u001b[A\n",
      "4255it [48:58,  1.45it/s]\u001b[A\n",
      "4256it [48:59,  1.45it/s]\u001b[A\n",
      "4257it [48:59,  1.45it/s]\u001b[A\n",
      "4258it [49:00,  1.45it/s]\u001b[A\n",
      "4259it [49:01,  1.45it/s]\u001b[A\n",
      "4260it [49:02,  1.45it/s]\u001b[A\n",
      "4261it [49:02,  1.45it/s]\u001b[A\n",
      "4262it [49:03,  1.46it/s]\u001b[A\n",
      "4263it [49:04,  1.45it/s]\u001b[A\n",
      "4264it [49:04,  1.45it/s]\u001b[A\n",
      "4265it [49:05,  1.45it/s]\u001b[A\n",
      "4266it [49:06,  1.45it/s]\u001b[A\n",
      "4267it [49:06,  1.45it/s]\u001b[A\n",
      "4268it [49:07,  1.45it/s]\u001b[A\n",
      "4269it [49:08,  1.45it/s]\u001b[A\n",
      "4270it [49:08,  1.45it/s]\u001b[A\n",
      "4271it [49:09,  1.45it/s]\u001b[A\n",
      "4272it [49:10,  1.45it/s]\u001b[A\n",
      "4273it [49:10,  1.45it/s]\u001b[A\n",
      "4274it [49:11,  1.45it/s]\u001b[A\n",
      "4275it [49:12,  1.45it/s]\u001b[A\n",
      "4276it [49:13,  1.45it/s]\u001b[A\n",
      "4277it [49:13,  1.45it/s]\u001b[A\n",
      "4278it [49:14,  1.45it/s]\u001b[A\n",
      "4279it [49:15,  1.45it/s]\u001b[A\n",
      "4280it [49:15,  1.45it/s]\u001b[A\n",
      "4281it [49:16,  1.45it/s]\u001b[A\n",
      "4282it [49:17,  1.45it/s]\u001b[A\n",
      "4283it [49:17,  1.45it/s]\u001b[A\n",
      "4284it [49:18,  1.45it/s]\u001b[A\n",
      "4285it [49:19,  1.45it/s]\u001b[A\n",
      "4286it [49:19,  1.45it/s]\u001b[A\n",
      "4287it [49:20,  1.45it/s]\u001b[A\n",
      "4288it [49:21,  1.45it/s]\u001b[A\n",
      "4289it [49:22,  1.45it/s]\u001b[A\n",
      "4290it [49:22,  1.45it/s]\u001b[A\n",
      "4291it [49:23,  1.45it/s]\u001b[A\n",
      "4292it [49:24,  1.45it/s]\u001b[A\n",
      "4293it [49:24,  1.45it/s]\u001b[A\n",
      "4294it [49:25,  1.45it/s]\u001b[A\n",
      "4295it [49:26,  1.45it/s]\u001b[A\n",
      "4296it [49:26,  1.45it/s]\u001b[A\n",
      "4297it [49:27,  1.45it/s]\u001b[A\n",
      "4298it [49:28,  1.45it/s]\u001b[A\n",
      "4299it [49:28,  1.45it/s]\u001b[A\n",
      "4300it [49:29,  1.45it/s]\u001b[A\n",
      "4301it [49:30,  1.45it/s]\u001b[A\n",
      "4302it [49:30,  1.45it/s]\u001b[A\n",
      "4303it [49:31,  1.45it/s]\u001b[A\n",
      "4304it [49:32,  1.45it/s]\u001b[A\n",
      "4305it [49:33,  1.45it/s]\u001b[A\n",
      "4306it [49:33,  1.45it/s]\u001b[A\n",
      "4307it [49:34,  1.45it/s]\u001b[A\n",
      "4308it [49:35,  1.45it/s]\u001b[A\n",
      "4309it [49:35,  1.45it/s]\u001b[A\n",
      "4310it [49:36,  1.45it/s]\u001b[A\n",
      "4311it [49:37,  1.45it/s]\u001b[A\n",
      "4312it [49:37,  1.45it/s]\u001b[A\n",
      "4313it [49:38,  1.45it/s]\u001b[A\n",
      "4314it [49:39,  1.45it/s]\u001b[A\n",
      "4315it [49:39,  1.45it/s]\u001b[A\n",
      "4316it [49:40,  1.45it/s]\u001b[A\n",
      "4317it [49:41,  1.45it/s]\u001b[A\n",
      "4318it [49:41,  1.45it/s]\u001b[A\n",
      "4319it [49:42,  1.45it/s]\u001b[A\n",
      "4320it [49:43,  1.45it/s]\u001b[A\n",
      "4321it [49:44,  1.45it/s]\u001b[A\n",
      "4322it [49:44,  1.45it/s]\u001b[A\n",
      "4323it [49:45,  1.45it/s]\u001b[A\n",
      "4324it [49:46,  1.45it/s]\u001b[A\n",
      "4325it [49:46,  1.45it/s]\u001b[A\n",
      "4326it [49:47,  1.45it/s]\u001b[A\n",
      "4327it [49:48,  1.45it/s]\u001b[A\n",
      "4328it [49:48,  1.45it/s]\u001b[A\n",
      "4329it [49:49,  1.45it/s]\u001b[A\n",
      "4330it [49:50,  1.45it/s]\u001b[A\n",
      "4331it [49:50,  1.45it/s]\u001b[A\n",
      "4332it [49:51,  1.45it/s]\u001b[A\n",
      "4333it [49:52,  1.45it/s]\u001b[A\n",
      "4334it [49:53,  1.45it/s]\u001b[A\n",
      "4335it [49:53,  1.45it/s]\u001b[A\n",
      "4336it [49:54,  1.45it/s]\u001b[A\n",
      "4337it [49:55,  1.45it/s]\u001b[A\n",
      "4338it [49:55,  1.45it/s]\u001b[A\n",
      "4339it [49:56,  1.45it/s]\u001b[A\n",
      "4340it [49:57,  1.45it/s]\u001b[A\n",
      "4341it [49:57,  1.45it/s]\u001b[A\n",
      "4342it [49:58,  1.45it/s]\u001b[A\n",
      "4343it [49:59,  1.45it/s]\u001b[A\n",
      "4344it [49:59,  1.45it/s]\u001b[A\n",
      "4345it [50:00,  1.45it/s]\u001b[A\n",
      "4346it [50:01,  1.45it/s]\u001b[A\n",
      "4347it [50:01,  1.45it/s]\u001b[A\n",
      "4348it [50:02,  1.45it/s]\u001b[A\n",
      "4349it [50:03,  1.45it/s]\u001b[A\n",
      "4350it [50:04,  1.45it/s]\u001b[A\n",
      "4351it [50:04,  1.45it/s]\u001b[A\n",
      "4352it [50:05,  1.45it/s]\u001b[A\n",
      "4353it [50:06,  1.45it/s]\u001b[A\n",
      "4354it [50:06,  1.45it/s]\u001b[A\n",
      "4355it [50:07,  1.45it/s]\u001b[A\n",
      "4356it [50:08,  1.45it/s]\u001b[A\n",
      "4357it [50:08,  1.45it/s]\u001b[A\n",
      "4358it [50:09,  1.45it/s]\u001b[A\n",
      "4359it [50:10,  1.45it/s]\u001b[A\n",
      "4360it [50:10,  1.45it/s]\u001b[A\n",
      "4361it [50:11,  1.45it/s]\u001b[A\n",
      "4362it [50:12,  1.45it/s]\u001b[A\n",
      "4363it [50:12,  1.45it/s]\u001b[A\n",
      "4364it [50:13,  1.45it/s]\u001b[A\n",
      "4365it [50:14,  1.45it/s]\u001b[A\n",
      "4366it [50:15,  1.45it/s]\u001b[A\n",
      "4367it [50:15,  1.45it/s]\u001b[A\n",
      "4368it [50:16,  1.45it/s]\u001b[A\n",
      "4369it [50:17,  1.45it/s]\u001b[A\n",
      "4370it [50:17,  1.45it/s]\u001b[A\n",
      "4371it [50:18,  1.45it/s]\u001b[A\n",
      "4372it [50:19,  1.45it/s]\u001b[A\n",
      "4373it [50:19,  1.45it/s]\u001b[A\n",
      "4374it [50:20,  1.45it/s]\u001b[A\n",
      "4375it [50:21,  1.45it/s]\u001b[A\n",
      "4376it [50:21,  1.45it/s]\u001b[A\n",
      "4377it [50:22,  1.45it/s]\u001b[A\n",
      "4378it [50:23,  1.45it/s]\u001b[A\n",
      "4379it [50:23,  1.45it/s]\u001b[A\n",
      "4380it [50:24,  1.45it/s]\u001b[A\n",
      "4381it [50:25,  1.45it/s]\u001b[A\n",
      "4382it [50:26,  1.45it/s]\u001b[A\n",
      "4383it [50:26,  1.45it/s]\u001b[A\n",
      "4384it [50:27,  1.45it/s]\u001b[A\n",
      "4385it [50:28,  1.45it/s]\u001b[A\n",
      "4386it [50:28,  1.45it/s]\u001b[A\n",
      "4387it [50:29,  1.45it/s]\u001b[A\n",
      "4388it [50:30,  1.45it/s]\u001b[A\n",
      "4389it [50:30,  1.45it/s]\u001b[A\n",
      "4390it [50:31,  1.45it/s]\u001b[A\n",
      "4391it [50:32,  1.45it/s]\u001b[A\n",
      "4392it [50:32,  1.45it/s]\u001b[A\n",
      "4393it [50:33,  1.45it/s]\u001b[A\n",
      "4394it [50:34,  1.45it/s]\u001b[A\n",
      "4395it [50:35,  1.45it/s]\u001b[A\n",
      "4396it [50:35,  1.45it/s]\u001b[A\n",
      "4397it [50:36,  1.45it/s]\u001b[A\n",
      "4398it [50:37,  1.45it/s]\u001b[A\n",
      "4399it [50:37,  1.45it/s]\u001b[A\n",
      "4400it [50:38,  1.45it/s]\u001b[A\n",
      "4401it [50:39,  1.45it/s]\u001b[A\n",
      "4402it [50:39,  1.45it/s]\u001b[A\n",
      "4403it [50:40,  1.46it/s]\u001b[A\n",
      "4404it [50:41,  1.45it/s]\u001b[A\n",
      "4405it [50:41,  1.45it/s]\u001b[A\n",
      "4406it [50:42,  1.45it/s]\u001b[A\n",
      "4407it [50:43,  1.45it/s]\u001b[A\n",
      "4408it [50:43,  1.45it/s]\u001b[A\n",
      "4409it [50:44,  1.45it/s]\u001b[A\n",
      "4410it [50:45,  1.45it/s]\u001b[A\n",
      "4411it [50:46,  1.45it/s]\u001b[A\n",
      "4412it [50:46,  1.45it/s]\u001b[A\n",
      "4413it [50:47,  1.45it/s]\u001b[A\n",
      "4414it [50:48,  1.45it/s]\u001b[A\n",
      "4415it [50:48,  1.45it/s]\u001b[A\n",
      "4416it [50:49,  1.45it/s]\u001b[A\n",
      "4417it [50:50,  1.45it/s]\u001b[A\n",
      "4418it [50:50,  1.45it/s]\u001b[A\n",
      "4419it [50:51,  1.45it/s]\u001b[A\n",
      "4420it [50:52,  1.45it/s]\u001b[A\n",
      "4421it [50:52,  1.45it/s]\u001b[A\n",
      "4422it [50:53,  1.45it/s]\u001b[A\n",
      "4423it [50:54,  1.45it/s]\u001b[A\n",
      "4424it [50:54,  1.45it/s]\u001b[A\n",
      "4425it [50:55,  1.45it/s]\u001b[A\n",
      "4426it [50:56,  1.45it/s]\u001b[A\n",
      "4427it [50:57,  1.45it/s]\u001b[A\n",
      "4428it [50:57,  1.45it/s]\u001b[A\n",
      "4429it [50:58,  1.45it/s]\u001b[A\n",
      "4430it [50:59,  1.45it/s]\u001b[A\n",
      "4431it [50:59,  1.45it/s]\u001b[A\n",
      "4432it [51:00,  1.45it/s]\u001b[A\n",
      "4433it [51:01,  1.45it/s]\u001b[A\n",
      "4434it [51:01,  1.45it/s]\u001b[A\n",
      "4435it [51:02,  1.45it/s]\u001b[A\n",
      "4436it [51:03,  1.45it/s]\u001b[A\n",
      "4437it [51:03,  1.45it/s]\u001b[A\n",
      "4438it [51:04,  1.45it/s]\u001b[A\n",
      "4439it [51:05,  1.45it/s]\u001b[A\n",
      "4440it [51:05,  1.45it/s]\u001b[A\n",
      "4441it [51:06,  1.45it/s]\u001b[A\n",
      "4442it [51:07,  1.45it/s]\u001b[A\n",
      "4443it [51:08,  1.45it/s]\u001b[A\n",
      "4444it [51:08,  1.45it/s]\u001b[A\n",
      "4445it [51:09,  1.45it/s]\u001b[A\n",
      "4446it [51:10,  1.45it/s]\u001b[A\n",
      "4447it [51:10,  1.45it/s]\u001b[A\n",
      "4448it [51:11,  1.45it/s]\u001b[A\n",
      "4449it [51:12,  1.45it/s]\u001b[A\n",
      "4450it [51:12,  1.45it/s]\u001b[A\n",
      "4451it [51:13,  1.45it/s]\u001b[A\n",
      "4452it [51:14,  1.45it/s]\u001b[A\n",
      "4453it [51:14,  1.45it/s]\u001b[A\n",
      "4454it [51:15,  1.45it/s]\u001b[A\n",
      "4455it [51:16,  1.45it/s]\u001b[A\n",
      "4456it [51:17,  1.45it/s]\u001b[A\n",
      "4457it [51:17,  1.45it/s]\u001b[A\n",
      "4458it [51:18,  1.45it/s]\u001b[A\n",
      "4459it [51:19,  1.45it/s]\u001b[A\n",
      "4460it [51:19,  1.45it/s]\u001b[A\n",
      "4461it [51:20,  1.45it/s]\u001b[A\n",
      "4462it [51:21,  1.45it/s]\u001b[A\n",
      "4463it [51:21,  1.45it/s]\u001b[A\n",
      "4464it [51:22,  1.45it/s]\u001b[A\n",
      "4465it [51:23,  1.45it/s]\u001b[A\n",
      "4466it [51:23,  1.45it/s]\u001b[A\n",
      "4467it [51:24,  1.45it/s]\u001b[A\n",
      "4468it [51:25,  1.45it/s]\u001b[A\n",
      "4469it [51:25,  1.45it/s]\u001b[A\n",
      "4470it [51:26,  1.45it/s]\u001b[A\n",
      "4471it [51:27,  1.45it/s]\u001b[A\n",
      "4472it [51:28,  1.45it/s]\u001b[A\n",
      "4473it [51:28,  1.45it/s]\u001b[A\n",
      "4474it [51:29,  1.45it/s]\u001b[A\n",
      "4475it [51:30,  1.45it/s]\u001b[A\n",
      "4476it [51:30,  1.45it/s]\u001b[A\n",
      "4477it [51:31,  1.45it/s]\u001b[A\n",
      "4478it [51:32,  1.45it/s]\u001b[A\n",
      "4479it [51:32,  1.45it/s]\u001b[A\n",
      "4480it [51:33,  1.45it/s]\u001b[A\n",
      "4481it [51:34,  1.45it/s]\u001b[A\n",
      "4482it [51:34,  1.45it/s]\u001b[A\n",
      "4483it [51:35,  1.45it/s]\u001b[A\n",
      "4484it [51:36,  1.45it/s]\u001b[A\n",
      "4485it [51:37,  1.45it/s]\u001b[A\n",
      "4486it [51:37,  1.45it/s]\u001b[A\n",
      "4487it [51:38,  1.45it/s]\u001b[A\n",
      "4488it [51:39,  1.45it/s]\u001b[A\n",
      "4489it [51:39,  1.45it/s]\u001b[A\n",
      "4490it [51:40,  1.45it/s]\u001b[A\n",
      "4491it [51:41,  1.46it/s]\u001b[A\n",
      "4492it [51:41,  1.45it/s]\u001b[A\n",
      "4493it [51:42,  1.45it/s]\u001b[A\n",
      "4494it [51:43,  1.45it/s]\u001b[A\n",
      "4495it [51:43,  1.45it/s]\u001b[A\n",
      "4496it [51:44,  1.45it/s]\u001b[A\n",
      "4497it [51:45,  1.45it/s]\u001b[A\n",
      "4498it [51:45,  1.45it/s]\u001b[A\n",
      "4499it [51:46,  1.45it/s]\u001b[A\n",
      "4500it [51:47,  1.45it/s]\u001b[A\n",
      "4501it [51:48,  1.45it/s]\u001b[A\n",
      "4502it [51:48,  1.45it/s]\u001b[A\n",
      "4503it [51:49,  1.45it/s]\u001b[A\n",
      "4504it [51:50,  1.45it/s]\u001b[A\n",
      "4505it [51:50,  1.45it/s]\u001b[A\n",
      "4506it [51:51,  1.45it/s]\u001b[A\n",
      "4507it [51:52,  1.45it/s]\u001b[A\n",
      "4508it [51:52,  1.45it/s]\u001b[A\n",
      "4509it [51:53,  1.45it/s]\u001b[A\n",
      "4510it [51:54,  1.45it/s]\u001b[A\n",
      "4511it [51:54,  1.45it/s]\u001b[A\n",
      "4512it [51:55,  1.45it/s]\u001b[A\n",
      "4513it [51:56,  1.45it/s]\u001b[A\n",
      "4514it [51:56,  1.45it/s]\u001b[A\n",
      "4515it [51:57,  1.45it/s]\u001b[A\n",
      "4516it [51:58,  1.45it/s]\u001b[A\n",
      "4517it [51:59,  1.45it/s]\u001b[A\n",
      "4518it [51:59,  1.45it/s]\u001b[A\n",
      "4519it [52:00,  1.45it/s]\u001b[A\n",
      "4520it [52:01,  1.45it/s]\u001b[A\n",
      "4521it [52:01,  1.45it/s]\u001b[A\n",
      "4522it [52:02,  1.45it/s]\u001b[A\n",
      "4523it [52:03,  1.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4524it [52:03,  1.45it/s]\u001b[A\n",
      "4525it [52:04,  1.45it/s]\u001b[A\n",
      "4526it [52:05,  1.45it/s]\u001b[A\n",
      "4527it [52:05,  1.45it/s]\u001b[A\n",
      "4528it [52:06,  1.45it/s]\u001b[A\n",
      "4529it [52:07,  1.45it/s]\u001b[A\n",
      "4530it [52:07,  1.45it/s]\u001b[A\n",
      "4531it [52:08,  1.45it/s]\u001b[A\n",
      "4532it [52:09,  1.45it/s]\u001b[A\n",
      "4533it [52:10,  1.45it/s]\u001b[A\n",
      "4534it [52:10,  1.45it/s]\u001b[A\n",
      "4535it [52:11,  1.45it/s]\u001b[A\n",
      "4536it [52:12,  1.45it/s]\u001b[A\n",
      "4537it [52:12,  1.45it/s]\u001b[A\n",
      "4538it [52:13,  1.45it/s]\u001b[A\n",
      "4539it [52:14,  1.45it/s]\u001b[A\n",
      "4540it [52:14,  1.45it/s]\u001b[A\n",
      "4541it [52:15,  1.45it/s]\u001b[A\n",
      "4542it [52:16,  1.45it/s]\u001b[A\n",
      "4543it [52:16,  1.45it/s]\u001b[A\n",
      "4544it [52:17,  1.45it/s]\u001b[A\n",
      "4545it [52:18,  1.45it/s]\u001b[A\n",
      "4546it [52:19,  1.45it/s]\u001b[A\n",
      "4547it [52:19,  1.45it/s]\u001b[A\n",
      "4548it [52:20,  1.45it/s]\u001b[A\n",
      "4549it [52:21,  1.45it/s]\u001b[A\n",
      "4550it [52:21,  1.45it/s]\u001b[A\n",
      "4551it [52:22,  1.45it/s]\u001b[A\n",
      "4552it [52:23,  1.45it/s]\u001b[A\n",
      "4553it [52:23,  1.45it/s]\u001b[A\n",
      "4554it [52:24,  1.45it/s]\u001b[A\n",
      "4555it [52:25,  1.45it/s]\u001b[A\n",
      "4556it [52:25,  1.45it/s]\u001b[A\n",
      "4557it [52:26,  1.45it/s]\u001b[A\n",
      "4558it [52:27,  1.45it/s]\u001b[A\n",
      "4559it [52:27,  1.45it/s]\u001b[A\n",
      "4560it [52:28,  1.45it/s]\u001b[A\n",
      "4561it [52:29,  1.45it/s]\u001b[A\n",
      "4562it [52:30,  1.45it/s]\u001b[A\n",
      "4563it [52:30,  1.45it/s]\u001b[A\n",
      "4564it [52:31,  1.45it/s]\u001b[A\n",
      "4565it [52:32,  1.45it/s]\u001b[A\n",
      "4566it [52:32,  1.45it/s]\u001b[A\n",
      "4567it [52:33,  1.45it/s]\u001b[A\n",
      "4568it [52:34,  1.45it/s]\u001b[A\n",
      "4569it [52:34,  1.45it/s]\u001b[A\n",
      "4570it [52:35,  1.45it/s]\u001b[A\n",
      "4571it [52:36,  1.45it/s]\u001b[A\n",
      "4572it [52:36,  1.45it/s]\u001b[A\n",
      "4573it [52:37,  1.45it/s]\u001b[A\n",
      "4574it [52:38,  1.45it/s]\u001b[A\n",
      "4575it [52:38,  1.45it/s]\u001b[A\n",
      "4576it [52:39,  1.45it/s]\u001b[A\n",
      "4577it [52:40,  1.45it/s]\u001b[A\n",
      "4578it [52:41,  1.45it/s]\u001b[A\n",
      "4579it [52:41,  1.45it/s]\u001b[A\n",
      "4580it [52:42,  1.45it/s]\u001b[A\n",
      "4581it [52:43,  1.45it/s]\u001b[A\n",
      "4582it [52:43,  1.45it/s]\u001b[A\n",
      "4583it [52:44,  1.45it/s]\u001b[A\n",
      "4584it [52:45,  1.45it/s]\u001b[A\n",
      "4585it [52:45,  1.45it/s]\u001b[A\n",
      "4586it [52:46,  1.45it/s]\u001b[A\n",
      "4587it [52:47,  1.45it/s]\u001b[A\n",
      "4588it [52:47,  1.45it/s]\u001b[A\n",
      "4589it [52:48,  1.45it/s]\u001b[A\n",
      "4590it [52:49,  1.45it/s]\u001b[A\n",
      "4591it [52:49,  1.45it/s]\u001b[A\n",
      "4592it [52:50,  1.45it/s]\u001b[A\n",
      "4593it [52:51,  1.45it/s]\u001b[A\n",
      "4594it [52:52,  1.45it/s]\u001b[A\n",
      "4595it [52:52,  1.45it/s]\u001b[A\n",
      "4596it [52:53,  1.45it/s]\u001b[A\n",
      "4597it [52:54,  1.45it/s]\u001b[A\n",
      "4598it [52:54,  1.45it/s]\u001b[A\n",
      "4599it [52:55,  1.45it/s]\u001b[A\n",
      "4600it [52:56,  1.45it/s]\u001b[A\n",
      "4601it [52:56,  1.45it/s]\u001b[A\n",
      "4602it [52:57,  1.45it/s]\u001b[A\n",
      "4603it [52:58,  1.45it/s]\u001b[A\n",
      "4604it [52:58,  1.45it/s]\u001b[A\n",
      "4605it [52:59,  1.45it/s]\u001b[A\n",
      "4606it [53:00,  1.45it/s]\u001b[A\n",
      "4607it [53:01,  1.45it/s]\u001b[A\n",
      "4608it [53:01,  1.45it/s]\u001b[A\n",
      "4609it [53:02,  1.45it/s]\u001b[A\n",
      "4610it [53:03,  1.45it/s]\u001b[A\n",
      "4611it [53:03,  1.45it/s]\u001b[A\n",
      "4612it [53:04,  1.45it/s]\u001b[A\n",
      "4613it [53:05,  1.45it/s]\u001b[A\n",
      "4614it [53:05,  1.45it/s]\u001b[A\n",
      "4615it [53:06,  1.45it/s]\u001b[A\n",
      "4616it [53:07,  1.45it/s]\u001b[A\n",
      "4617it [53:07,  1.45it/s]\u001b[A\n",
      "4618it [53:08,  1.45it/s]\u001b[A\n",
      "4619it [53:09,  1.45it/s]\u001b[A\n",
      "4620it [53:09,  1.45it/s]\u001b[A\n",
      "4621it [53:10,  1.45it/s]\u001b[A\n",
      "4622it [53:11,  1.45it/s]\u001b[A\n",
      "4623it [53:12,  1.45it/s]\u001b[A\n",
      "4624it [53:12,  1.45it/s]\u001b[A\n",
      "4625it [53:13,  1.45it/s]\u001b[A\n",
      "4626it [53:14,  1.45it/s]\u001b[A\n",
      "4627it [53:14,  1.45it/s]\u001b[A\n",
      "4628it [53:15,  1.45it/s]\u001b[A\n",
      "4629it [53:16,  1.45it/s]\u001b[A\n",
      "4630it [53:16,  1.45it/s]\u001b[A\n",
      "4631it [53:17,  1.45it/s]\u001b[A\n",
      "4632it [53:18,  1.45it/s]\u001b[A\n",
      "4633it [53:18,  1.45it/s]\u001b[A\n",
      "4634it [53:19,  1.45it/s]\u001b[A\n",
      "4635it [53:20,  1.45it/s]\u001b[A\n",
      "4636it [53:20,  1.45it/s]\u001b[A\n",
      "4637it [53:21,  1.45it/s]\u001b[A\n",
      "4638it [53:22,  1.45it/s]\u001b[A\n",
      "4639it [53:23,  1.45it/s]\u001b[A\n",
      "4640it [53:23,  1.45it/s]\u001b[A\n",
      "4641it [53:24,  1.45it/s]\u001b[A\n",
      "4642it [53:25,  1.45it/s]\u001b[A\n",
      "4643it [53:25,  1.45it/s]\u001b[A\n",
      "4644it [53:26,  1.45it/s]\u001b[A\n",
      "4645it [53:27,  1.45it/s]\u001b[A\n",
      "4646it [53:27,  1.46it/s]\u001b[A\n",
      "4647it [53:28,  1.45it/s]\u001b[A\n",
      "4648it [53:29,  1.45it/s]\u001b[A\n",
      "4649it [53:29,  1.45it/s]\u001b[A\n",
      "4650it [53:30,  1.45it/s]\u001b[A\n",
      "4651it [53:31,  1.45it/s]\u001b[A\n",
      "4652it [53:32,  1.45it/s]\u001b[A\n",
      "4653it [53:32,  1.45it/s]\u001b[A\n",
      "4654it [53:33,  1.45it/s]\u001b[A\n",
      "4655it [53:34,  1.45it/s]\u001b[A\n",
      "4656it [53:34,  1.45it/s]\u001b[A\n",
      "4657it [53:35,  1.45it/s]\u001b[A\n",
      "4658it [53:36,  1.45it/s]\u001b[A\n",
      "4659it [53:36,  1.45it/s]\u001b[A\n",
      "4660it [53:37,  1.45it/s]\u001b[A\n",
      "4661it [53:38,  1.45it/s]\u001b[A\n",
      "4662it [53:38,  1.45it/s]\u001b[A\n",
      "4663it [53:39,  1.45it/s]\u001b[A\n",
      "4664it [53:40,  1.45it/s]\u001b[A\n",
      "4665it [53:40,  1.45it/s]\u001b[A\n",
      "4666it [53:41,  1.45it/s]\u001b[A\n",
      "4667it [53:42,  1.45it/s]\u001b[A\n",
      "4668it [53:43,  1.45it/s]\u001b[A\n",
      "4669it [53:43,  1.45it/s]\u001b[A\n",
      "4670it [53:44,  1.45it/s]\u001b[A\n",
      "4671it [53:45,  1.45it/s]\u001b[A\n",
      "4672it [53:45,  1.45it/s]\u001b[A\n",
      "4673it [53:46,  1.45it/s]\u001b[A\n",
      "4674it [53:47,  1.45it/s]\u001b[A\n",
      "4675it [53:47,  1.45it/s]\u001b[A\n",
      "4676it [53:48,  1.45it/s]\u001b[A\n",
      "4677it [53:49,  1.45it/s]\u001b[A\n",
      "4678it [53:49,  1.45it/s]\u001b[A\n",
      "4679it [53:50,  1.45it/s]\u001b[A\n",
      "4680it [53:51,  1.45it/s]\u001b[A\n",
      "4681it [53:51,  1.45it/s]\u001b[A\n",
      "4682it [53:52,  1.45it/s]\u001b[A\n",
      "4683it [53:53,  1.45it/s]\u001b[A\n",
      "4684it [53:54,  1.45it/s]\u001b[A\n",
      "4685it [53:54,  1.45it/s]\u001b[A\n",
      "4686it [53:55,  1.45it/s]\u001b[A\n",
      "4687it [53:56,  1.45it/s]\u001b[A\n",
      "4688it [53:56,  1.45it/s]\u001b[A\n",
      "4689it [53:57,  1.45it/s]\u001b[A\n",
      "4690it [53:58,  1.45it/s]\u001b[A\n",
      "4691it [53:58,  1.45it/s]\u001b[A\n",
      "4692it [53:59,  1.45it/s]\u001b[A\n",
      "4693it [54:00,  1.45it/s]\u001b[A\n",
      "4694it [54:00,  1.45it/s]\u001b[A\n",
      "4695it [54:01,  1.45it/s]\u001b[A\n",
      "4696it [54:02,  1.45it/s]\u001b[A\n",
      "4697it [54:03,  1.45it/s]\u001b[A\n",
      "4698it [54:03,  1.45it/s]\u001b[A\n",
      "4699it [54:04,  1.45it/s]\u001b[A\n",
      "4700it [54:05,  1.45it/s]\u001b[A\n",
      "4701it [54:05,  1.45it/s]\u001b[A\n",
      "4702it [54:06,  1.45it/s]\u001b[A\n",
      "4703it [54:07,  1.45it/s]\u001b[A\n",
      "4704it [54:07,  1.45it/s]\u001b[A\n",
      "4705it [54:08,  1.45it/s]\u001b[A\n",
      "4706it [54:09,  1.45it/s]\u001b[A\n",
      "4707it [54:09,  1.45it/s]\u001b[A\n",
      "4708it [54:10,  1.45it/s]\u001b[A\n",
      "4709it [54:11,  1.45it/s]\u001b[A\n",
      "4710it [54:11,  1.45it/s]\u001b[A\n",
      "4711it [54:12,  1.45it/s]\u001b[A\n",
      "4712it [54:13,  1.45it/s]\u001b[A\n",
      "4713it [54:14,  1.45it/s]\u001b[A\n",
      "4714it [54:14,  1.45it/s]\u001b[A\n",
      "4715it [54:15,  1.45it/s]\u001b[A\n",
      "4716it [54:16,  1.45it/s]\u001b[A\n",
      "4717it [54:16,  1.45it/s]\u001b[A\n",
      "4718it [54:17,  1.45it/s]\u001b[A\n",
      "4719it [54:18,  1.45it/s]\u001b[A\n",
      "4720it [54:18,  1.45it/s]\u001b[A\n",
      "4721it [54:19,  1.45it/s]\u001b[A\n",
      "4722it [54:20,  1.45it/s]\u001b[A\n",
      "4723it [54:20,  1.45it/s]\u001b[A\n",
      "4724it [54:21,  1.45it/s]\u001b[A\n",
      "4725it [54:22,  1.45it/s]\u001b[A\n",
      "4726it [54:22,  1.45it/s]\u001b[A\n",
      "4727it [54:23,  1.45it/s]\u001b[A\n",
      "4728it [54:24,  1.45it/s]\u001b[A\n",
      "4729it [54:25,  1.45it/s]\u001b[A\n",
      "4730it [54:25,  1.45it/s]\u001b[A\n",
      "4731it [54:26,  1.45it/s]\u001b[A\n",
      "4732it [54:27,  1.45it/s]\u001b[A\n",
      "4733it [54:27,  1.45it/s]\u001b[A\n",
      "4734it [54:28,  1.45it/s]\u001b[A\n",
      "4735it [54:29,  1.45it/s]\u001b[A\n",
      "4736it [54:29,  1.45it/s]\u001b[A\n",
      "4737it [54:30,  1.45it/s]\u001b[A\n",
      "4738it [54:31,  1.45it/s]\u001b[A\n",
      "4739it [54:31,  1.45it/s]\u001b[A\n",
      "4740it [54:32,  1.45it/s]\u001b[A\n",
      "4741it [54:33,  1.45it/s]\u001b[A\n",
      "4742it [54:34,  1.45it/s]\u001b[A\n",
      "4743it [54:34,  1.45it/s]\u001b[A\n",
      "4744it [54:35,  1.45it/s]\u001b[A\n",
      "4745it [54:36,  1.45it/s]\u001b[A\n",
      "4746it [54:36,  1.45it/s]\u001b[A\n",
      "4747it [54:37,  1.45it/s]\u001b[A\n",
      "4748it [54:38,  1.45it/s]\u001b[A\n",
      "4749it [54:38,  1.45it/s]\u001b[A\n",
      "4750it [54:39,  1.45it/s]\u001b[A\n",
      "4751it [54:40,  1.45it/s]\u001b[A\n",
      "4752it [54:40,  1.45it/s]\u001b[A\n",
      "4753it [54:41,  1.45it/s]\u001b[A\n",
      "4754it [54:42,  1.45it/s]\u001b[A\n",
      "4755it [54:42,  1.45it/s]\u001b[A\n",
      "4756it [54:43,  1.45it/s]\u001b[A\n",
      "4757it [54:44,  1.45it/s]\u001b[A\n",
      "4758it [54:45,  1.45it/s]\u001b[A\n",
      "4759it [54:45,  1.45it/s]\u001b[A\n",
      "4760it [54:46,  1.45it/s]\u001b[A\n",
      "4761it [54:47,  1.45it/s]\u001b[A\n",
      "4762it [54:47,  1.45it/s]\u001b[A\n",
      "4763it [54:48,  1.45it/s]\u001b[A\n",
      "4764it [54:49,  1.45it/s]\u001b[A\n",
      "4765it [54:49,  1.45it/s]\u001b[A\n",
      "4766it [54:50,  1.45it/s]\u001b[A\n",
      "4767it [54:51,  1.45it/s]\u001b[A\n",
      "4768it [54:51,  1.45it/s]\u001b[A\n",
      "4769it [54:52,  1.45it/s]\u001b[A\n",
      "4770it [54:53,  1.45it/s]\u001b[A\n",
      "4771it [54:53,  1.45it/s]\u001b[A\n",
      "4772it [54:54,  1.45it/s]\u001b[A\n",
      "4773it [54:55,  1.45it/s]\u001b[A\n",
      "4774it [54:56,  1.45it/s]\u001b[A\n",
      "4775it [54:56,  1.45it/s]\u001b[A\n",
      "4776it [54:57,  1.45it/s]\u001b[A\n",
      "4777it [54:58,  1.45it/s]\u001b[A\n",
      "4778it [54:58,  1.45it/s]\u001b[A\n",
      "4779it [54:59,  1.45it/s]\u001b[A\n",
      "4780it [55:00,  1.45it/s]\u001b[A\n",
      "4781it [55:00,  1.45it/s]\u001b[A\n",
      "4782it [55:01,  1.45it/s]\u001b[A\n",
      "4783it [55:02,  1.45it/s]\u001b[A\n",
      "4784it [55:02,  1.45it/s]\u001b[A\n",
      "4785it [55:03,  1.45it/s]\u001b[A\n",
      "4786it [55:04,  1.45it/s]\u001b[A\n",
      "4787it [55:04,  1.45it/s]\u001b[A\n",
      "4788it [55:05,  1.45it/s]\u001b[A\n",
      "4789it [55:06,  1.45it/s]\u001b[A\n",
      "4790it [55:07,  1.45it/s]\u001b[A\n",
      "4791it [55:07,  1.45it/s]\u001b[A\n",
      "4792it [55:08,  1.45it/s]\u001b[A\n",
      "4793it [55:09,  1.45it/s]\u001b[A\n",
      "4794it [55:09,  1.45it/s]\u001b[A\n",
      "4795it [55:10,  1.45it/s]\u001b[A\n",
      "4796it [55:11,  1.45it/s]\u001b[A\n",
      "4797it [55:11,  1.45it/s]\u001b[A\n",
      "4798it [55:12,  1.45it/s]\u001b[A\n",
      "4799it [55:13,  1.45it/s]\u001b[A\n",
      "4800it [55:13,  1.45it/s]\u001b[A\n",
      "4801it [55:14,  1.45it/s]\u001b[A\n",
      "4802it [55:15,  1.45it/s]\u001b[A\n",
      "4803it [55:16,  1.45it/s]\u001b[A\n",
      "4804it [55:16,  1.45it/s]\u001b[A\n",
      "4805it [55:17,  1.45it/s]\u001b[A\n",
      "4806it [55:18,  1.45it/s]\u001b[A\n",
      "4807it [55:18,  1.45it/s]\u001b[A\n",
      "4808it [55:19,  1.45it/s]\u001b[A\n",
      "4809it [55:20,  1.45it/s]\u001b[A\n",
      "4810it [55:20,  1.45it/s]\u001b[A\n",
      "4811it [55:21,  1.45it/s]\u001b[A\n",
      "4812it [55:22,  1.45it/s]\u001b[A\n",
      "4813it [55:22,  1.45it/s]\u001b[A\n",
      "4814it [55:23,  1.45it/s]\u001b[A\n",
      "4815it [55:24,  1.45it/s]\u001b[A\n",
      "4816it [55:24,  1.45it/s]\u001b[A\n",
      "4817it [55:25,  1.45it/s]\u001b[A\n",
      "4818it [55:26,  1.45it/s]\u001b[A\n",
      "4819it [55:27,  1.45it/s]\u001b[A\n",
      "4820it [55:27,  1.45it/s]\u001b[A\n",
      "4821it [55:28,  1.45it/s]\u001b[A\n",
      "4822it [55:29,  1.45it/s]\u001b[A\n",
      "4823it [55:29,  1.45it/s]\u001b[A\n",
      "4824it [55:30,  1.45it/s]\u001b[A\n",
      "4825it [55:31,  1.45it/s]\u001b[A\n",
      "4826it [55:31,  1.45it/s]\u001b[A\n",
      "4827it [55:32,  1.45it/s]\u001b[A\n",
      "4828it [55:33,  1.45it/s]\u001b[A\n",
      "4829it [55:33,  1.45it/s]\u001b[A\n",
      "4830it [55:34,  1.45it/s]\u001b[A\n",
      "4831it [55:35,  1.45it/s]\u001b[A\n",
      "4832it [55:35,  1.45it/s]\u001b[A\n",
      "4833it [55:36,  1.45it/s]\u001b[A\n",
      "4834it [55:37,  1.45it/s]\u001b[A\n",
      "4835it [55:38,  1.45it/s]\u001b[A\n",
      "4836it [55:38,  1.45it/s]\u001b[A\n",
      "4837it [55:39,  1.45it/s]\u001b[A\n",
      "4838it [55:40,  1.45it/s]\u001b[A\n",
      "4839it [55:40,  1.45it/s]\u001b[A\n",
      "4840it [55:41,  1.45it/s]\u001b[A\n",
      "4841it [55:42,  1.45it/s]\u001b[A\n",
      "4842it [55:42,  1.45it/s]\u001b[A\n",
      "4843it [55:43,  1.45it/s]\u001b[A\n",
      "4844it [55:44,  1.45it/s]\u001b[A\n",
      "4845it [55:44,  1.45it/s]\u001b[A\n",
      "4846it [55:45,  1.45it/s]\u001b[A\n",
      "4847it [55:46,  1.45it/s]\u001b[A\n",
      "4848it [55:47,  1.45it/s]\u001b[A\n",
      "4849it [55:47,  1.45it/s]\u001b[A\n",
      "4850it [55:48,  1.45it/s]\u001b[A\n",
      "4851it [55:49,  1.45it/s]\u001b[A\n",
      "4852it [55:49,  1.45it/s]\u001b[A\n",
      "4853it [55:50,  1.45it/s]\u001b[A\n",
      "4854it [55:51,  1.45it/s]\u001b[A\n",
      "4855it [55:51,  1.45it/s]\u001b[A\n",
      "4856it [55:52,  1.45it/s]\u001b[A\n",
      "4857it [55:53,  1.45it/s]\u001b[A\n",
      "4858it [55:53,  1.45it/s]\u001b[A\n",
      "4859it [55:54,  1.45it/s]\u001b[A\n",
      "4860it [55:55,  1.45it/s]\u001b[A\n",
      "4861it [55:55,  1.45it/s]\u001b[A\n",
      "4862it [55:56,  1.45it/s]\u001b[A\n",
      "4863it [55:57,  1.45it/s]\u001b[A\n",
      "4864it [55:58,  1.45it/s]\u001b[A\n",
      "4865it [55:58,  1.45it/s]\u001b[A\n",
      "4866it [55:59,  1.45it/s]\u001b[A\n",
      "4867it [56:00,  1.45it/s]\u001b[A\n",
      "4868it [56:00,  1.45it/s]\u001b[A\n",
      "4869it [56:01,  1.45it/s]\u001b[A\n",
      "4870it [56:02,  1.45it/s]\u001b[A\n",
      "4871it [56:02,  1.45it/s]\u001b[A\n",
      "4872it [56:03,  1.45it/s]\u001b[A\n",
      "4873it [56:04,  1.45it/s]\u001b[A\n",
      "4874it [56:04,  1.45it/s]\u001b[A\n",
      "4875it [56:05,  1.45it/s]\u001b[A\n",
      "4876it [56:06,  1.45it/s]\u001b[A\n",
      "4877it [56:06,  1.45it/s]\u001b[A\n",
      "4878it [56:07,  1.45it/s]\u001b[A\n",
      "4879it [56:08,  1.45it/s]\u001b[A\n",
      "4880it [56:09,  1.45it/s]\u001b[A\n",
      "4881it [56:09,  1.45it/s]\u001b[A\n",
      "4882it [56:10,  1.45it/s]\u001b[A\n",
      "4883it [56:11,  1.45it/s]\u001b[A\n",
      "4884it [56:11,  1.45it/s]\u001b[A\n",
      "4885it [56:12,  1.45it/s]\u001b[A\n",
      "4886it [56:13,  1.45it/s]\u001b[A\n",
      "4887it [56:13,  1.45it/s]\u001b[A\n",
      "4888it [56:14,  1.45it/s]\u001b[A\n",
      "4889it [56:15,  1.45it/s]\u001b[A\n",
      "4890it [56:15,  1.45it/s]\u001b[A\n",
      "4891it [56:16,  1.45it/s]\u001b[A\n",
      "4892it [56:17,  1.45it/s]\u001b[A\n",
      "4893it [56:18,  1.45it/s]\u001b[A\n",
      "4894it [56:18,  1.45it/s]\u001b[A\n",
      "4895it [56:19,  1.45it/s]\u001b[A\n",
      "4896it [56:20,  1.45it/s]\u001b[A\n",
      "4897it [56:20,  1.45it/s]\u001b[A\n",
      "4898it [56:21,  1.45it/s]\u001b[A\n",
      "4899it [56:22,  1.45it/s]\u001b[A\n",
      "4900it [56:22,  1.45it/s]\u001b[A\n",
      "4901it [56:23,  1.45it/s]\u001b[A\n",
      "4902it [56:24,  1.45it/s]\u001b[A\n",
      "4903it [56:24,  1.45it/s]\u001b[A\n",
      "4904it [56:25,  1.45it/s]\u001b[A\n",
      "4905it [56:26,  1.45it/s]\u001b[A\n",
      "4906it [56:26,  1.45it/s]\u001b[A\n",
      "4907it [56:27,  1.45it/s]\u001b[A\n",
      "4908it [56:28,  1.45it/s]\u001b[A\n",
      "4909it [56:29,  1.45it/s]\u001b[A\n",
      "4910it [56:29,  1.45it/s]\u001b[A\n",
      "4911it [56:30,  1.45it/s]\u001b[A\n",
      "4912it [56:31,  1.45it/s]\u001b[A\n",
      "4913it [56:31,  1.45it/s]\u001b[A\n",
      "4914it [56:32,  1.45it/s]\u001b[A\n",
      "4915it [56:33,  1.45it/s]\u001b[A\n",
      "4916it [56:33,  1.45it/s]\u001b[A\n",
      "4917it [56:34,  1.45it/s]\u001b[A\n",
      "4918it [56:35,  1.45it/s]\u001b[A\n",
      "4919it [56:35,  1.45it/s]\u001b[A\n",
      "4920it [56:36,  1.45it/s]\u001b[A\n",
      "4921it [56:37,  1.45it/s]\u001b[A\n",
      "4922it [56:37,  1.45it/s]\u001b[A\n",
      "4923it [56:38,  1.45it/s]\u001b[A\n",
      "4924it [56:39,  1.45it/s]\u001b[A\n",
      "4925it [56:40,  1.45it/s]\u001b[A\n",
      "4926it [56:40,  1.45it/s]\u001b[A\n",
      "4927it [56:41,  1.45it/s]\u001b[A\n",
      "4928it [56:42,  1.45it/s]\u001b[A\n",
      "4929it [56:42,  1.45it/s]\u001b[A\n",
      "4930it [56:43,  1.45it/s]\u001b[A\n",
      "4931it [56:44,  1.45it/s]\u001b[A\n",
      "4932it [56:44,  1.45it/s]\u001b[A\n",
      "4933it [56:45,  1.45it/s]\u001b[A\n",
      "4934it [56:46,  1.45it/s]\u001b[A\n",
      "4935it [56:46,  1.45it/s]\u001b[A\n",
      "4936it [56:47,  1.45it/s]\u001b[A\n",
      "4937it [56:48,  1.45it/s]\u001b[A\n",
      "4938it [56:49,  1.45it/s]\u001b[A\n",
      "4939it [56:49,  1.45it/s]\u001b[A\n",
      "4940it [56:50,  1.45it/s]\u001b[A\n",
      "4941it [56:51,  1.45it/s]\u001b[A\n",
      "4942it [56:51,  1.45it/s]\u001b[A\n",
      "4943it [56:52,  1.45it/s]\u001b[A\n",
      "4944it [56:53,  1.45it/s]\u001b[A\n",
      "4945it [56:53,  1.45it/s]\u001b[A\n",
      "4946it [56:54,  1.45it/s]\u001b[A\n",
      "4947it [56:55,  1.45it/s]\u001b[A\n",
      "4948it [56:55,  1.45it/s]\u001b[A\n",
      "4949it [56:56,  1.45it/s]\u001b[A\n",
      "4950it [56:57,  1.45it/s]\u001b[A\n",
      "4951it [56:57,  1.45it/s]\u001b[A\n",
      "4952it [56:58,  1.45it/s]\u001b[A\n",
      "4953it [56:59,  1.45it/s]\u001b[A\n",
      "4954it [57:00,  1.45it/s]\u001b[A\n",
      "4955it [57:00,  1.45it/s]\u001b[A\n",
      "4956it [57:01,  1.45it/s]\u001b[A\n",
      "4957it [57:02,  1.45it/s]\u001b[A\n",
      "4958it [57:02,  1.45it/s]\u001b[A\n",
      "4959it [57:03,  1.45it/s]\u001b[A\n",
      "4960it [57:04,  1.45it/s]\u001b[A\n",
      "4961it [57:04,  1.45it/s]\u001b[A\n",
      "4962it [57:05,  1.45it/s]\u001b[A\n",
      "4963it [57:06,  1.45it/s]\u001b[A\n",
      "4964it [57:06,  1.45it/s]\u001b[A\n",
      "4965it [57:07,  1.45it/s]\u001b[A\n",
      "4966it [57:08,  1.45it/s]\u001b[A\n",
      "4967it [57:08,  1.45it/s]\u001b[A\n",
      "4968it [57:09,  1.45it/s]\u001b[A\n",
      "4969it [57:10,  1.45it/s]\u001b[A\n",
      "4970it [57:11,  1.45it/s]\u001b[A\n",
      "4971it [57:11,  1.45it/s]\u001b[A\n",
      "4972it [57:12,  1.45it/s]\u001b[A\n",
      "4973it [57:13,  1.45it/s]\u001b[A\n",
      "4974it [57:13,  1.45it/s]\u001b[A\n",
      "4975it [57:14,  1.45it/s]\u001b[A\n",
      "4976it [57:15,  1.45it/s]\u001b[A\n",
      "4977it [57:15,  1.45it/s]\u001b[A\n",
      "4978it [57:16,  1.45it/s]\u001b[A\n",
      "4979it [57:17,  1.45it/s]\u001b[A\n",
      "4980it [57:17,  1.45it/s]\u001b[A\n",
      "4981it [57:18,  1.45it/s]\u001b[A\n",
      "4982it [57:19,  1.45it/s]\u001b[A\n",
      "4983it [57:20,  1.45it/s]\u001b[A\n",
      "4984it [57:20,  1.45it/s]\u001b[A\n",
      "4985it [57:21,  1.45it/s]\u001b[A\n",
      "4986it [57:22,  1.45it/s]\u001b[A\n",
      "4987it [57:22,  1.45it/s]\u001b[A\n",
      "4988it [57:23,  1.45it/s]\u001b[A\n",
      "4989it [57:24,  1.45it/s]\u001b[A\n",
      "4990it [57:24,  1.45it/s]\u001b[A\n",
      "4991it [57:25,  1.45it/s]\u001b[A\n",
      "4992it [57:26,  1.45it/s]\u001b[A\n",
      "4993it [57:26,  1.45it/s]\u001b[A\n",
      "4994it [57:27,  1.45it/s]\u001b[A\n",
      "4995it [57:28,  1.45it/s]\u001b[A\n",
      "4996it [57:28,  1.45it/s]\u001b[A\n",
      "4997it [57:29,  1.45it/s]\u001b[A\n",
      "4998it [57:30,  1.45it/s]\u001b[A\n",
      "4999it [57:31,  1.45it/s]\u001b[A\n",
      "5000it [57:31,  1.45it/s]\u001b[A\n",
      "5001it [57:32,  1.45it/s]\u001b[A\n",
      "5002it [57:33,  1.45it/s]\u001b[A\n",
      "5003it [57:33,  1.45it/s]\u001b[A\n",
      "5004it [57:34,  1.45it/s]\u001b[A\n",
      "5005it [57:35,  1.45it/s]\u001b[A\n",
      "5006it [57:35,  1.45it/s]\u001b[A\n",
      "5007it [57:36,  1.45it/s]\u001b[A\n",
      "5008it [57:37,  1.45it/s]\u001b[A\n",
      "5009it [57:37,  1.45it/s]\u001b[A\n",
      "5010it [57:38,  1.45it/s]\u001b[A\n",
      "5011it [57:39,  1.45it/s]\u001b[A\n",
      "5012it [57:39,  1.45it/s]\u001b[A\n",
      "5013it [57:40,  1.45it/s]\u001b[A\n",
      "5014it [57:41,  1.45it/s]\u001b[A\n",
      "5015it [57:42,  1.45it/s]\u001b[A\n",
      "5016it [57:42,  1.45it/s]\u001b[A\n",
      "5017it [57:43,  1.45it/s]\u001b[A\n",
      "5018it [57:44,  1.45it/s]\u001b[A\n",
      "5019it [57:44,  1.45it/s]\u001b[A\n",
      "5020it [57:45,  1.45it/s]\u001b[A\n",
      "5021it [57:46,  1.45it/s]\u001b[A\n",
      "5022it [57:46,  1.45it/s]\u001b[A\n",
      "5023it [57:47,  1.45it/s]\u001b[A\n",
      "5024it [57:48,  1.45it/s]\u001b[A\n",
      "5025it [57:48,  1.45it/s]\u001b[A\n",
      "5026it [57:49,  1.45it/s]\u001b[A\n",
      "5027it [57:50,  1.45it/s]\u001b[A\n",
      "5028it [57:50,  1.45it/s]\u001b[A\n",
      "5029it [57:51,  1.45it/s]\u001b[A\n",
      "5030it [57:52,  1.45it/s]\u001b[A\n",
      "5031it [57:53,  1.45it/s]\u001b[A\n",
      "5032it [57:53,  1.45it/s]\u001b[A\n",
      "5033it [57:54,  1.45it/s]\u001b[A\n",
      "5034it [57:55,  1.45it/s]\u001b[A\n",
      "5035it [57:55,  1.45it/s]\u001b[A\n",
      "5036it [57:56,  1.45it/s]\u001b[A\n",
      "5037it [57:57,  1.45it/s]\u001b[A\n",
      "5038it [57:57,  1.45it/s]\u001b[A\n",
      "5039it [57:58,  1.45it/s]\u001b[A\n",
      "5040it [57:59,  1.45it/s]\u001b[A\n",
      "5041it [57:59,  1.45it/s]\u001b[A\n",
      "5042it [58:00,  1.45it/s]\u001b[A\n",
      "5043it [58:01,  1.45it/s]\u001b[A\n",
      "5044it [58:02,  1.45it/s]\u001b[A\n",
      "5045it [58:02,  1.45it/s]\u001b[A\n",
      "5046it [58:03,  1.45it/s]\u001b[A\n",
      "5047it [58:04,  1.45it/s]\u001b[A\n",
      "5048it [58:04,  1.45it/s]\u001b[A\n",
      "5049it [58:05,  1.45it/s]\u001b[A\n",
      "5050it [58:06,  1.45it/s]\u001b[A\n",
      "5051it [58:06,  1.45it/s]\u001b[A\n",
      "5052it [58:07,  1.45it/s]\u001b[A\n",
      "5053it [58:08,  1.45it/s]\u001b[A\n",
      "5054it [58:08,  1.45it/s]\u001b[A\n",
      "5055it [58:09,  1.45it/s]\u001b[A\n",
      "5056it [58:10,  1.45it/s]\u001b[A\n",
      "5057it [58:10,  1.45it/s]\u001b[A\n",
      "5058it [58:11,  1.45it/s]\u001b[A\n",
      "5059it [58:12,  1.45it/s]\u001b[A\n",
      "5060it [58:13,  1.45it/s]\u001b[A\n",
      "5061it [58:13,  1.45it/s]\u001b[A\n",
      "5062it [58:14,  1.45it/s]\u001b[A\n",
      "5063it [58:15,  1.45it/s]\u001b[A\n",
      "5064it [58:15,  1.45it/s]\u001b[A\n",
      "5065it [58:16,  1.45it/s]\u001b[A\n",
      "5066it [58:17,  1.45it/s]\u001b[A\n",
      "5067it [58:17,  1.45it/s]\u001b[A\n",
      "5068it [58:18,  1.45it/s]\u001b[A\n",
      "5069it [58:19,  1.45it/s]\u001b[A\n",
      "5070it [58:19,  1.45it/s]\u001b[A\n",
      "5071it [58:20,  1.45it/s]\u001b[A\n",
      "5072it [58:21,  1.45it/s]\u001b[A\n",
      "5073it [58:21,  1.45it/s]\u001b[A\n",
      "5074it [58:22,  1.45it/s]\u001b[A\n",
      "5075it [58:23,  1.45it/s]\u001b[A\n",
      "5076it [58:24,  1.45it/s]\u001b[A\n",
      "5077it [58:24,  1.45it/s]\u001b[A\n",
      "5078it [58:25,  1.45it/s]\u001b[A\n",
      "5079it [58:26,  1.45it/s]\u001b[A\n",
      "5080it [58:26,  1.45it/s]\u001b[A\n",
      "5081it [58:27,  1.45it/s]\u001b[A\n",
      "5082it [58:28,  1.45it/s]\u001b[A\n",
      "5083it [58:28,  1.45it/s]\u001b[A\n",
      "5084it [58:29,  1.45it/s]\u001b[A\n",
      "5085it [58:30,  1.45it/s]\u001b[A\n",
      "5086it [58:30,  1.45it/s]\u001b[A\n",
      "5087it [58:31,  1.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5088it [58:32,  1.45it/s]\u001b[A\n",
      "5089it [58:32,  1.45it/s]\u001b[A\n",
      "5090it [58:33,  1.45it/s]\u001b[A\n",
      "5091it [58:34,  1.45it/s]\u001b[A\n",
      "5092it [58:35,  1.45it/s]\u001b[A\n",
      "5093it [58:35,  1.45it/s]\u001b[A\n",
      "5094it [58:36,  1.45it/s]\u001b[A\n",
      "5095it [58:37,  1.45it/s]\u001b[A\n",
      "5096it [58:37,  1.45it/s]\u001b[A\n",
      "5097it [58:38,  1.45it/s]\u001b[A\n",
      "5098it [58:39,  1.45it/s]\u001b[A\n",
      "5099it [58:39,  1.45it/s]\u001b[A\n",
      "5100it [58:40,  1.45it/s]\u001b[A\n",
      "5101it [58:41,  1.45it/s]\u001b[A\n",
      "5102it [58:41,  1.45it/s]\u001b[A\n",
      "5103it [58:42,  1.45it/s]\u001b[A\n",
      "5104it [58:43,  1.45it/s]\u001b[A\n",
      "5105it [58:44,  1.45it/s]\u001b[A\n",
      "5106it [58:44,  1.45it/s]\u001b[A\n",
      "5107it [58:45,  1.45it/s]\u001b[A\n",
      "5108it [58:46,  1.45it/s]\u001b[A\n",
      "5109it [58:46,  1.45it/s]\u001b[A\n",
      "5110it [58:47,  1.45it/s]\u001b[A\n",
      "5111it [58:48,  1.45it/s]\u001b[A\n",
      "5112it [58:48,  1.45it/s]\u001b[A\n",
      "5113it [58:49,  1.45it/s]\u001b[A\n",
      "5114it [58:50,  1.45it/s]\u001b[A\n",
      "5115it [58:50,  1.45it/s]\u001b[A\n",
      "5116it [58:51,  1.45it/s]\u001b[A\n",
      "5117it [58:52,  1.45it/s]\u001b[A\n",
      "5118it [58:52,  1.45it/s]\u001b[A\n",
      "5119it [58:53,  1.45it/s]\u001b[A\n",
      "5120it [58:54,  1.45it/s]\u001b[A\n",
      "5121it [58:55,  1.45it/s]\u001b[A\n",
      "5122it [58:55,  1.45it/s]\u001b[A\n",
      "5123it [58:56,  1.45it/s]\u001b[A\n",
      "5124it [58:57,  1.45it/s]\u001b[A\n",
      "5125it [58:57,  1.45it/s]\u001b[A\n",
      "5126it [58:58,  1.45it/s]\u001b[A\n",
      "5127it [58:59,  1.45it/s]\u001b[A\n",
      "5128it [58:59,  1.45it/s]\u001b[A\n",
      "5129it [59:00,  1.45it/s]\u001b[A\n",
      "5130it [59:01,  1.45it/s]\u001b[A\n",
      "5131it [59:01,  1.45it/s]\u001b[A\n",
      "5132it [59:02,  1.45it/s]\u001b[A\n",
      "5133it [59:03,  1.45it/s]\u001b[A\n",
      "5134it [59:03,  1.45it/s]\u001b[A\n",
      "5135it [59:04,  1.45it/s]\u001b[A\n",
      "5136it [59:05,  1.45it/s]\u001b[A\n",
      "5137it [59:06,  1.45it/s]\u001b[A\n",
      "5138it [59:06,  1.45it/s]\u001b[A\n",
      "5139it [59:07,  1.45it/s]\u001b[A\n",
      "5140it [59:08,  1.45it/s]\u001b[A\n",
      "5141it [59:08,  1.45it/s]\u001b[A\n",
      "5142it [59:09,  1.45it/s]\u001b[A\n",
      "5143it [59:10,  1.45it/s]\u001b[A\n",
      "5144it [59:10,  1.45it/s]\u001b[A\n",
      "5145it [59:11,  1.45it/s]\u001b[A\n",
      "5146it [59:12,  1.45it/s]\u001b[A\n",
      "5147it [59:12,  1.45it/s]\u001b[A\n",
      "5148it [59:13,  1.45it/s]\u001b[A\n",
      "5149it [59:14,  1.45it/s]\u001b[A\n",
      "5150it [59:15,  1.45it/s]\u001b[A\n",
      "5151it [59:15,  1.45it/s]\u001b[A\n",
      "5152it [59:16,  1.45it/s]\u001b[A\n",
      "5153it [59:17,  1.45it/s]\u001b[A\n",
      "5154it [59:17,  1.45it/s]\u001b[A\n",
      "5155it [59:18,  1.45it/s]\u001b[A\n",
      "5156it [59:19,  1.45it/s]\u001b[A\n",
      "5157it [59:19,  1.45it/s]\u001b[A\n",
      "5158it [59:20,  1.45it/s]\u001b[A\n",
      "5159it [59:21,  1.45it/s]\u001b[A\n",
      "5160it [59:21,  1.45it/s]\u001b[A\n",
      "5161it [59:22,  1.45it/s]\u001b[A\n",
      "5162it [59:23,  1.45it/s]\u001b[A\n",
      "5163it [59:23,  1.45it/s]\u001b[A\n",
      "5164it [59:24,  1.45it/s]\u001b[A\n",
      "5165it [59:25,  1.45it/s]\u001b[A\n",
      "5166it [59:26,  1.45it/s]\u001b[A\n",
      "5167it [59:26,  1.45it/s]\u001b[A\n",
      "5168it [59:27,  1.45it/s]\u001b[A\n",
      "5169it [59:28,  1.45it/s]\u001b[A\n",
      "5170it [59:28,  1.45it/s]\u001b[A\n",
      "5171it [59:29,  1.45it/s]\u001b[A\n",
      "5172it [59:30,  1.45it/s]\u001b[A\n",
      "5173it [59:30,  1.45it/s]\u001b[A\n",
      "5174it [59:31,  1.45it/s]\u001b[A\n",
      "5175it [59:32,  1.45it/s]\u001b[A\n",
      "5176it [59:32,  1.45it/s]\u001b[A\n",
      "5177it [59:33,  1.45it/s]\u001b[A\n",
      "5178it [59:34,  1.45it/s]\u001b[A\n",
      "5179it [59:34,  1.45it/s]\u001b[A\n",
      "5180it [59:35,  1.45it/s]\u001b[A\n",
      "5181it [59:36,  1.45it/s]\u001b[A\n",
      "5182it [59:37,  1.45it/s]\u001b[A\n",
      "5183it [59:37,  1.45it/s]\u001b[A\n",
      "5184it [59:38,  1.45it/s]\u001b[A\n",
      "5185it [59:39,  1.45it/s]\u001b[A\n",
      "5186it [59:39,  1.45it/s]\u001b[A\n",
      "5187it [59:40,  1.45it/s]\u001b[A\n",
      "5188it [59:41,  1.45it/s]\u001b[A\n",
      "5189it [59:41,  1.45it/s]\u001b[A\n",
      "5190it [59:42,  1.45it/s]\u001b[A\n",
      "5191it [59:43,  1.45it/s]\u001b[A\n",
      "5192it [59:43,  1.45it/s]\u001b[A\n",
      "5193it [59:44,  1.45it/s]\u001b[A\n",
      "5194it [59:45,  1.45it/s]\u001b[A\n",
      "5195it [59:46,  1.45it/s]\u001b[A\n",
      "5196it [59:46,  1.45it/s]\u001b[A\n",
      "5197it [59:47,  1.45it/s]\u001b[A\n",
      "5198it [59:48,  1.45it/s]\u001b[A\n",
      "5199it [59:48,  1.45it/s]\u001b[A\n",
      "5200it [59:49,  1.45it/s]\u001b[A\n",
      "5201it [59:50,  1.45it/s]\u001b[A\n",
      "5202it [59:50,  1.45it/s]\u001b[A\n",
      "5203it [59:51,  1.45it/s]\u001b[A\n",
      "5204it [59:52,  1.45it/s]\u001b[A\n",
      "5205it [59:52,  1.45it/s]\u001b[A\n",
      "5206it [59:53,  1.45it/s]\u001b[A\n",
      "5207it [59:54,  1.45it/s]\u001b[A\n",
      "5208it [59:54,  1.45it/s]\u001b[A\n",
      "5209it [59:55,  1.45it/s]\u001b[A\n",
      "5210it [59:56,  1.45it/s]\u001b[A\n",
      "5211it [59:57,  1.45it/s]\u001b[A\n",
      "5212it [59:57,  1.45it/s]\u001b[A\n",
      "5213it [59:58,  1.45it/s]\u001b[A\n",
      "5214it [59:59,  1.45it/s]\u001b[A\n",
      "5215it [59:59,  1.45it/s]\u001b[A\n",
      "5216it [1:00:00,  1.45it/s]\u001b[A\n",
      "5217it [1:00:01,  1.45it/s]\u001b[A\n",
      "5218it [1:00:01,  1.45it/s]\u001b[A\n",
      "5219it [1:00:02,  1.45it/s]\u001b[A\n",
      "5220it [1:00:03,  1.45it/s]\u001b[A\n",
      "5221it [1:00:03,  1.45it/s]\u001b[A\n",
      "5222it [1:00:04,  1.45it/s]\u001b[A\n",
      "5223it [1:00:05,  1.45it/s]\u001b[A\n",
      "5224it [1:00:05,  1.45it/s]\u001b[A\n",
      "5225it [1:00:06,  1.45it/s]\u001b[A\n",
      "5226it [1:00:07,  1.45it/s]\u001b[A\n",
      "5227it [1:00:08,  1.45it/s]\u001b[A\n",
      "5228it [1:00:08,  1.45it/s]\u001b[A\n",
      "5229it [1:00:09,  1.45it/s]\u001b[A\n",
      "5230it [1:00:10,  1.45it/s]\u001b[A\n",
      "5231it [1:00:10,  1.45it/s]\u001b[A\n",
      "5232it [1:00:11,  1.45it/s]\u001b[A\n",
      "5233it [1:00:12,  1.45it/s]\u001b[A\n",
      "5234it [1:00:12,  1.45it/s]\u001b[A\n",
      "5235it [1:00:13,  1.45it/s]\u001b[A\n",
      "5236it [1:00:14,  1.45it/s]\u001b[A\n",
      "5237it [1:00:14,  1.45it/s]\u001b[A\n",
      "5238it [1:00:15,  1.45it/s]\u001b[A\n",
      "5239it [1:00:16,  1.45it/s]\u001b[A\n",
      "5240it [1:00:16,  1.45it/s]\u001b[A\n",
      "5241it [1:00:17,  1.45it/s]\u001b[A\n",
      "5242it [1:00:18,  1.45it/s]\u001b[A\n",
      "5243it [1:00:19,  1.45it/s]\u001b[A\n",
      "5244it [1:00:19,  1.45it/s]\u001b[A\n",
      "5245it [1:00:20,  1.45it/s]\u001b[A\n",
      "5246it [1:00:21,  1.45it/s]\u001b[A\n",
      "5247it [1:00:21,  1.45it/s]\u001b[A\n",
      "5248it [1:00:22,  1.45it/s]\u001b[A\n",
      "5249it [1:00:23,  1.45it/s]\u001b[A\n",
      "5250it [1:00:23,  1.45it/s]\u001b[A\n",
      "5251it [1:00:24,  1.45it/s]\u001b[A\n",
      "5252it [1:00:25,  1.45it/s]\u001b[A\n",
      "5253it [1:00:25,  1.45it/s]\u001b[A\n",
      "5254it [1:00:26,  1.45it/s]\u001b[A\n",
      "5255it [1:00:27,  1.45it/s]\u001b[A\n",
      "5256it [1:00:28,  1.45it/s]\u001b[A\n",
      "5257it [1:00:28,  1.45it/s]\u001b[A\n",
      "5258it [1:00:29,  1.45it/s]\u001b[A\n",
      "5259it [1:00:30,  1.45it/s]\u001b[A\n",
      "5260it [1:00:30,  1.45it/s]\u001b[A\n",
      "5261it [1:00:31,  1.45it/s]\u001b[A\n",
      "5262it [1:00:32,  1.45it/s]\u001b[A\n",
      "5263it [1:00:32,  1.45it/s]\u001b[A\n",
      "5264it [1:00:33,  1.45it/s]\u001b[A\n",
      "5265it [1:00:34,  1.45it/s]\u001b[A\n",
      "5266it [1:00:34,  1.45it/s]\u001b[A\n",
      "5267it [1:00:35,  1.45it/s]\u001b[A\n",
      "5268it [1:00:36,  1.45it/s]\u001b[A\n",
      "5269it [1:00:36,  1.45it/s]\u001b[A\n",
      "5270it [1:00:37,  1.45it/s]\u001b[A\n",
      "5271it [1:00:38,  1.45it/s]\u001b[A\n",
      "5272it [1:00:39,  1.45it/s]\u001b[A\n",
      "5273it [1:00:39,  1.45it/s]\u001b[A\n",
      "5274it [1:00:40,  1.45it/s]\u001b[A\n",
      "5275it [1:00:41,  1.45it/s]\u001b[A\n",
      "5276it [1:00:41,  1.45it/s]\u001b[A\n",
      "5277it [1:00:42,  1.45it/s]\u001b[A\n",
      "5278it [1:00:43,  1.45it/s]\u001b[A\n",
      "5279it [1:00:43,  1.45it/s]\u001b[A\n",
      "5280it [1:00:44,  1.45it/s]\u001b[A\n",
      "5281it [1:00:45,  1.45it/s]\u001b[A\n",
      "5282it [1:00:45,  1.45it/s]\u001b[A\n",
      "5283it [1:00:46,  1.45it/s]\u001b[A\n",
      "5284it [1:00:47,  1.45it/s]\u001b[A\n",
      "5285it [1:00:47,  1.45it/s]\u001b[A\n",
      "5286it [1:00:48,  1.45it/s]\u001b[A\n",
      "5287it [1:00:49,  1.45it/s]\u001b[A\n",
      "5288it [1:00:50,  1.45it/s]\u001b[A\n",
      "5289it [1:00:50,  1.45it/s]\u001b[A\n",
      "5290it [1:00:51,  1.45it/s]\u001b[A\n",
      "5291it [1:00:52,  1.45it/s]\u001b[A\n",
      "5292it [1:00:52,  1.45it/s]\u001b[A\n",
      "5293it [1:00:53,  1.45it/s]\u001b[A\n",
      "5294it [1:00:54,  1.45it/s]\u001b[A\n",
      "5295it [1:00:54,  1.45it/s]\u001b[A\n",
      "5296it [1:00:55,  1.45it/s]\u001b[A\n",
      "5297it [1:00:56,  1.45it/s]\u001b[A\n",
      "5298it [1:00:56,  1.46it/s]\u001b[A\n",
      "5299it [1:00:57,  1.45it/s]\u001b[A\n",
      "5300it [1:00:58,  1.45it/s]\u001b[A\n",
      "5301it [1:00:58,  1.45it/s]\u001b[A\n",
      "5302it [1:00:59,  1.45it/s]\u001b[A\n",
      "5303it [1:01:00,  1.45it/s]\u001b[A\n",
      "5304it [1:01:01,  1.45it/s]\u001b[A\n",
      "5305it [1:01:01,  1.45it/s]\u001b[A\n",
      "5306it [1:01:02,  1.45it/s]\u001b[A\n",
      "5307it [1:01:03,  1.45it/s]\u001b[A\n",
      "5308it [1:01:03,  1.45it/s]\u001b[A\n",
      "5309it [1:01:04,  1.45it/s]\u001b[A\n",
      "5310it [1:01:05,  1.45it/s]\u001b[A\n",
      "5311it [1:01:05,  1.45it/s]\u001b[A\n",
      "5312it [1:01:06,  1.45it/s]\u001b[A\n",
      "5313it [1:01:07,  1.45it/s]\u001b[A\n",
      "5314it [1:01:07,  1.45it/s]\u001b[A\n",
      "5315it [1:01:08,  1.45it/s]\u001b[A\n",
      "5316it [1:01:09,  1.45it/s]\u001b[A\n",
      "5317it [1:01:10,  1.45it/s]\u001b[A\n",
      "5318it [1:01:10,  1.45it/s]\u001b[A\n",
      "5319it [1:01:11,  1.45it/s]\u001b[A\n",
      "5320it [1:01:12,  1.45it/s]\u001b[A\n",
      "5321it [1:01:12,  1.45it/s]\u001b[A\n",
      "5322it [1:01:13,  1.45it/s]\u001b[A\n",
      "5323it [1:01:14,  1.45it/s]\u001b[A\n",
      "5324it [1:01:14,  1.45it/s]\u001b[A\n",
      "5325it [1:01:15,  1.45it/s]\u001b[A\n",
      "5326it [1:01:16,  1.45it/s]\u001b[A\n",
      "5327it [1:01:16,  1.45it/s]\u001b[A\n",
      "5328it [1:01:17,  1.45it/s]\u001b[A\n",
      "5329it [1:01:18,  1.45it/s]\u001b[A\n",
      "5330it [1:01:18,  1.45it/s]\u001b[A\n",
      "5331it [1:01:19,  1.45it/s]\u001b[A\n",
      "5332it [1:01:20,  1.45it/s]\u001b[A\n",
      "5333it [1:01:21,  1.45it/s]\u001b[A\n",
      "5334it [1:01:21,  1.45it/s]\u001b[A\n",
      "5335it [1:01:22,  1.45it/s]\u001b[A\n",
      "5336it [1:01:23,  1.45it/s]\u001b[A\n",
      "5337it [1:01:23,  1.45it/s]\u001b[A\n",
      "5338it [1:01:24,  1.45it/s]\u001b[A\n",
      "5339it [1:01:25,  1.45it/s]\u001b[A\n",
      "5340it [1:01:25,  1.45it/s]\u001b[A\n",
      "5341it [1:01:26,  1.45it/s]\u001b[A\n",
      "5342it [1:01:27,  1.45it/s]\u001b[A\n",
      "5343it [1:01:27,  1.45it/s]\u001b[A\n",
      "5344it [1:01:28,  1.45it/s]\u001b[A\n",
      "5345it [1:01:29,  1.45it/s]\u001b[A\n",
      "5346it [1:01:29,  1.45it/s]\u001b[A\n",
      "5347it [1:01:30,  1.45it/s]\u001b[A\n",
      "5348it [1:01:31,  1.45it/s]\u001b[A\n",
      "5349it [1:01:32,  1.45it/s]\u001b[A\n",
      "5350it [1:01:32,  1.45it/s]\u001b[A\n",
      "5351it [1:01:33,  1.45it/s]\u001b[A\n",
      "5352it [1:01:34,  1.45it/s]\u001b[A\n",
      "5353it [1:01:34,  1.45it/s]\u001b[A\n",
      "5354it [1:01:35,  1.45it/s]\u001b[A\n",
      "5355it [1:01:36,  1.45it/s]\u001b[A\n",
      "5356it [1:01:36,  1.45it/s]\u001b[A\n",
      "5357it [1:01:37,  1.45it/s]\u001b[A\n",
      "5358it [1:01:38,  1.45it/s]\u001b[A\n",
      "5359it [1:01:38,  1.45it/s]\u001b[A\n",
      "5360it [1:01:39,  1.45it/s]\u001b[A\n",
      "5361it [1:01:40,  1.45it/s]\u001b[A\n",
      "5362it [1:01:41,  1.45it/s]\u001b[A\n",
      "5363it [1:01:41,  1.45it/s]\u001b[A\n",
      "5364it [1:01:42,  1.45it/s]\u001b[A\n",
      "5365it [1:01:43,  1.45it/s]\u001b[A\n",
      "5366it [1:01:43,  1.45it/s]\u001b[A\n",
      "5367it [1:01:44,  1.45it/s]\u001b[A\n",
      "5368it [1:01:45,  1.45it/s]\u001b[A\n",
      "5369it [1:01:45,  1.45it/s]\u001b[A\n",
      "5370it [1:01:46,  1.45it/s]\u001b[A\n",
      "5371it [1:01:47,  1.45it/s]\u001b[A\n",
      "5372it [1:01:47,  1.45it/s]\u001b[A\n",
      "5373it [1:01:48,  1.45it/s]\u001b[A\n",
      "5374it [1:01:49,  1.45it/s]\u001b[A\n",
      "5375it [1:01:49,  1.45it/s]\u001b[A\n",
      "5376it [1:01:50,  1.45it/s]\u001b[A\n",
      "5377it [1:01:51,  1.45it/s]\u001b[A\n",
      "5378it [1:01:52,  1.45it/s]\u001b[A\n",
      "5379it [1:01:52,  1.45it/s]\u001b[A\n",
      "5380it [1:01:53,  1.45it/s]\u001b[A\n",
      "5381it [1:01:54,  1.45it/s]\u001b[A\n",
      "5382it [1:01:54,  1.45it/s]\u001b[A\n",
      "5383it [1:01:55,  1.45it/s]\u001b[A\n",
      "5384it [1:01:56,  1.45it/s]\u001b[A\n",
      "5385it [1:01:56,  1.45it/s]\u001b[A\n",
      "5386it [1:01:57,  1.45it/s]\u001b[A\n",
      "5387it [1:01:58,  1.45it/s]\u001b[A\n",
      "5388it [1:01:58,  1.45it/s]\u001b[A\n",
      "5389it [1:01:59,  1.45it/s]\u001b[A\n",
      "5390it [1:02:00,  1.45it/s]\u001b[A\n",
      "5391it [1:02:00,  1.45it/s]\u001b[A\n",
      "5392it [1:02:01,  1.45it/s]\u001b[A\n",
      "5393it [1:02:02,  1.45it/s]\u001b[A\n",
      "5394it [1:02:03,  1.45it/s]\u001b[A\n",
      "5395it [1:02:03,  1.45it/s]\u001b[A\n",
      "5396it [1:02:04,  1.45it/s]\u001b[A\n",
      "5397it [1:02:05,  1.45it/s]\u001b[A\n",
      "5398it [1:02:05,  1.45it/s]\u001b[A\n",
      "5399it [1:02:06,  1.45it/s]\u001b[A\n",
      "5400it [1:02:07,  1.45it/s]\u001b[A\n",
      "5401it [1:02:07,  1.45it/s]\u001b[A\n",
      "5402it [1:02:08,  1.45it/s]\u001b[A\n",
      "5403it [1:02:09,  1.45it/s]\u001b[A\n",
      "5404it [1:02:09,  1.45it/s]\u001b[A\n",
      "5405it [1:02:10,  1.45it/s]\u001b[A\n",
      "5406it [1:02:11,  1.45it/s]\u001b[A\n",
      "5407it [1:02:11,  1.45it/s]\u001b[A\n",
      "5408it [1:02:12,  1.45it/s]\u001b[A\n",
      "5409it [1:02:13,  1.45it/s]\u001b[A\n",
      "5410it [1:02:14,  1.45it/s]\u001b[A\n",
      "5411it [1:02:14,  1.45it/s]\u001b[A\n",
      "5412it [1:02:15,  1.45it/s]\u001b[A\n",
      "5413it [1:02:16,  1.45it/s]\u001b[A\n",
      "5414it [1:02:16,  1.45it/s]\u001b[A\n",
      "5415it [1:02:17,  1.45it/s]\u001b[A\n",
      "5416it [1:02:18,  1.45it/s]\u001b[A\n",
      "5417it [1:02:18,  1.45it/s]\u001b[A\n",
      "5418it [1:02:19,  1.45it/s]\u001b[A\n",
      "5419it [1:02:20,  1.45it/s]\u001b[A\n",
      "5420it [1:02:20,  1.45it/s]\u001b[A\n",
      "5421it [1:02:21,  1.45it/s]\u001b[A\n",
      "5422it [1:02:22,  1.45it/s]\u001b[A\n",
      "5423it [1:02:23,  1.45it/s]\u001b[A\n",
      "5424it [1:02:23,  1.45it/s]\u001b[A\n",
      "5425it [1:02:24,  1.45it/s]\u001b[A\n",
      "5426it [1:02:25,  1.45it/s]\u001b[A\n",
      "5427it [1:02:25,  1.45it/s]\u001b[A\n",
      "5428it [1:02:26,  1.45it/s]\u001b[A\n",
      "5429it [1:02:27,  1.45it/s]\u001b[A\n",
      "5430it [1:02:27,  1.45it/s]\u001b[A\n",
      "5431it [1:02:28,  1.45it/s]\u001b[A\n",
      "5432it [1:02:29,  1.45it/s]\u001b[A\n",
      "5433it [1:02:29,  1.45it/s]\u001b[A\n",
      "5434it [1:02:30,  1.45it/s]\u001b[A\n",
      "5435it [1:02:31,  1.45it/s]\u001b[A\n",
      "5436it [1:02:31,  1.45it/s]\u001b[A\n",
      "5437it [1:02:32,  1.45it/s]\u001b[A\n",
      "5438it [1:02:33,  1.45it/s]\u001b[A\n",
      "5439it [1:02:34,  1.45it/s]\u001b[A\n",
      "5440it [1:02:34,  1.45it/s]\u001b[A\n",
      "5441it [1:02:35,  1.45it/s]\u001b[A\n",
      "5442it [1:02:36,  1.45it/s]\u001b[A\n",
      "5443it [1:02:36,  1.45it/s]\u001b[A\n",
      "5444it [1:02:37,  1.45it/s]\u001b[A\n",
      "5445it [1:02:38,  1.45it/s]\u001b[A\n",
      "5446it [1:02:38,  1.45it/s]\u001b[A\n",
      "5447it [1:02:39,  1.45it/s]\u001b[A\n",
      "5448it [1:02:40,  1.45it/s]\u001b[A\n",
      "5449it [1:02:40,  1.45it/s]\u001b[A\n",
      "5450it [1:02:41,  1.45it/s]\u001b[A\n",
      "5451it [1:02:42,  1.45it/s]\u001b[A\n",
      "5452it [1:02:42,  1.45it/s]\u001b[A\n",
      "5453it [1:02:43,  1.45it/s]\u001b[A\n",
      "5454it [1:02:44,  1.45it/s]\u001b[A\n",
      "5455it [1:02:45,  1.45it/s]\u001b[A\n",
      "5456it [1:02:45,  1.45it/s]\u001b[A\n",
      "5457it [1:02:46,  1.45it/s]\u001b[A\n",
      "5458it [1:02:47,  1.45it/s]\u001b[A\n",
      "5459it [1:02:47,  1.45it/s]\u001b[A\n",
      "5460it [1:02:48,  1.45it/s]\u001b[A\n",
      "5461it [1:02:49,  1.45it/s]\u001b[A\n",
      "5462it [1:02:49,  1.45it/s]\u001b[A\n",
      "5463it [1:02:50,  1.45it/s]\u001b[A\n",
      "5464it [1:02:51,  1.45it/s]\u001b[A\n",
      "5465it [1:02:51,  1.45it/s]\u001b[A\n",
      "5466it [1:02:52,  1.45it/s]\u001b[A\n",
      "5467it [1:02:53,  1.45it/s]\u001b[A\n",
      "5468it [1:02:53,  1.45it/s]\u001b[A\n",
      "5469it [1:02:54,  1.45it/s]\u001b[A\n",
      "5470it [1:02:55,  1.45it/s]\u001b[A\n",
      "5471it [1:02:56,  1.45it/s]\u001b[A\n",
      "5472it [1:02:56,  1.45it/s]\u001b[A\n",
      "5473it [1:02:57,  1.45it/s]\u001b[A\n",
      "5474it [1:02:58,  1.45it/s]\u001b[A\n",
      "5475it [1:02:58,  1.45it/s]\u001b[A\n",
      "5476it [1:02:59,  1.45it/s]\u001b[A\n",
      "5477it [1:03:00,  1.45it/s]\u001b[A\n",
      "5478it [1:03:00,  1.45it/s]\u001b[A\n",
      "5479it [1:03:01,  1.45it/s]\u001b[A\n",
      "5480it [1:03:02,  1.45it/s]\u001b[A\n",
      "5481it [1:03:02,  1.45it/s]\u001b[A\n",
      "5482it [1:03:03,  1.45it/s]\u001b[A\n",
      "5483it [1:03:04,  1.45it/s]\u001b[A\n",
      "5484it [1:03:05,  1.45it/s]\u001b[A\n",
      "5485it [1:03:05,  1.45it/s]\u001b[A\n",
      "5486it [1:03:06,  1.45it/s]\u001b[A\n",
      "5487it [1:03:07,  1.45it/s]\u001b[A\n",
      "5488it [1:03:07,  1.45it/s]\u001b[A\n",
      "5489it [1:03:08,  1.45it/s]\u001b[A\n",
      "5490it [1:03:09,  1.45it/s]\u001b[A\n",
      "5491it [1:03:09,  1.45it/s]\u001b[A\n",
      "5492it [1:03:10,  1.45it/s]\u001b[A\n",
      "5493it [1:03:11,  1.45it/s]\u001b[A\n",
      "5494it [1:03:11,  1.45it/s]\u001b[A\n",
      "5495it [1:03:12,  1.45it/s]\u001b[A\n",
      "5496it [1:03:13,  1.45it/s]\u001b[A\n",
      "5497it [1:03:13,  1.45it/s]\u001b[A\n",
      "5498it [1:03:14,  1.45it/s]\u001b[A\n",
      "5499it [1:03:15,  1.45it/s]\u001b[A\n",
      "5500it [1:03:16,  1.45it/s]\u001b[A\n",
      "5501it [1:03:16,  1.45it/s]\u001b[A\n",
      "5502it [1:03:17,  1.45it/s]\u001b[A\n",
      "5503it [1:03:18,  1.45it/s]\u001b[A\n",
      "5504it [1:03:18,  1.45it/s]\u001b[A\n",
      "5505it [1:03:19,  1.45it/s]\u001b[A\n",
      "5506it [1:03:20,  1.45it/s]\u001b[A\n",
      "5507it [1:03:20,  1.45it/s]\u001b[A\n",
      "5508it [1:03:21,  1.45it/s]\u001b[A\n",
      "5509it [1:03:22,  1.45it/s]\u001b[A\n",
      "5510it [1:03:22,  1.45it/s]\u001b[A\n",
      "5511it [1:03:23,  1.45it/s]\u001b[A\n",
      "5512it [1:03:24,  1.45it/s]\u001b[A\n",
      "5513it [1:03:24,  1.45it/s]\u001b[A\n",
      "5514it [1:03:25,  1.45it/s]\u001b[A\n",
      "5515it [1:03:26,  1.45it/s]\u001b[A\n",
      "5516it [1:03:27,  1.45it/s]\u001b[A\n",
      "5517it [1:03:27,  1.45it/s]\u001b[A\n",
      "5518it [1:03:28,  1.45it/s]\u001b[A\n",
      "5519it [1:03:29,  1.45it/s]\u001b[A\n",
      "5520it [1:03:29,  1.45it/s]\u001b[A\n",
      "5521it [1:03:30,  1.45it/s]\u001b[A\n",
      "5522it [1:03:31,  1.45it/s]\u001b[A\n",
      "5523it [1:03:31,  1.45it/s]\u001b[A\n",
      "5524it [1:03:32,  1.45it/s]\u001b[A\n",
      "5525it [1:03:33,  1.45it/s]\u001b[A\n",
      "5526it [1:03:33,  1.45it/s]\u001b[A\n",
      "5527it [1:03:34,  1.45it/s]\u001b[A\n",
      "5528it [1:03:35,  1.45it/s]\u001b[A\n",
      "5529it [1:03:36,  1.45it/s]\u001b[A\n",
      "5530it [1:03:36,  1.45it/s]\u001b[A\n",
      "5531it [1:03:37,  1.45it/s]\u001b[A\n",
      "5532it [1:03:38,  1.45it/s]\u001b[A\n",
      "5533it [1:03:38,  1.45it/s]\u001b[A\n",
      "5534it [1:03:39,  1.45it/s]\u001b[A\n",
      "5535it [1:03:40,  1.45it/s]\u001b[A\n",
      "5536it [1:03:40,  1.45it/s]\u001b[A\n",
      "5537it [1:03:41,  1.45it/s]\u001b[A\n",
      "5538it [1:03:42,  1.45it/s]\u001b[A\n",
      "5539it [1:03:42,  1.45it/s]\u001b[A\n",
      "5540it [1:03:43,  1.45it/s]\u001b[A\n",
      "5541it [1:03:44,  1.45it/s]\u001b[A\n",
      "5542it [1:03:44,  1.45it/s]\u001b[A\n",
      "5543it [1:03:45,  1.45it/s]\u001b[A\n",
      "5544it [1:03:46,  1.45it/s]\u001b[A\n",
      "5545it [1:03:47,  1.45it/s]\u001b[A\n",
      "5546it [1:03:47,  1.45it/s]\u001b[A\n",
      "5547it [1:03:48,  1.45it/s]\u001b[A\n",
      "5548it [1:03:49,  1.45it/s]\u001b[A\n",
      "5549it [1:03:49,  1.45it/s]\u001b[A\n",
      "5550it [1:03:50,  1.45it/s]\u001b[A\n",
      "5551it [1:03:51,  1.45it/s]\u001b[A\n",
      "5552it [1:03:51,  1.45it/s]\u001b[A\n",
      "5553it [1:03:52,  1.45it/s]\u001b[A\n",
      "5554it [1:03:53,  1.45it/s]\u001b[A\n",
      "5555it [1:03:53,  1.45it/s]\u001b[A\n",
      "5556it [1:03:54,  1.45it/s]\u001b[A\n",
      "5557it [1:03:55,  1.45it/s]\u001b[A\n",
      "5558it [1:03:55,  1.45it/s]\u001b[A\n",
      "5559it [1:03:56,  1.45it/s]\u001b[A\n",
      "5560it [1:03:57,  1.45it/s]\u001b[A\n",
      "5561it [1:03:58,  1.45it/s]\u001b[A\n",
      "5562it [1:03:58,  1.45it/s]\u001b[A\n",
      "5563it [1:03:59,  1.45it/s]\u001b[A\n",
      "5564it [1:04:00,  1.45it/s]\u001b[A\n",
      "5565it [1:04:00,  1.45it/s]\u001b[A\n",
      "5566it [1:04:01,  1.45it/s]\u001b[A\n",
      "5567it [1:04:02,  1.45it/s]\u001b[A\n",
      "5568it [1:04:02,  1.45it/s]\u001b[A\n",
      "5569it [1:04:03,  1.45it/s]\u001b[A\n",
      "5570it [1:04:04,  1.45it/s]\u001b[A\n",
      "5571it [1:04:04,  1.45it/s]\u001b[A\n",
      "5572it [1:04:05,  1.45it/s]\u001b[A\n",
      "5573it [1:04:06,  1.45it/s]\u001b[A\n",
      "5574it [1:04:06,  1.45it/s]\u001b[A\n",
      "5575it [1:04:07,  1.45it/s]\u001b[A\n",
      "5576it [1:04:08,  1.45it/s]\u001b[A\n",
      "5577it [1:04:09,  1.45it/s]\u001b[A\n",
      "5578it [1:04:09,  1.45it/s]\u001b[A\n",
      "5579it [1:04:10,  1.45it/s]\u001b[A\n",
      "5580it [1:04:11,  1.45it/s]\u001b[A\n",
      "5581it [1:04:11,  1.45it/s]\u001b[A\n",
      "5582it [1:04:12,  1.45it/s]\u001b[A\n",
      "5583it [1:04:13,  1.45it/s]\u001b[A\n",
      "5584it [1:04:13,  1.45it/s]\u001b[A\n",
      "5585it [1:04:14,  1.45it/s]\u001b[A\n",
      "5586it [1:04:15,  1.45it/s]\u001b[A\n",
      "5587it [1:04:15,  1.45it/s]\u001b[A\n",
      "5588it [1:04:16,  1.45it/s]\u001b[A\n",
      "5589it [1:04:17,  1.45it/s]\u001b[A\n",
      "5590it [1:04:18,  1.45it/s]\u001b[A\n",
      "5591it [1:04:18,  1.45it/s]\u001b[A\n",
      "5592it [1:04:19,  1.45it/s]\u001b[A\n",
      "5593it [1:04:20,  1.45it/s]\u001b[A\n",
      "5594it [1:04:20,  1.45it/s]\u001b[A\n",
      "5595it [1:04:21,  1.45it/s]\u001b[A\n",
      "5596it [1:04:22,  1.45it/s]\u001b[A\n",
      "5597it [1:04:22,  1.45it/s]\u001b[A\n",
      "5598it [1:04:23,  1.45it/s]\u001b[A\n",
      "5599it [1:04:24,  1.45it/s]\u001b[A\n",
      "5600it [1:04:24,  1.45it/s]\u001b[A\n",
      "5601it [1:04:25,  1.45it/s]\u001b[A\n",
      "5602it [1:04:26,  1.45it/s]\u001b[A\n",
      "5603it [1:04:26,  1.45it/s]\u001b[A\n",
      "5604it [1:04:27,  1.45it/s]\u001b[A\n",
      "5605it [1:04:28,  1.45it/s]\u001b[A\n",
      "5606it [1:04:29,  1.45it/s]\u001b[A\n",
      "5607it [1:04:29,  1.45it/s]\u001b[A\n",
      "5608it [1:04:30,  1.45it/s]\u001b[A\n",
      "5609it [1:04:31,  1.45it/s]\u001b[A\n",
      "5610it [1:04:31,  1.45it/s]\u001b[A\n",
      "5611it [1:04:32,  1.45it/s]\u001b[A\n",
      "5612it [1:04:33,  1.45it/s]\u001b[A\n",
      "5613it [1:04:33,  1.45it/s]\u001b[A\n",
      "5614it [1:04:34,  1.45it/s]\u001b[A\n",
      "5615it [1:04:35,  1.45it/s]\u001b[A\n",
      "5616it [1:04:35,  1.45it/s]\u001b[A\n",
      "5617it [1:04:36,  1.45it/s]\u001b[A\n",
      "5618it [1:04:37,  1.45it/s]\u001b[A\n",
      "5619it [1:04:37,  1.45it/s]\u001b[A\n",
      "5620it [1:04:38,  1.45it/s]\u001b[A\n",
      "5621it [1:04:39,  1.45it/s]\u001b[A\n",
      "5622it [1:04:40,  1.45it/s]\u001b[A\n",
      "5623it [1:04:40,  1.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5624it [1:04:41,  1.45it/s]\u001b[A\n",
      "5625it [1:04:42,  1.45it/s]\u001b[A\n",
      "5626it [1:04:42,  1.45it/s]\u001b[A\n",
      "5627it [1:04:43,  1.45it/s]\u001b[A\n",
      "5628it [1:04:44,  1.45it/s]\u001b[A\n",
      "5629it [1:04:44,  1.45it/s]\u001b[A\n",
      "5630it [1:04:45,  1.45it/s]\u001b[A\n",
      "5631it [1:04:46,  1.45it/s]\u001b[A\n",
      "5632it [1:04:46,  1.45it/s]\u001b[A\n",
      "5633it [1:04:47,  1.45it/s]\u001b[A\n",
      "5634it [1:04:48,  1.45it/s]\u001b[A\n",
      "5635it [1:04:49,  1.45it/s]\u001b[A\n",
      "5636it [1:04:49,  1.45it/s]\u001b[A\n",
      "5637it [1:04:50,  1.45it/s]\u001b[A\n",
      "5638it [1:04:51,  1.45it/s]\u001b[A\n",
      "5639it [1:04:51,  1.45it/s]\u001b[A\n",
      "5640it [1:04:52,  1.45it/s]\u001b[A\n",
      "5641it [1:04:53,  1.45it/s]\u001b[A\n",
      "5642it [1:04:53,  1.45it/s]\u001b[A\n",
      "5643it [1:04:54,  1.45it/s]\u001b[A\n",
      "5644it [1:04:55,  1.45it/s]\u001b[A\n",
      "5645it [1:04:55,  1.45it/s]\u001b[A\n",
      "5646it [1:04:56,  1.45it/s]\u001b[A\n",
      "5647it [1:04:57,  1.45it/s]\u001b[A\n",
      "5648it [1:04:57,  1.45it/s]\u001b[A\n",
      "5649it [1:04:58,  1.45it/s]\u001b[A\n",
      "5650it [1:04:59,  1.45it/s]\u001b[A\n",
      "5651it [1:05:00,  1.45it/s]\u001b[A\n",
      "5652it [1:05:00,  1.45it/s]\u001b[A\n",
      "5653it [1:05:01,  1.45it/s]\u001b[A\n",
      "5654it [1:05:02,  1.45it/s]\u001b[A\n",
      "5655it [1:05:02,  1.45it/s]\u001b[A\n",
      "5656it [1:05:03,  1.45it/s]\u001b[A\n",
      "5657it [1:05:04,  1.45it/s]\u001b[A\n",
      "5658it [1:05:04,  1.45it/s]\u001b[A\n",
      "5659it [1:05:05,  1.45it/s]\u001b[A\n",
      "5660it [1:05:06,  1.45it/s]\u001b[A\n",
      "5661it [1:05:06,  1.45it/s]\u001b[A\n",
      "5662it [1:05:07,  1.45it/s]\u001b[A\n",
      "5663it [1:05:08,  1.45it/s]\u001b[A\n",
      "5664it [1:05:08,  1.45it/s]\u001b[A\n",
      "5665it [1:05:09,  1.45it/s]\u001b[A\n",
      "5666it [1:05:10,  1.45it/s]\u001b[A\n",
      "5667it [1:05:11,  1.45it/s]\u001b[A\n",
      "5668it [1:05:11,  1.45it/s]\u001b[A\n",
      "5669it [1:05:12,  1.45it/s]\u001b[A\n",
      "5670it [1:05:13,  1.45it/s]\u001b[A\n",
      "5671it [1:05:13,  1.45it/s]\u001b[A\n",
      "5672it [1:05:14,  1.45it/s]\u001b[A\n",
      "5673it [1:05:15,  1.45it/s]\u001b[A\n",
      "5674it [1:05:15,  1.45it/s]\u001b[A\n",
      "5675it [1:05:16,  1.45it/s]\u001b[A\n",
      "5676it [1:05:17,  1.45it/s]\u001b[A\n",
      "5677it [1:05:17,  1.45it/s]\u001b[A\n",
      "5678it [1:05:18,  1.45it/s]\u001b[A\n",
      "5679it [1:05:19,  1.45it/s]\u001b[A\n",
      "5680it [1:05:20,  1.45it/s]\u001b[A\n",
      "5681it [1:05:20,  1.45it/s]\u001b[A\n",
      "5682it [1:05:21,  1.45it/s]\u001b[A\n",
      "5683it [1:05:22,  1.45it/s]\u001b[A\n",
      "5684it [1:05:22,  1.45it/s]\u001b[A\n",
      "5685it [1:05:23,  1.45it/s]\u001b[A\n",
      "5686it [1:05:24,  1.45it/s]\u001b[A\n",
      "5687it [1:05:24,  1.45it/s]\u001b[A\n",
      "5688it [1:05:25,  1.45it/s]\u001b[A\n",
      "5689it [1:05:26,  1.45it/s]\u001b[A\n",
      "5690it [1:05:26,  1.45it/s]\u001b[A\n",
      "5691it [1:05:27,  1.45it/s]\u001b[A\n",
      "5692it [1:05:28,  1.45it/s]\u001b[A\n",
      "5693it [1:05:28,  1.45it/s]\u001b[A\n",
      "5694it [1:05:29,  1.45it/s]\u001b[A\n",
      "5695it [1:05:30,  1.45it/s]\u001b[A\n",
      "5696it [1:05:31,  1.45it/s]\u001b[A\n",
      "5697it [1:05:31,  1.45it/s]\u001b[A\n",
      "5698it [1:05:32,  1.45it/s]\u001b[A\n",
      "5699it [1:05:33,  1.45it/s]\u001b[A\n",
      "5700it [1:05:33,  1.45it/s]\u001b[A\n",
      "5701it [1:05:34,  1.45it/s]\u001b[A\n",
      "5702it [1:05:35,  1.45it/s]\u001b[A\n",
      "5703it [1:05:35,  1.45it/s]\u001b[A\n",
      "5704it [1:05:36,  1.45it/s]\u001b[A\n",
      "5705it [1:05:37,  1.45it/s]\u001b[A\n",
      "5706it [1:05:37,  1.45it/s]\u001b[A\n",
      "5707it [1:05:38,  1.45it/s]\u001b[A\n",
      "5708it [1:05:39,  1.45it/s]\u001b[A\n",
      "5709it [1:05:39,  1.45it/s]\u001b[A\n",
      "5710it [1:05:40,  1.45it/s]\u001b[A\n",
      "5711it [1:05:41,  1.45it/s]\u001b[A\n",
      "5712it [1:05:42,  1.45it/s]\u001b[A\n",
      "5713it [1:05:42,  1.45it/s]\u001b[A\n",
      "5714it [1:05:43,  1.45it/s]\u001b[A\n",
      "5715it [1:05:44,  1.45it/s]\u001b[A\n",
      "5716it [1:05:44,  1.45it/s]\u001b[A\n",
      "5717it [1:05:45,  1.45it/s]\u001b[A\n",
      "5718it [1:05:46,  1.45it/s]\u001b[A\n",
      "5719it [1:05:46,  1.45it/s]\u001b[A\n",
      "5720it [1:05:47,  1.45it/s]\u001b[A\n",
      "5721it [1:05:48,  1.45it/s]\u001b[A\n",
      "5722it [1:05:48,  1.45it/s]\u001b[A\n",
      "5723it [1:05:49,  1.45it/s]\u001b[A\n",
      "5724it [1:05:50,  1.45it/s]\u001b[A\n",
      "5725it [1:05:50,  1.45it/s]\u001b[A\n",
      "5726it [1:05:51,  1.45it/s]\u001b[A\n",
      "5727it [1:05:52,  1.45it/s]\u001b[A\n",
      "5728it [1:05:53,  1.45it/s]\u001b[A\n",
      "5729it [1:05:53,  1.45it/s]\u001b[A\n",
      "5730it [1:05:54,  1.45it/s]\u001b[A\n",
      "5731it [1:05:55,  1.45it/s]\u001b[A\n",
      "5732it [1:05:55,  1.45it/s]\u001b[A\n",
      "5733it [1:05:56,  1.45it/s]\u001b[A\n",
      "5734it [1:05:57,  1.45it/s]\u001b[A\n",
      "5735it [1:05:57,  1.45it/s]\u001b[A\n",
      "5736it [1:05:58,  1.45it/s]\u001b[A\n",
      "5737it [1:05:59,  1.45it/s]\u001b[A\n",
      "5738it [1:05:59,  1.45it/s]\u001b[A\n",
      "5739it [1:06:00,  1.45it/s]\u001b[A\n",
      "5740it [1:06:01,  1.45it/s]\u001b[A\n",
      "5741it [1:06:02,  1.45it/s]\u001b[A\n",
      "5742it [1:06:02,  1.45it/s]\u001b[A\n",
      "5743it [1:06:03,  1.45it/s]\u001b[A\n",
      "5744it [1:06:04,  1.45it/s]\u001b[A\n",
      "5745it [1:06:04,  1.45it/s]\u001b[A\n",
      "5746it [1:06:05,  1.45it/s]\u001b[A\n",
      "5747it [1:06:06,  1.45it/s]\u001b[A\n",
      "5748it [1:06:06,  1.45it/s]\u001b[A\n",
      "5749it [1:06:07,  1.45it/s]\u001b[A\n",
      "5750it [1:06:08,  1.45it/s]\u001b[A\n",
      "5751it [1:06:08,  1.45it/s]\u001b[A\n",
      "5752it [1:06:09,  1.45it/s]\u001b[A\n",
      "5753it [1:06:10,  1.45it/s]\u001b[A\n",
      "5754it [1:06:10,  1.45it/s]\u001b[A\n",
      "5755it [1:06:11,  1.45it/s]\u001b[A\n",
      "5756it [1:06:12,  1.45it/s]\u001b[A\n",
      "5757it [1:06:13,  1.45it/s]\u001b[A\n",
      "5758it [1:06:13,  1.45it/s]\u001b[A\n",
      "5759it [1:06:14,  1.45it/s]\u001b[A\n",
      "5760it [1:06:15,  1.45it/s]\u001b[A\n",
      "5761it [1:06:15,  1.45it/s]\u001b[A\n",
      "5762it [1:06:16,  1.45it/s]\u001b[A\n",
      "5763it [1:06:17,  1.45it/s]\u001b[A\n",
      "5764it [1:06:17,  1.45it/s]\u001b[A\n",
      "5765it [1:06:18,  1.45it/s]\u001b[A\n",
      "5766it [1:06:19,  1.45it/s]\u001b[A\n",
      "5767it [1:06:19,  1.45it/s]\u001b[A\n",
      "5768it [1:06:20,  1.45it/s]\u001b[A\n",
      "5769it [1:06:21,  1.45it/s]\u001b[A\n",
      "5770it [1:06:21,  1.45it/s]\u001b[A\n",
      "5771it [1:06:22,  1.45it/s]\u001b[A\n",
      "5772it [1:06:23,  1.45it/s]\u001b[A\n",
      "5773it [1:06:24,  1.45it/s]\u001b[A\n",
      "5774it [1:06:24,  1.45it/s]\u001b[A\n",
      "5775it [1:06:25,  1.45it/s]\u001b[A\n",
      "5776it [1:06:26,  1.45it/s]\u001b[A\n",
      "5777it [1:06:26,  1.45it/s]\u001b[A\n",
      "5778it [1:06:27,  1.45it/s]\u001b[A\n",
      "5779it [1:06:28,  1.45it/s]\u001b[A\n",
      "5780it [1:06:28,  1.45it/s]\u001b[A\n",
      "5781it [1:06:29,  1.45it/s]\u001b[A\n",
      "5782it [1:06:30,  1.45it/s]\u001b[A\n",
      "5783it [1:06:30,  1.45it/s]\u001b[A\n",
      "5784it [1:06:31,  1.45it/s]\u001b[A\n",
      "5785it [1:06:32,  1.45it/s]\u001b[A\n",
      "5786it [1:06:33,  1.45it/s]\u001b[A\n",
      "5787it [1:06:33,  1.45it/s]\u001b[A\n",
      "5788it [1:06:34,  1.45it/s]\u001b[A\n",
      "5789it [1:06:35,  1.45it/s]\u001b[A\n",
      "5790it [1:06:35,  1.45it/s]\u001b[A\n",
      "5791it [1:06:36,  1.45it/s]\u001b[A\n",
      "5792it [1:06:37,  1.45it/s]\u001b[A\n",
      "5793it [1:06:37,  1.45it/s]\u001b[A\n",
      "5794it [1:06:38,  1.45it/s]\u001b[A\n",
      "5795it [1:06:39,  1.45it/s]\u001b[A\n",
      "5796it [1:06:39,  1.45it/s]\u001b[A\n",
      "5797it [1:06:40,  1.45it/s]\u001b[A\n",
      "5798it [1:06:41,  1.45it/s]\u001b[A\n",
      "5799it [1:06:41,  1.45it/s]\u001b[A\n",
      "5800it [1:06:42,  1.45it/s]\u001b[A\n",
      "5801it [1:06:43,  1.45it/s]\u001b[A\n",
      "5802it [1:06:44,  1.45it/s]\u001b[A\n",
      "5803it [1:06:44,  1.45it/s]\u001b[A\n",
      "5804it [1:06:45,  1.45it/s]\u001b[A\n",
      "5805it [1:06:46,  1.45it/s]\u001b[A\n",
      "5806it [1:06:46,  1.45it/s]\u001b[A\n",
      "5807it [1:06:47,  1.45it/s]\u001b[A\n",
      "5808it [1:06:48,  1.45it/s]\u001b[A\n",
      "5809it [1:06:48,  1.45it/s]\u001b[A\n",
      "5810it [1:06:49,  1.45it/s]\u001b[A\n",
      "5811it [1:06:50,  1.45it/s]\u001b[A\n",
      "5812it [1:06:50,  1.45it/s]\u001b[A\n",
      "5813it [1:06:51,  1.45it/s]\u001b[A\n",
      "5814it [1:06:52,  1.45it/s]\u001b[A\n",
      "5815it [1:06:52,  1.45it/s]\u001b[A\n",
      "5816it [1:06:53,  1.45it/s]\u001b[A\n",
      "5817it [1:06:54,  1.45it/s]\u001b[A\n",
      "5818it [1:06:55,  1.45it/s]\u001b[A\n",
      "5819it [1:06:55,  1.45it/s]\u001b[A\n",
      "5820it [1:06:56,  1.45it/s]\u001b[A\n",
      "5821it [1:06:57,  1.45it/s]\u001b[A\n",
      "5822it [1:06:57,  1.45it/s]\u001b[A\n",
      "5823it [1:06:58,  1.45it/s]\u001b[A\n",
      "5824it [1:06:59,  1.45it/s]\u001b[A\n",
      "5825it [1:06:59,  1.45it/s]\u001b[A\n",
      "5826it [1:07:00,  1.45it/s]\u001b[A\n",
      "5827it [1:07:01,  1.45it/s]\u001b[A\n",
      "5828it [1:07:01,  1.45it/s]\u001b[A\n",
      "5829it [1:07:02,  1.45it/s]\u001b[A\n",
      "5830it [1:07:03,  1.45it/s]\u001b[A\n",
      "5831it [1:07:04,  1.45it/s]\u001b[A\n",
      "5832it [1:07:04,  1.45it/s]\u001b[A\n",
      "5833it [1:07:05,  1.45it/s]\u001b[A\n",
      "5834it [1:07:06,  1.45it/s]\u001b[A\n",
      "5835it [1:07:06,  1.45it/s]\u001b[A\n",
      "5836it [1:07:07,  1.45it/s]\u001b[A\n",
      "5837it [1:07:08,  1.45it/s]\u001b[A\n",
      "5838it [1:07:08,  1.45it/s]\u001b[A\n",
      "5839it [1:07:09,  1.45it/s]\u001b[A\n",
      "5840it [1:07:10,  1.45it/s]\u001b[A\n",
      "5841it [1:07:10,  1.45it/s]\u001b[A\n",
      "5842it [1:07:11,  1.45it/s]\u001b[A\n",
      "5843it [1:07:12,  1.45it/s]\u001b[A\n",
      "5844it [1:07:12,  1.45it/s]\u001b[A\n",
      "5845it [1:07:13,  1.45it/s]\u001b[A\n",
      "5846it [1:07:14,  1.45it/s]\u001b[A\n",
      "5847it [1:07:15,  1.45it/s]\u001b[A\n",
      "5848it [1:07:15,  1.45it/s]\u001b[A\n",
      "5849it [1:07:16,  1.45it/s]\u001b[A\n",
      "5850it [1:07:17,  1.45it/s]\u001b[A\n",
      "5851it [1:07:17,  1.45it/s]\u001b[A\n",
      "5852it [1:07:18,  1.45it/s]\u001b[A\n",
      "5853it [1:07:19,  1.45it/s]\u001b[A\n",
      "5854it [1:07:19,  1.45it/s]\u001b[A\n",
      "5855it [1:07:20,  1.45it/s]\u001b[A\n",
      "5856it [1:07:21,  1.45it/s]\u001b[A\n",
      "5857it [1:07:21,  1.45it/s]\u001b[A\n",
      "5858it [1:07:22,  1.45it/s]\u001b[A\n",
      "5859it [1:07:23,  1.45it/s]\u001b[A\n",
      "5860it [1:07:23,  1.45it/s]\u001b[A\n",
      "5861it [1:07:24,  1.45it/s]\u001b[A\n",
      "5862it [1:07:25,  1.45it/s]\u001b[A\n",
      "5863it [1:07:26,  1.45it/s]\u001b[A\n",
      "5864it [1:07:26,  1.45it/s]\u001b[A\n",
      "5865it [1:07:27,  1.45it/s]\u001b[A\n",
      "5866it [1:07:28,  1.45it/s]\u001b[A\n",
      "5867it [1:07:28,  1.45it/s]\u001b[A\n",
      "5868it [1:07:29,  1.45it/s]\u001b[A\n",
      "5869it [1:07:30,  1.45it/s]\u001b[A\n",
      "5870it [1:07:30,  1.45it/s]\u001b[A\n",
      "5871it [1:07:31,  1.45it/s]\u001b[A\n",
      "5872it [1:07:32,  1.45it/s]\u001b[A\n",
      "5873it [1:07:32,  1.45it/s]\u001b[A\n",
      "5874it [1:07:33,  1.45it/s]\u001b[A\n",
      "5875it [1:07:34,  1.45it/s]\u001b[A\n",
      "5876it [1:07:34,  1.45it/s]\u001b[A\n",
      "5877it [1:07:35,  1.45it/s]\u001b[A\n",
      "5878it [1:07:36,  1.45it/s]\u001b[A\n",
      "5879it [1:07:37,  1.45it/s]\u001b[A\n",
      "5880it [1:07:37,  1.45it/s]\u001b[A\n",
      "5881it [1:07:38,  1.45it/s]\u001b[A\n",
      "5882it [1:07:39,  1.45it/s]\u001b[A\n",
      "5883it [1:07:39,  1.45it/s]\u001b[A\n",
      "5884it [1:07:40,  1.45it/s]\u001b[A\n",
      "5885it [1:07:41,  1.45it/s]\u001b[A\n",
      "5886it [1:07:41,  1.45it/s]\u001b[A\n",
      "5887it [1:07:42,  1.45it/s]\u001b[A\n",
      "5888it [1:07:43,  1.45it/s]\u001b[A\n",
      "5889it [1:07:43,  1.45it/s]\u001b[A\n",
      "5890it [1:07:44,  1.45it/s]\u001b[A\n",
      "5891it [1:07:45,  1.45it/s]\u001b[A\n",
      "5892it [1:07:46,  1.45it/s]\u001b[A\n",
      "5893it [1:07:46,  1.45it/s]\u001b[A\n",
      "5894it [1:07:47,  1.45it/s]\u001b[A\n",
      "5895it [1:07:48,  1.45it/s]\u001b[A\n",
      "5896it [1:07:48,  1.45it/s]\u001b[A\n",
      "5897it [1:07:49,  1.45it/s]\u001b[A\n",
      "5898it [1:07:50,  1.45it/s]\u001b[A\n",
      "5899it [1:07:50,  1.45it/s]\u001b[A\n",
      "5900it [1:07:51,  1.45it/s]\u001b[A\n",
      "5901it [1:07:52,  1.45it/s]\u001b[A\n",
      "5902it [1:07:52,  1.45it/s]\u001b[A\n",
      "5903it [1:07:53,  1.45it/s]\u001b[A\n",
      "5904it [1:07:54,  1.45it/s]\u001b[A\n",
      "5905it [1:07:54,  1.45it/s]\u001b[A\n",
      "5906it [1:07:55,  1.45it/s]\u001b[A\n",
      "5907it [1:07:56,  1.45it/s]\u001b[A\n",
      "5908it [1:07:57,  1.45it/s]\u001b[A\n",
      "5909it [1:07:57,  1.45it/s]\u001b[A\n",
      "5910it [1:07:58,  1.45it/s]\u001b[A\n",
      "5911it [1:07:59,  1.45it/s]\u001b[A\n",
      "5912it [1:07:59,  1.45it/s]\u001b[A\n",
      "5913it [1:08:00,  1.45it/s]\u001b[A\n",
      "5914it [1:08:01,  1.45it/s]\u001b[A\n",
      "5915it [1:08:01,  1.45it/s]\u001b[A\n",
      "5916it [1:08:02,  1.45it/s]\u001b[A\n",
      "5917it [1:08:03,  1.45it/s]\u001b[A\n",
      "5918it [1:08:03,  1.45it/s]\u001b[A\n",
      "5919it [1:08:04,  1.45it/s]\u001b[A\n",
      "5920it [1:08:05,  1.45it/s]\u001b[A\n",
      "5921it [1:08:05,  1.45it/s]\u001b[A\n",
      "5922it [1:08:06,  1.45it/s]\u001b[A\n",
      "5923it [1:08:07,  1.45it/s]\u001b[A\n",
      "5924it [1:08:08,  1.45it/s]\u001b[A\n",
      "5925it [1:08:08,  1.45it/s]\u001b[A\n",
      "5926it [1:08:09,  1.45it/s]\u001b[A\n",
      "5927it [1:08:10,  1.45it/s]\u001b[A\n",
      "5928it [1:08:10,  1.45it/s]\u001b[A\n",
      "5929it [1:08:11,  1.45it/s]\u001b[A\n",
      "5930it [1:08:12,  1.45it/s]\u001b[A\n",
      "5931it [1:08:12,  1.45it/s]\u001b[A\n",
      "5932it [1:08:13,  1.45it/s]\u001b[A\n",
      "5933it [1:08:14,  1.45it/s]\u001b[A\n",
      "5934it [1:08:14,  1.45it/s]\u001b[A\n",
      "5935it [1:08:15,  1.45it/s]\u001b[A\n",
      "5936it [1:08:16,  1.45it/s]\u001b[A\n",
      "5937it [1:08:16,  1.45it/s]\u001b[A\n",
      "5938it [1:08:17,  1.45it/s]\u001b[A\n",
      "5939it [1:08:18,  1.45it/s]\u001b[A\n",
      "5940it [1:08:19,  1.45it/s]\u001b[A\n",
      "5941it [1:08:19,  1.45it/s]\u001b[A\n",
      "5942it [1:08:20,  1.45it/s]\u001b[A\n",
      "5943it [1:08:21,  1.45it/s]\u001b[A\n",
      "5944it [1:08:21,  1.45it/s]\u001b[A\n",
      "5945it [1:08:22,  1.45it/s]\u001b[A\n",
      "5946it [1:08:23,  1.45it/s]\u001b[A\n",
      "5947it [1:08:23,  1.45it/s]\u001b[A\n",
      "5948it [1:08:24,  1.45it/s]\u001b[A\n",
      "5949it [1:08:25,  1.45it/s]\u001b[A\n",
      "5950it [1:08:25,  1.45it/s]\u001b[A\n",
      "5951it [1:08:26,  1.45it/s]\u001b[A\n",
      "5952it [1:08:27,  1.45it/s]\u001b[A\n",
      "5953it [1:08:28,  1.45it/s]\u001b[A\n",
      "5954it [1:08:28,  1.45it/s]\u001b[A\n",
      "5955it [1:08:29,  1.45it/s]\u001b[A\n",
      "5956it [1:08:30,  1.45it/s]\u001b[A\n",
      "5957it [1:08:30,  1.45it/s]\u001b[A\n",
      "5958it [1:08:31,  1.45it/s]\u001b[A\n",
      "5959it [1:08:32,  1.45it/s]\u001b[A\n",
      "5960it [1:08:32,  1.45it/s]\u001b[A\n",
      "5961it [1:08:33,  1.45it/s]\u001b[A\n",
      "5962it [1:08:34,  1.45it/s]\u001b[A\n",
      "5963it [1:08:34,  1.45it/s]\u001b[A\n",
      "5964it [1:08:35,  1.45it/s]\u001b[A\n",
      "5965it [1:08:36,  1.45it/s]\u001b[A\n",
      "5966it [1:08:36,  1.45it/s]\u001b[A\n",
      "5967it [1:08:37,  1.45it/s]\u001b[A\n",
      "5968it [1:08:38,  1.45it/s]\u001b[A\n",
      "5969it [1:08:39,  1.45it/s]\u001b[A\n",
      "5970it [1:08:39,  1.45it/s]\u001b[A\n",
      "5971it [1:08:40,  1.45it/s]\u001b[A\n",
      "5972it [1:08:41,  1.45it/s]\u001b[A\n",
      "5973it [1:08:41,  1.45it/s]\u001b[A\n",
      "5974it [1:08:42,  1.45it/s]\u001b[A\n",
      "5975it [1:08:43,  1.45it/s]\u001b[A\n",
      "5976it [1:08:43,  1.45it/s]\u001b[A\n",
      "5977it [1:08:44,  1.45it/s]\u001b[A\n",
      "5978it [1:08:45,  1.45it/s]\u001b[A\n",
      "5979it [1:08:45,  1.45it/s]\u001b[A\n",
      "5980it [1:08:46,  1.45it/s]\u001b[A\n",
      "5981it [1:08:47,  1.45it/s]\u001b[A\n",
      "5982it [1:08:47,  1.45it/s]\u001b[A\n",
      "5983it [1:08:48,  1.45it/s]\u001b[A\n",
      "5984it [1:08:49,  1.45it/s]\u001b[A\n",
      "5985it [1:08:50,  1.45it/s]\u001b[A\n",
      "5986it [1:08:50,  1.45it/s]\u001b[A\n",
      "5987it [1:08:51,  1.45it/s]\u001b[A\n",
      "5988it [1:08:52,  1.45it/s]\u001b[A\n",
      "5989it [1:08:52,  1.45it/s]\u001b[A\n",
      "5990it [1:08:53,  1.45it/s]\u001b[A\n",
      "5991it [1:08:54,  1.45it/s]\u001b[A\n",
      "5992it [1:08:54,  1.45it/s]\u001b[A\n",
      "5993it [1:08:55,  1.45it/s]\u001b[A\n",
      "5994it [1:08:56,  1.45it/s]\u001b[A\n",
      "5995it [1:08:56,  1.45it/s]\u001b[A\n",
      "5996it [1:08:57,  1.45it/s]\u001b[A\n",
      "5997it [1:08:58,  1.45it/s]\u001b[A\n",
      "5998it [1:08:59,  1.45it/s]\u001b[A\n",
      "5999it [1:08:59,  1.45it/s]\u001b[A\n",
      "6000it [1:09:00,  1.45it/s]\u001b[A\n",
      "6001it [1:09:01,  1.45it/s]\u001b[A\n",
      "6002it [1:09:01,  1.45it/s]\u001b[A\n",
      "6003it [1:09:02,  1.45it/s]\u001b[A\n",
      "6004it [1:09:03,  1.45it/s]\u001b[A\n",
      "6005it [1:09:03,  1.45it/s]\u001b[A\n",
      "6006it [1:09:04,  1.45it/s]\u001b[A\n",
      "6007it [1:09:05,  1.45it/s]\u001b[A\n",
      "6008it [1:09:05,  1.45it/s]\u001b[A\n",
      "6009it [1:09:06,  1.45it/s]\u001b[A\n",
      "6010it [1:09:07,  1.45it/s]\u001b[A\n",
      "6011it [1:09:07,  1.45it/s]\u001b[A\n",
      "6012it [1:09:08,  1.45it/s]\u001b[A\n",
      "6013it [1:09:09,  1.45it/s]\u001b[A\n",
      "6014it [1:09:10,  1.45it/s]\u001b[A\n",
      "6015it [1:09:10,  1.45it/s]\u001b[A\n",
      "6016it [1:09:11,  1.45it/s]\u001b[A\n",
      "6017it [1:09:12,  1.45it/s]\u001b[A\n",
      "6018it [1:09:12,  1.45it/s]\u001b[A\n",
      "6019it [1:09:13,  1.45it/s]\u001b[A\n",
      "6020it [1:09:14,  1.45it/s]\u001b[A\n",
      "6021it [1:09:14,  1.45it/s]\u001b[A\n",
      "6022it [1:09:15,  1.45it/s]\u001b[A\n",
      "6023it [1:09:16,  1.45it/s]\u001b[A\n",
      "6024it [1:09:16,  1.45it/s]\u001b[A\n",
      "6025it [1:09:17,  1.45it/s]\u001b[A\n",
      "6026it [1:09:18,  1.45it/s]\u001b[A\n",
      "6027it [1:09:18,  1.45it/s]\u001b[A\n",
      "6028it [1:09:19,  1.45it/s]\u001b[A\n",
      "6029it [1:09:20,  1.45it/s]\u001b[A\n",
      "6030it [1:09:21,  1.45it/s]\u001b[A\n",
      "6031it [1:09:21,  1.45it/s]\u001b[A\n",
      "6032it [1:09:22,  1.45it/s]\u001b[A\n",
      "6033it [1:09:23,  1.45it/s]\u001b[A\n",
      "6034it [1:09:23,  1.45it/s]\u001b[A\n",
      "6035it [1:09:24,  1.45it/s]\u001b[A\n",
      "6036it [1:09:25,  1.45it/s]\u001b[A\n",
      "6037it [1:09:25,  1.45it/s]\u001b[A\n",
      "6038it [1:09:26,  1.45it/s]\u001b[A\n",
      "6039it [1:09:27,  1.45it/s]\u001b[A\n",
      "6040it [1:09:27,  1.45it/s]\u001b[A\n",
      "6041it [1:09:28,  1.45it/s]\u001b[A\n",
      "6042it [1:09:29,  1.45it/s]\u001b[A\n",
      "6043it [1:09:30,  1.45it/s]\u001b[A\n",
      "6044it [1:09:30,  1.45it/s]\u001b[A\n",
      "6045it [1:09:31,  1.45it/s]\u001b[A\n",
      "6046it [1:09:32,  1.45it/s]\u001b[A\n",
      "6047it [1:09:32,  1.45it/s]\u001b[A\n",
      "6048it [1:09:33,  1.45it/s]\u001b[A\n",
      "6049it [1:09:34,  1.45it/s]\u001b[A\n",
      "6050it [1:09:34,  1.45it/s]\u001b[A\n",
      "6051it [1:09:35,  1.45it/s]\u001b[A\n",
      "6052it [1:09:36,  1.45it/s]\u001b[A\n",
      "6053it [1:09:36,  1.45it/s]\u001b[A\n",
      "6054it [1:09:37,  1.45it/s]\u001b[A\n",
      "6055it [1:09:38,  1.45it/s]\u001b[A\n",
      "6056it [1:09:38,  1.45it/s]\u001b[A\n",
      "6057it [1:09:39,  1.45it/s]\u001b[A\n",
      "6058it [1:09:40,  1.45it/s]\u001b[A\n",
      "6059it [1:09:41,  1.45it/s]\u001b[A\n",
      "6060it [1:09:41,  1.45it/s]\u001b[A\n",
      "6061it [1:09:42,  1.45it/s]\u001b[A\n",
      "6062it [1:09:43,  1.45it/s]\u001b[A\n",
      "6063it [1:09:43,  1.45it/s]\u001b[A\n",
      "6064it [1:09:44,  1.45it/s]\u001b[A\n",
      "6065it [1:09:45,  1.45it/s]\u001b[A\n",
      "6066it [1:09:45,  1.45it/s]\u001b[A\n",
      "6067it [1:09:46,  1.45it/s]\u001b[A\n",
      "6068it [1:09:47,  1.45it/s]\u001b[A\n",
      "6069it [1:09:47,  1.45it/s]\u001b[A\n",
      "6070it [1:09:48,  1.45it/s]\u001b[A\n",
      "6071it [1:09:49,  1.45it/s]\u001b[A\n",
      "6072it [1:09:49,  1.45it/s]\u001b[A\n",
      "6073it [1:09:50,  1.45it/s]\u001b[A\n",
      "6074it [1:09:51,  1.45it/s]\u001b[A\n",
      "6075it [1:09:52,  1.45it/s]\u001b[A\n",
      "6076it [1:09:52,  1.45it/s]\u001b[A\n",
      "6077it [1:09:53,  1.45it/s]\u001b[A\n",
      "6078it [1:09:54,  1.45it/s]\u001b[A\n",
      "6079it [1:09:54,  1.45it/s]\u001b[A\n",
      "6080it [1:09:55,  1.45it/s]\u001b[A\n",
      "6081it [1:09:56,  1.45it/s]\u001b[A\n",
      "6082it [1:09:56,  1.45it/s]\u001b[A\n",
      "6083it [1:09:57,  1.45it/s]\u001b[A\n",
      "6084it [1:09:58,  1.45it/s]\u001b[A\n",
      "6085it [1:09:58,  1.45it/s]\u001b[A\n",
      "6086it [1:09:59,  1.45it/s]\u001b[A\n",
      "6087it [1:10:00,  1.45it/s]\u001b[A\n",
      "6088it [1:10:00,  1.45it/s]\u001b[A\n",
      "6089it [1:10:01,  1.45it/s]\u001b[A\n",
      "6090it [1:10:02,  1.45it/s]\u001b[A\n",
      "6091it [1:10:03,  1.45it/s]\u001b[A\n",
      "6092it [1:10:03,  1.45it/s]\u001b[A\n",
      "6093it [1:10:04,  1.45it/s]\u001b[A\n",
      "6094it [1:10:05,  1.45it/s]\u001b[A\n",
      "6095it [1:10:05,  1.45it/s]\u001b[A\n",
      "6096it [1:10:06,  1.45it/s]\u001b[A\n",
      "6097it [1:10:07,  1.45it/s]\u001b[A\n",
      "6098it [1:10:07,  1.45it/s]\u001b[A\n",
      "6099it [1:10:08,  1.45it/s]\u001b[A\n",
      "6100it [1:10:09,  1.45it/s]\u001b[A\n",
      "6101it [1:10:09,  1.45it/s]\u001b[A\n",
      "6102it [1:10:10,  1.45it/s]\u001b[A\n",
      "6103it [1:10:11,  1.45it/s]\u001b[A\n",
      "6104it [1:10:12,  1.45it/s]\u001b[A\n",
      "6105it [1:10:12,  1.45it/s]\u001b[A\n",
      "6106it [1:10:13,  1.45it/s]\u001b[A\n",
      "6107it [1:10:14,  1.45it/s]\u001b[A\n",
      "6108it [1:10:14,  1.45it/s]\u001b[A\n",
      "6109it [1:10:15,  1.45it/s]\u001b[A\n",
      "6110it [1:10:16,  1.45it/s]\u001b[A\n",
      "6111it [1:10:16,  1.45it/s]\u001b[A\n",
      "6112it [1:10:17,  1.45it/s]\u001b[A\n",
      "6113it [1:10:18,  1.45it/s]\u001b[A\n",
      "6114it [1:10:18,  1.45it/s]\u001b[A\n",
      "6115it [1:10:19,  1.45it/s]\u001b[A\n",
      "6116it [1:10:20,  1.45it/s]\u001b[A\n",
      "6117it [1:10:20,  1.45it/s]\u001b[A\n",
      "6118it [1:10:21,  1.45it/s]\u001b[A\n",
      "6119it [1:10:22,  1.45it/s]\u001b[A\n",
      "6120it [1:10:23,  1.45it/s]\u001b[A\n",
      "6121it [1:10:23,  1.45it/s]\u001b[A\n",
      "6122it [1:10:24,  1.45it/s]\u001b[A\n",
      "6123it [1:10:25,  1.45it/s]\u001b[A\n",
      "6124it [1:10:25,  1.45it/s]\u001b[A\n",
      "6125it [1:10:26,  1.45it/s]\u001b[A\n",
      "6126it [1:10:27,  1.45it/s]\u001b[A\n",
      "6127it [1:10:27,  1.45it/s]\u001b[A\n",
      "6128it [1:10:28,  1.45it/s]\u001b[A\n",
      "6129it [1:10:29,  1.45it/s]\u001b[A\n",
      "6130it [1:10:29,  1.45it/s]\u001b[A\n",
      "6131it [1:10:30,  1.45it/s]\u001b[A\n",
      "6132it [1:10:31,  1.45it/s]\u001b[A\n",
      "6133it [1:10:31,  1.45it/s]\u001b[A\n",
      "6134it [1:10:32,  1.45it/s]\u001b[A\n",
      "6135it [1:10:33,  1.45it/s]\u001b[A\n",
      "6136it [1:10:34,  1.45it/s]\u001b[A\n",
      "6137it [1:10:34,  1.45it/s]\u001b[A\n",
      "6138it [1:10:35,  1.45it/s]\u001b[A\n",
      "6139it [1:10:36,  1.45it/s]\u001b[A\n",
      "6140it [1:10:36,  1.45it/s]\u001b[A\n",
      "6141it [1:10:37,  1.45it/s]\u001b[A\n",
      "6142it [1:10:38,  1.45it/s]\u001b[A\n",
      "6143it [1:10:38,  1.45it/s]\u001b[A\n",
      "6144it [1:10:39,  1.45it/s]\u001b[A\n",
      "6145it [1:10:40,  1.45it/s]\u001b[A\n",
      "6146it [1:10:40,  1.45it/s]\u001b[A\n",
      "6147it [1:10:41,  1.45it/s]\u001b[A\n",
      "6148it [1:10:42,  1.45it/s]\u001b[A\n",
      "6149it [1:10:43,  1.45it/s]\u001b[A\n",
      "6150it [1:10:43,  1.45it/s]\u001b[A\n",
      "6151it [1:10:44,  1.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6152it [1:10:45,  1.45it/s]\u001b[A\n",
      "6153it [1:10:45,  1.45it/s]\u001b[A\n",
      "6154it [1:10:46,  1.45it/s]\u001b[A\n",
      "6155it [1:10:47,  1.45it/s]\u001b[A\n",
      "6156it [1:10:47,  1.45it/s]\u001b[A\n",
      "6157it [1:10:48,  1.45it/s]\u001b[A\n",
      "6158it [1:10:49,  1.45it/s]\u001b[A\n",
      "6159it [1:10:49,  1.45it/s]\u001b[A\n",
      "6160it [1:10:50,  1.45it/s]\u001b[A\n",
      "6161it [1:10:51,  1.45it/s]\u001b[A\n",
      "6162it [1:10:51,  1.45it/s]\u001b[A\n",
      "6163it [1:10:52,  1.45it/s]\u001b[A\n",
      "6164it [1:10:53,  1.45it/s]\u001b[A\n",
      "6165it [1:10:54,  1.45it/s]\u001b[A\n",
      "6166it [1:10:54,  1.45it/s]\u001b[A\n",
      "6167it [1:10:55,  1.45it/s]\u001b[A\n",
      "6168it [1:10:56,  1.45it/s]\u001b[A\n",
      "6169it [1:10:56,  1.45it/s]\u001b[A\n",
      "6170it [1:10:57,  1.45it/s]\u001b[A\n",
      "6171it [1:10:58,  1.45it/s]\u001b[A\n",
      "6172it [1:10:58,  1.45it/s]\u001b[A\n",
      "6173it [1:10:59,  1.45it/s]\u001b[A\n",
      "6174it [1:11:00,  1.45it/s]\u001b[A\n",
      "6175it [1:11:00,  1.45it/s]\u001b[A\n",
      "6176it [1:11:01,  1.45it/s]\u001b[A\n",
      "6177it [1:11:02,  1.45it/s]\u001b[A\n",
      "6178it [1:11:02,  1.45it/s]\u001b[A\n",
      "6179it [1:11:03,  1.45it/s]\u001b[A\n",
      "6180it [1:11:04,  1.45it/s]\u001b[A\n",
      "6181it [1:11:05,  1.45it/s]\u001b[A\n",
      "6182it [1:11:05,  1.45it/s]\u001b[A\n",
      "6183it [1:11:06,  1.45it/s]\u001b[A\n",
      "6184it [1:11:07,  1.45it/s]\u001b[A\n",
      "6185it [1:11:07,  1.45it/s]\u001b[A\n",
      "6186it [1:11:08,  1.45it/s]\u001b[A\n",
      "6187it [1:11:09,  1.45it/s]\u001b[A\n",
      "6188it [1:11:09,  1.45it/s]\u001b[A\n",
      "6189it [1:11:10,  1.45it/s]\u001b[A\n",
      "6190it [1:11:11,  1.45it/s]\u001b[A\n",
      "6191it [1:11:11,  1.45it/s]\u001b[A\n",
      "6192it [1:11:12,  1.45it/s]\u001b[A\n",
      "6193it [1:11:13,  1.45it/s]\u001b[A\n",
      "6194it [1:11:13,  1.45it/s]\u001b[A\n",
      "6195it [1:11:14,  1.45it/s]\u001b[A\n",
      "6196it [1:11:15,  1.45it/s]\u001b[A\n",
      "6197it [1:11:16,  1.45it/s]\u001b[A\n",
      "6198it [1:11:16,  1.45it/s]\u001b[A\n",
      "6199it [1:11:17,  1.45it/s]\u001b[A\n",
      "6200it [1:11:18,  1.45it/s]\u001b[A\n",
      "6201it [1:11:18,  1.45it/s]\u001b[A\n",
      "6202it [1:11:19,  1.45it/s]\u001b[A\n",
      "6203it [1:11:20,  1.45it/s]\u001b[A\n",
      "6204it [1:11:20,  1.45it/s]\u001b[A\n",
      "6205it [1:11:21,  1.45it/s]\u001b[A\n",
      "6206it [1:11:22,  1.45it/s]\u001b[A\n",
      "6207it [1:11:22,  1.45it/s]\u001b[A\n",
      "6208it [1:11:23,  1.45it/s]\u001b[A\n",
      "6209it [1:11:24,  1.45it/s]\u001b[A\n",
      "6210it [1:11:25,  1.45it/s]\u001b[A\n",
      "6211it [1:11:25,  1.45it/s]\u001b[A\n",
      "6212it [1:11:26,  1.45it/s]\u001b[A\n",
      "6213it [1:11:27,  1.45it/s]\u001b[A\n",
      "6214it [1:11:27,  1.45it/s]\u001b[A\n",
      "6215it [1:11:28,  1.45it/s]\u001b[A\n",
      "6216it [1:11:29,  1.45it/s]\u001b[A\n",
      "6217it [1:11:29,  1.45it/s]\u001b[A\n",
      "6218it [1:11:30,  1.45it/s]\u001b[A\n",
      "6219it [1:11:31,  1.45it/s]\u001b[A\n",
      "6220it [1:11:31,  1.45it/s]\u001b[A\n",
      "6221it [1:11:32,  1.45it/s]\u001b[A\n",
      "6222it [1:11:33,  1.45it/s]\u001b[A\n",
      "6223it [1:11:33,  1.45it/s]\u001b[A\n",
      "6224it [1:11:34,  1.45it/s]\u001b[A\n",
      "6225it [1:11:35,  1.45it/s]\u001b[A\n",
      "6226it [1:11:36,  1.45it/s]\u001b[A\n",
      "6227it [1:11:36,  1.45it/s]\u001b[A\n",
      "6228it [1:11:37,  1.45it/s]\u001b[A\n",
      "6229it [1:11:38,  1.45it/s]\u001b[A\n",
      "6230it [1:11:38,  1.45it/s]\u001b[A\n",
      "6231it [1:11:39,  1.45it/s]\u001b[A\n",
      "6232it [1:11:40,  1.45it/s]\u001b[A\n",
      "6233it [1:11:40,  1.45it/s]\u001b[A\n",
      "6234it [1:11:41,  1.45it/s]\u001b[A\n",
      "6235it [1:11:42,  1.45it/s]\u001b[A\n",
      "6236it [1:11:42,  1.45it/s]\u001b[A\n",
      "6237it [1:11:43,  1.45it/s]\u001b[A\n",
      "6238it [1:11:44,  1.45it/s]\u001b[A\n",
      "6239it [1:11:44,  1.45it/s]\u001b[A\n",
      "6240it [1:11:45,  1.45it/s]\u001b[A\n",
      "6241it [1:11:46,  1.45it/s]\u001b[A\n",
      "6242it [1:11:47,  1.45it/s]\u001b[A\n",
      "6243it [1:11:47,  1.45it/s]\u001b[A\n",
      "6244it [1:11:48,  1.45it/s]\u001b[A\n",
      "6245it [1:11:49,  1.45it/s]\u001b[A\n",
      "6246it [1:11:49,  1.45it/s]\u001b[A\n",
      "6247it [1:11:50,  1.45it/s]\u001b[A\n",
      "6248it [1:11:51,  1.45it/s]\u001b[A\n",
      "6249it [1:11:51,  1.45it/s]\u001b[A\n",
      "6250it [1:11:52,  1.45it/s]\u001b[A\n",
      "6251it [1:11:53,  1.45it/s]\u001b[A\n",
      "6252it [1:11:53,  1.45it/s]\u001b[A\n",
      "6253it [1:11:54,  1.45it/s]\u001b[A\n",
      "6254it [1:11:55,  1.45it/s]\u001b[A\n",
      "6255it [1:11:55,  1.45it/s]\u001b[A\n",
      "6256it [1:11:56,  1.45it/s]\u001b[A\n",
      "6257it [1:11:57,  1.45it/s]\u001b[A\n",
      "6258it [1:11:58,  1.45it/s]\u001b[A\n",
      "6259it [1:11:58,  1.45it/s]\u001b[A\n",
      "6260it [1:11:59,  1.45it/s]\u001b[A\n",
      "6261it [1:12:00,  1.45it/s]\u001b[A\n",
      "6262it [1:12:00,  1.45it/s]\u001b[A\n",
      "6263it [1:12:01,  1.45it/s]\u001b[A\n",
      "6264it [1:12:02,  1.45it/s]\u001b[A\n",
      "6265it [1:12:02,  1.45it/s]\u001b[A\n",
      "6266it [1:12:03,  1.45it/s]\u001b[A\n",
      "6267it [1:12:04,  1.45it/s]\u001b[A\n",
      "6268it [1:12:04,  1.45it/s]\u001b[A\n",
      "6269it [1:12:05,  1.45it/s]\u001b[A\n",
      "6270it [1:12:06,  1.45it/s]\u001b[A\n",
      "6271it [1:12:07,  1.45it/s]\u001b[A\n",
      "6272it [1:12:07,  1.45it/s]\u001b[A\n",
      "6273it [1:12:08,  1.45it/s]\u001b[A\n",
      "6274it [1:12:09,  1.45it/s]\u001b[A\n",
      "6275it [1:12:09,  1.45it/s]\u001b[A\n",
      "6276it [1:12:10,  1.45it/s]\u001b[A\n",
      "6277it [1:12:11,  1.45it/s]\u001b[A\n",
      "6278it [1:12:11,  1.45it/s]\u001b[A\n",
      "6279it [1:12:12,  1.45it/s]\u001b[A\n",
      "6280it [1:12:13,  1.45it/s]\u001b[A\n",
      "6281it [1:12:13,  1.45it/s]\u001b[A\n",
      "6282it [1:12:14,  1.45it/s]\u001b[A\n",
      "6283it [1:12:15,  1.45it/s]\u001b[A\n",
      "6284it [1:12:15,  1.45it/s]\u001b[A\n",
      "6285it [1:12:16,  1.45it/s]\u001b[A\n",
      "6286it [1:12:17,  1.45it/s]\u001b[A\n",
      "6287it [1:12:18,  1.45it/s]\u001b[A\n",
      "6288it [1:12:18,  1.45it/s]\u001b[A\n",
      "6289it [1:12:19,  1.45it/s]\u001b[A\n",
      "6290it [1:12:20,  1.45it/s]\u001b[A\n",
      "6291it [1:12:20,  1.45it/s]\u001b[A\n",
      "6292it [1:12:21,  1.45it/s]\u001b[A\n",
      "6293it [1:12:22,  1.45it/s]\u001b[A\n",
      "6294it [1:12:22,  1.45it/s]\u001b[A\n",
      "6295it [1:12:23,  1.45it/s]\u001b[A\n",
      "6296it [1:12:24,  1.45it/s]\u001b[A\n",
      "6297it [1:12:24,  1.45it/s]\u001b[A\n",
      "6298it [1:12:25,  1.45it/s]\u001b[A\n",
      "6299it [1:12:26,  1.45it/s]\u001b[A\n",
      "6300it [1:12:26,  1.45it/s]\u001b[A\n",
      "6301it [1:12:27,  1.45it/s]\u001b[A\n",
      "6302it [1:12:28,  1.45it/s]\u001b[A\n",
      "6303it [1:12:29,  1.45it/s]\u001b[A\n",
      "6304it [1:12:29,  1.45it/s]\u001b[A\n",
      "6305it [1:12:30,  1.45it/s]\u001b[A\n",
      "6306it [1:12:31,  1.45it/s]\u001b[A\n",
      "6307it [1:12:31,  1.45it/s]\u001b[A\n",
      "6308it [1:12:32,  1.45it/s]\u001b[A\n",
      "6309it [1:12:33,  1.45it/s]\u001b[A\n",
      "6310it [1:12:33,  1.45it/s]\u001b[A\n",
      "6311it [1:12:34,  1.45it/s]\u001b[A\n",
      "6312it [1:12:35,  1.45it/s]\u001b[A\n",
      "6313it [1:12:35,  1.45it/s]\u001b[A\n",
      "6314it [1:12:36,  1.45it/s]\u001b[A\n",
      "6315it [1:12:37,  1.45it/s]\u001b[A\n",
      "6316it [1:12:37,  1.45it/s]\u001b[A\n",
      "6317it [1:12:38,  1.45it/s]\u001b[A\n",
      "6318it [1:12:39,  1.45it/s]\u001b[A\n",
      "6319it [1:12:40,  1.45it/s]\u001b[A\n",
      "6320it [1:12:40,  1.45it/s]\u001b[A\n",
      "6321it [1:12:41,  1.45it/s]\u001b[A\n",
      "6322it [1:12:42,  1.45it/s]\u001b[A\n",
      "6323it [1:12:42,  1.45it/s]\u001b[A\n",
      "6324it [1:12:43,  1.45it/s]\u001b[A\n",
      "6325it [1:12:44,  1.45it/s]\u001b[A\n",
      "6326it [1:12:44,  1.45it/s]\u001b[A\n",
      "6327it [1:12:45,  1.45it/s]\u001b[A\n",
      "6328it [1:12:46,  1.45it/s]\u001b[A\n",
      "6329it [1:12:46,  1.45it/s]\u001b[A\n",
      "6330it [1:12:47,  1.45it/s]\u001b[A\n",
      "6331it [1:12:48,  1.45it/s]\u001b[A\n",
      "6332it [1:12:49,  1.45it/s]\u001b[A\n",
      "6333it [1:12:49,  1.45it/s]\u001b[A\n",
      "6334it [1:12:50,  1.45it/s]\u001b[A\n",
      "6335it [1:12:51,  1.45it/s]\u001b[A\n",
      "6336it [1:12:51,  1.45it/s]\u001b[A\n",
      "6337it [1:12:52,  1.45it/s]\u001b[A\n",
      "6338it [1:12:53,  1.45it/s]\u001b[A\n",
      "6339it [1:12:53,  1.45it/s]\u001b[A\n",
      "6340it [1:12:54,  1.45it/s]\u001b[A\n",
      "6341it [1:12:55,  1.45it/s]\u001b[A\n",
      "6342it [1:12:55,  1.45it/s]\u001b[A\n",
      "6343it [1:12:56,  1.45it/s]\u001b[A\n",
      "6344it [1:12:57,  1.45it/s]\u001b[A\n",
      "6345it [1:12:57,  1.45it/s]\u001b[A\n",
      "6346it [1:12:58,  1.45it/s]\u001b[A\n",
      "6347it [1:12:59,  1.45it/s]\u001b[A\n",
      "6348it [1:13:00,  1.45it/s]\u001b[A\n",
      "6349it [1:13:00,  1.45it/s]\u001b[A\n",
      "6350it [1:13:01,  1.45it/s]\u001b[A\n",
      "6351it [1:13:02,  1.45it/s]\u001b[A\n",
      "6352it [1:13:02,  1.45it/s]\u001b[A\n",
      "6353it [1:13:03,  1.45it/s]\u001b[A\n",
      "6354it [1:13:04,  1.45it/s]\u001b[A\n",
      "6355it [1:13:04,  1.45it/s]\u001b[A\n",
      "6356it [1:13:05,  1.45it/s]\u001b[A\n",
      "6357it [1:13:06,  1.45it/s]\u001b[A\n",
      "6358it [1:13:06,  1.45it/s]\u001b[A\n",
      "6359it [1:13:07,  1.45it/s]\u001b[A\n",
      "6360it [1:13:08,  1.45it/s]\u001b[A\n",
      "6361it [1:13:08,  1.45it/s]\u001b[A\n",
      "6362it [1:13:09,  1.45it/s]\u001b[A\n",
      "6363it [1:13:10,  1.45it/s]\u001b[A\n",
      "6364it [1:13:11,  1.45it/s]\u001b[A\n",
      "6365it [1:13:11,  1.45it/s]\u001b[A\n",
      "6366it [1:13:12,  1.45it/s]\u001b[A\n",
      "6367it [1:13:13,  1.45it/s]\u001b[A\n",
      "6368it [1:13:13,  1.45it/s]\u001b[A\n",
      "6369it [1:13:14,  1.45it/s]\u001b[A\n",
      "6370it [1:13:15,  1.45it/s]\u001b[A\n",
      "6371it [1:13:15,  1.45it/s]\u001b[A\n",
      "6372it [1:13:16,  1.45it/s]\u001b[A\n",
      "6373it [1:13:17,  1.45it/s]\u001b[A\n",
      "6374it [1:13:17,  1.45it/s]\u001b[A\n",
      "6375it [1:13:18,  1.45it/s]\u001b[A\n",
      "6376it [1:13:19,  1.45it/s]\u001b[A\n",
      "6377it [1:13:19,  1.45it/s]\u001b[A\n",
      "6378it [1:13:20,  1.45it/s]\u001b[A\n",
      "6379it [1:13:21,  1.45it/s]\u001b[A\n",
      "6380it [1:13:22,  1.45it/s]\u001b[A\n",
      "6381it [1:13:22,  1.45it/s]\u001b[A\n",
      "6382it [1:13:23,  1.45it/s]\u001b[A\n",
      "6383it [1:13:24,  1.45it/s]\u001b[A\n",
      "6384it [1:13:24,  1.45it/s]\u001b[A\n",
      "6385it [1:13:25,  1.45it/s]\u001b[A\n",
      "6386it [1:13:26,  1.45it/s]\u001b[A\n",
      "6387it [1:13:26,  1.45it/s]\u001b[A\n",
      "6388it [1:13:27,  1.45it/s]\u001b[A\n",
      "6389it [1:13:28,  1.45it/s]\u001b[A\n",
      "6390it [1:13:28,  1.45it/s]\u001b[A\n",
      "6391it [1:13:29,  1.45it/s]\u001b[A\n",
      "6392it [1:13:30,  1.45it/s]\u001b[A\n",
      "6393it [1:13:31,  1.45it/s]\u001b[A\n",
      "6394it [1:13:31,  1.45it/s]\u001b[A\n",
      "6395it [1:13:32,  1.45it/s]\u001b[A\n",
      "6396it [1:13:33,  1.45it/s]\u001b[A\n",
      "6397it [1:13:33,  1.45it/s]\u001b[A\n",
      "6398it [1:13:34,  1.45it/s]\u001b[A\n",
      "6399it [1:13:35,  1.45it/s]\u001b[A\n",
      "6400it [1:13:35,  1.45it/s]\u001b[A\n",
      "6401it [1:13:36,  1.45it/s]\u001b[A\n",
      "6402it [1:13:37,  1.45it/s]\u001b[A\n",
      "6403it [1:13:37,  1.45it/s]\u001b[A\n",
      "6404it [1:13:38,  1.45it/s]\u001b[A\n",
      "6405it [1:13:39,  1.45it/s]\u001b[A\n",
      "6406it [1:13:39,  1.45it/s]\u001b[A\n",
      "6407it [1:13:40,  1.45it/s]\u001b[A\n",
      "6408it [1:13:41,  1.45it/s]\u001b[A\n",
      "6409it [1:13:42,  1.45it/s]\u001b[A\n",
      "6410it [1:13:42,  1.45it/s]\u001b[A\n",
      "6411it [1:13:43,  1.45it/s]\u001b[A\n",
      "6412it [1:13:44,  1.45it/s]\u001b[A\n",
      "6413it [1:13:44,  1.45it/s]\u001b[A\n",
      "6414it [1:13:45,  1.45it/s]\u001b[A\n",
      "6415it [1:13:46,  1.45it/s]\u001b[A\n",
      "6416it [1:13:46,  1.45it/s]\u001b[A\n",
      "6417it [1:13:47,  1.45it/s]\u001b[A\n",
      "6418it [1:13:48,  1.45it/s]\u001b[A\n",
      "6419it [1:13:48,  1.45it/s]\u001b[A\n",
      "6420it [1:13:49,  1.45it/s]\u001b[A\n",
      "6421it [1:13:50,  1.45it/s]\u001b[A\n",
      "6422it [1:13:50,  1.45it/s]\u001b[A\n",
      "6423it [1:13:51,  1.45it/s]\u001b[A\n",
      "6424it [1:13:52,  1.45it/s]\u001b[A\n",
      "6425it [1:13:53,  1.45it/s]\u001b[A\n",
      "6426it [1:13:53,  1.45it/s]\u001b[A\n",
      "6427it [1:13:54,  1.45it/s]\u001b[A\n",
      "6428it [1:13:55,  1.45it/s]\u001b[A\n",
      "6429it [1:13:55,  1.45it/s]\u001b[A\n",
      "6430it [1:13:56,  1.45it/s]\u001b[A\n",
      "6431it [1:13:57,  1.45it/s]\u001b[A\n",
      "6432it [1:13:57,  1.45it/s]\u001b[A\n",
      "6433it [1:13:58,  1.45it/s]\u001b[A\n",
      "6434it [1:13:59,  1.45it/s]\u001b[A\n",
      "6435it [1:13:59,  1.45it/s]\u001b[A\n",
      "6436it [1:14:00,  1.45it/s]\u001b[A\n",
      "6437it [1:14:01,  1.45it/s]\u001b[A\n",
      "6438it [1:14:01,  1.45it/s]\u001b[A\n",
      "6439it [1:14:02,  1.45it/s]\u001b[A\n",
      "6440it [1:14:03,  1.45it/s]\u001b[A\n",
      "6441it [1:14:04,  1.45it/s]\u001b[A\n",
      "6442it [1:14:04,  1.45it/s]\u001b[A\n",
      "6443it [1:14:05,  1.45it/s]\u001b[A\n",
      "6444it [1:14:06,  1.45it/s]\u001b[A\n",
      "6445it [1:14:06,  1.45it/s]\u001b[A\n",
      "6446it [1:14:07,  1.45it/s]\u001b[A\n",
      "6447it [1:14:08,  1.45it/s]\u001b[A\n",
      "6448it [1:14:08,  1.45it/s]\u001b[A\n",
      "6449it [1:14:09,  1.45it/s]\u001b[A\n",
      "6450it [1:14:10,  1.45it/s]\u001b[A\n",
      "6451it [1:14:10,  1.45it/s]\u001b[A\n",
      "6452it [1:14:11,  1.45it/s]\u001b[A\n",
      "6453it [1:14:12,  1.45it/s]\u001b[A\n",
      "6454it [1:14:13,  1.45it/s]\u001b[A\n",
      "6455it [1:14:13,  1.45it/s]\u001b[A\n",
      "6456it [1:14:14,  1.45it/s]\u001b[A\n",
      "6457it [1:14:15,  1.45it/s]\u001b[A\n",
      "6458it [1:14:15,  1.45it/s]\u001b[A\n",
      "6459it [1:14:16,  1.45it/s]\u001b[A\n",
      "6460it [1:14:17,  1.45it/s]\u001b[A\n",
      "6461it [1:14:17,  1.45it/s]\u001b[A\n",
      "6462it [1:14:18,  1.45it/s]\u001b[A\n",
      "6463it [1:14:19,  1.45it/s]\u001b[A\n",
      "6464it [1:14:19,  1.45it/s]\u001b[A\n",
      "6465it [1:14:20,  1.45it/s]\u001b[A\n",
      "6466it [1:14:21,  1.45it/s]\u001b[A\n",
      "6467it [1:14:21,  1.45it/s]\u001b[A\n",
      "6468it [1:14:22,  1.45it/s]\u001b[A\n",
      "6469it [1:14:23,  1.45it/s]\u001b[A\n",
      "6470it [1:14:24,  1.46it/s]\u001b[A\n",
      "6471it [1:14:24,  1.45it/s]\u001b[A\n",
      "6472it [1:14:25,  1.45it/s]\u001b[A\n",
      "6473it [1:14:26,  1.45it/s]\u001b[A\n",
      "6474it [1:14:26,  1.45it/s]\u001b[A\n",
      "6475it [1:14:27,  1.45it/s]\u001b[A\n",
      "6476it [1:14:28,  1.45it/s]\u001b[A\n",
      "6477it [1:14:28,  1.45it/s]\u001b[A\n",
      "6478it [1:14:29,  1.45it/s]\u001b[A\n",
      "6479it [1:14:30,  1.45it/s]\u001b[A\n",
      "6480it [1:14:30,  1.45it/s]\u001b[A\n",
      "6481it [1:14:31,  1.45it/s]\u001b[A\n",
      "6482it [1:14:32,  1.45it/s]\u001b[A\n",
      "6483it [1:14:32,  1.45it/s]\u001b[A\n",
      "6484it [1:14:33,  1.45it/s]\u001b[A\n",
      "6485it [1:14:34,  1.45it/s]\u001b[A\n",
      "6486it [1:14:35,  1.45it/s]\u001b[A\n",
      "6487it [1:14:35,  1.45it/s]\u001b[A\n",
      "6488it [1:14:36,  1.45it/s]\u001b[A\n",
      "6489it [1:14:37,  1.45it/s]\u001b[A\n",
      "6490it [1:14:37,  1.45it/s]\u001b[A\n",
      "6491it [1:14:38,  1.45it/s]\u001b[A\n",
      "6492it [1:14:39,  1.45it/s]\u001b[A\n",
      "6493it [1:14:39,  1.45it/s]\u001b[A\n",
      "6494it [1:14:40,  1.45it/s]\u001b[A\n",
      "6495it [1:14:41,  1.45it/s]\u001b[A\n",
      "6496it [1:14:41,  1.45it/s]\u001b[A\n",
      "6497it [1:14:42,  1.45it/s]\u001b[A\n",
      "6498it [1:14:43,  1.45it/s]\u001b[A\n",
      "6499it [1:14:43,  1.45it/s]\u001b[A\n",
      "6500it [1:14:44,  1.45it/s]\u001b[A\n",
      "6501it [1:14:45,  1.45it/s]\u001b[A\n",
      "6502it [1:14:46,  1.45it/s]\u001b[A\n",
      "6503it [1:14:46,  1.45it/s]\u001b[A\n",
      "6504it [1:14:47,  1.45it/s]\u001b[A\n",
      "6505it [1:14:48,  1.45it/s]\u001b[A\n",
      "6506it [1:14:48,  1.45it/s]\u001b[A\n",
      "6507it [1:14:49,  1.45it/s]\u001b[A\n",
      "6508it [1:14:50,  1.45it/s]\u001b[A\n",
      "6509it [1:14:50,  1.45it/s]\u001b[A\n",
      "6510it [1:14:51,  1.45it/s]\u001b[A\n",
      "6511it [1:14:52,  1.45it/s]\u001b[A\n",
      "6512it [1:14:52,  1.45it/s]\u001b[A\n",
      "6513it [1:14:53,  1.45it/s]\u001b[A\n",
      "6514it [1:14:54,  1.45it/s]\u001b[A\n",
      "6515it [1:14:55,  1.45it/s]\u001b[A\n",
      "6516it [1:14:55,  1.45it/s]\u001b[A\n",
      "6517it [1:14:56,  1.45it/s]\u001b[A\n",
      "6518it [1:14:57,  1.45it/s]\u001b[A\n",
      "6519it [1:14:57,  1.45it/s]\u001b[A\n",
      "6520it [1:14:58,  1.45it/s]\u001b[A\n",
      "6521it [1:14:59,  1.45it/s]\u001b[A\n",
      "6522it [1:14:59,  1.45it/s]\u001b[A\n",
      "6523it [1:15:00,  1.45it/s]\u001b[A\n",
      "6524it [1:15:01,  1.45it/s]\u001b[A\n",
      "6525it [1:15:01,  1.45it/s]\u001b[A\n",
      "6526it [1:15:02,  1.45it/s]\u001b[A\n",
      "6527it [1:15:03,  1.45it/s]\u001b[A\n",
      "6528it [1:15:03,  1.45it/s]\u001b[A\n",
      "6529it [1:15:04,  1.45it/s]\u001b[A\n",
      "6530it [1:15:05,  1.45it/s]\u001b[A\n",
      "6531it [1:15:06,  1.45it/s]\u001b[A\n",
      "6532it [1:15:06,  1.45it/s]\u001b[A\n",
      "6533it [1:15:07,  1.45it/s]\u001b[A\n",
      "6534it [1:15:08,  1.45it/s]\u001b[A\n",
      "6535it [1:15:08,  1.45it/s]\u001b[A\n",
      "6536it [1:15:09,  1.45it/s]\u001b[A\n",
      "6537it [1:15:10,  1.45it/s]\u001b[A\n",
      "6538it [1:15:10,  1.45it/s]\u001b[A\n",
      "6539it [1:15:11,  1.45it/s]\u001b[A\n",
      "6540it [1:15:12,  1.45it/s]\u001b[A\n",
      "6541it [1:15:12,  1.45it/s]\u001b[A\n",
      "6542it [1:15:13,  1.45it/s]\u001b[A\n",
      "6543it [1:15:14,  1.45it/s]\u001b[A\n",
      "6544it [1:15:14,  1.45it/s]\u001b[A\n",
      "6545it [1:15:15,  1.45it/s]\u001b[A\n",
      "6546it [1:15:16,  1.45it/s]\u001b[A\n",
      "6547it [1:15:17,  1.45it/s]\u001b[A\n",
      "6548it [1:15:17,  1.45it/s]\u001b[A\n",
      "6549it [1:15:18,  1.45it/s]\u001b[A\n",
      "6550it [1:15:19,  1.45it/s]\u001b[A\n",
      "6551it [1:15:19,  1.45it/s]\u001b[A\n",
      "6552it [1:15:20,  1.45it/s]\u001b[A\n",
      "6553it [1:15:21,  1.45it/s]\u001b[A\n",
      "6554it [1:15:21,  1.45it/s]\u001b[A\n",
      "6555it [1:15:22,  1.45it/s]\u001b[A\n",
      "6556it [1:15:23,  1.45it/s]\u001b[A\n",
      "6557it [1:15:23,  1.45it/s]\u001b[A\n",
      "6558it [1:15:24,  1.45it/s]\u001b[A\n",
      "6559it [1:15:25,  1.45it/s]\u001b[A\n",
      "6560it [1:15:26,  1.45it/s]\u001b[A\n",
      "6561it [1:15:26,  1.45it/s]\u001b[A\n",
      "6562it [1:15:27,  1.45it/s]\u001b[A\n",
      "6563it [1:15:28,  1.45it/s]\u001b[A\n",
      "6564it [1:15:28,  1.45it/s]\u001b[A\n",
      "6565it [1:15:29,  1.45it/s]\u001b[A\n",
      "6566it [1:15:30,  1.45it/s]\u001b[A\n",
      "6567it [1:15:30,  1.45it/s]\u001b[A\n",
      "6568it [1:15:31,  1.45it/s]\u001b[A\n",
      "6569it [1:15:32,  1.45it/s]\u001b[A\n",
      "6570it [1:15:32,  1.45it/s]\u001b[A\n",
      "6571it [1:15:33,  1.45it/s]\u001b[A\n",
      "6572it [1:15:34,  1.45it/s]\u001b[A\n",
      "6573it [1:15:34,  1.45it/s]\u001b[A\n",
      "6574it [1:15:35,  1.45it/s]\u001b[A\n",
      "6575it [1:15:36,  1.45it/s]\u001b[A\n",
      "6576it [1:15:37,  1.45it/s]\u001b[A\n",
      "6577it [1:15:37,  1.45it/s]\u001b[A\n",
      "6578it [1:15:38,  1.45it/s]\u001b[A\n",
      "6579it [1:15:39,  1.45it/s]\u001b[A\n",
      "6580it [1:15:39,  1.45it/s]\u001b[A\n",
      "6581it [1:15:40,  1.45it/s]\u001b[A\n",
      "6582it [1:15:41,  1.45it/s]\u001b[A\n",
      "6583it [1:15:41,  1.45it/s]\u001b[A\n",
      "6584it [1:15:42,  1.45it/s]\u001b[A\n",
      "6585it [1:15:43,  1.45it/s]\u001b[A\n",
      "6586it [1:15:43,  1.45it/s]\u001b[A\n",
      "6587it [1:15:44,  1.45it/s]\u001b[A\n",
      "6588it [1:15:45,  1.45it/s]\u001b[A\n",
      "6589it [1:15:45,  1.45it/s]\u001b[A\n",
      "6590it [1:15:46,  1.45it/s]\u001b[A\n",
      "6591it [1:15:47,  1.45it/s]\u001b[A\n",
      "6592it [1:15:48,  1.45it/s]\u001b[A\n",
      "6593it [1:15:48,  1.45it/s]\u001b[A\n",
      "6594it [1:15:49,  1.45it/s]\u001b[A\n",
      "6595it [1:15:50,  1.45it/s]\u001b[A\n",
      "6596it [1:15:50,  1.45it/s]\u001b[A\n",
      "6597it [1:15:51,  1.45it/s]\u001b[A\n",
      "6598it [1:15:52,  1.45it/s]\u001b[A\n",
      "6599it [1:15:52,  1.45it/s]\u001b[A\n",
      "6600it [1:15:53,  1.45it/s]\u001b[A\n",
      "6601it [1:15:54,  1.45it/s]\u001b[A\n",
      "6602it [1:15:54,  1.45it/s]\u001b[A\n",
      "6603it [1:15:55,  1.45it/s]\u001b[A\n",
      "6604it [1:15:56,  1.45it/s]\u001b[A\n",
      "6605it [1:15:56,  1.45it/s]\u001b[A\n",
      "6606it [1:15:57,  1.45it/s]\u001b[A\n",
      "6607it [1:15:58,  1.45it/s]\u001b[A\n",
      "6608it [1:15:59,  1.45it/s]\u001b[A\n",
      "6609it [1:15:59,  1.45it/s]\u001b[A\n",
      "6610it [1:16:00,  1.45it/s]\u001b[A\n",
      "6611it [1:16:01,  1.45it/s]\u001b[A\n",
      "6612it [1:16:01,  1.45it/s]\u001b[A\n",
      "6613it [1:16:02,  1.45it/s]\u001b[A\n",
      "6614it [1:16:03,  1.45it/s]\u001b[A\n",
      "6615it [1:16:03,  1.45it/s]\u001b[A\n",
      "6616it [1:16:04,  1.45it/s]\u001b[A\n",
      "6617it [1:16:05,  1.45it/s]\u001b[A\n",
      "6618it [1:16:05,  1.45it/s]\u001b[A\n",
      "6619it [1:16:06,  1.45it/s]\u001b[A\n",
      "6620it [1:16:07,  1.45it/s]\u001b[A\n",
      "6621it [1:16:08,  1.45it/s]\u001b[A\n",
      "6622it [1:16:08,  1.45it/s]\u001b[A\n",
      "6623it [1:16:09,  1.45it/s]\u001b[A\n",
      "6624it [1:16:10,  1.45it/s]\u001b[A\n",
      "6625it [1:16:10,  1.45it/s]\u001b[A\n",
      "6626it [1:16:11,  1.45it/s]\u001b[A\n",
      "6627it [1:16:12,  1.45it/s]\u001b[A\n",
      "6628it [1:16:12,  1.45it/s]\u001b[A\n",
      "6629it [1:16:13,  1.45it/s]\u001b[A\n",
      "6630it [1:16:14,  1.45it/s]\u001b[A\n",
      "6631it [1:16:14,  1.45it/s]\u001b[A\n",
      "6632it [1:16:15,  1.45it/s]\u001b[A\n",
      "6633it [1:16:16,  1.45it/s]\u001b[A\n",
      "6634it [1:16:16,  1.45it/s]\u001b[A\n",
      "6635it [1:16:17,  1.45it/s]\u001b[A\n",
      "6636it [1:16:18,  1.45it/s]\u001b[A\n",
      "6637it [1:16:19,  1.45it/s]\u001b[A\n",
      "6638it [1:16:19,  1.45it/s]\u001b[A\n",
      "6639it [1:16:20,  1.45it/s]\u001b[A\n",
      "6640it [1:16:21,  1.45it/s]\u001b[A\n",
      "6641it [1:16:21,  1.45it/s]\u001b[A\n",
      "6642it [1:16:22,  1.45it/s]\u001b[A\n",
      "6643it [1:16:23,  1.45it/s]\u001b[A\n",
      "6644it [1:16:23,  1.45it/s]\u001b[A\n",
      "6645it [1:16:24,  1.45it/s]\u001b[A\n",
      "6646it [1:16:25,  1.45it/s]\u001b[A\n",
      "6647it [1:16:25,  1.45it/s]\u001b[A\n",
      "6648it [1:16:26,  1.45it/s]\u001b[A\n",
      "6649it [1:16:27,  1.45it/s]\u001b[A\n",
      "6650it [1:16:27,  1.45it/s]\u001b[A\n",
      "6651it [1:16:28,  1.45it/s]\u001b[A\n",
      "6652it [1:16:29,  1.45it/s]\u001b[A\n",
      "6653it [1:16:30,  1.45it/s]\u001b[A\n",
      "6654it [1:16:30,  1.45it/s]\u001b[A\n",
      "6655it [1:16:31,  1.45it/s]\u001b[A\n",
      "6656it [1:16:32,  1.45it/s]\u001b[A\n",
      "6657it [1:16:32,  1.45it/s]\u001b[A\n",
      "6658it [1:16:33,  1.45it/s]\u001b[A\n",
      "6659it [1:16:34,  1.45it/s]\u001b[A\n",
      "6660it [1:16:34,  1.45it/s]\u001b[A\n",
      "6661it [1:16:35,  1.45it/s]\u001b[A\n",
      "6662it [1:16:36,  1.45it/s]\u001b[A\n",
      "6663it [1:16:36,  1.45it/s]\u001b[A\n",
      "6664it [1:16:37,  1.45it/s]\u001b[A\n",
      "6665it [1:16:38,  1.45it/s]\u001b[A\n",
      "6666it [1:16:39,  1.45it/s]\u001b[A\n",
      "6667it [1:16:39,  1.45it/s]\u001b[A\n",
      "6668it [1:16:40,  1.45it/s]\u001b[A\n",
      "6669it [1:16:41,  1.45it/s]\u001b[A\n",
      "6670it [1:16:41,  1.45it/s]\u001b[A\n",
      "6671it [1:16:42,  1.45it/s]\u001b[A\n",
      "6672it [1:16:43,  1.45it/s]\u001b[A\n",
      "6673it [1:16:43,  1.45it/s]\u001b[A\n",
      "6674it [1:16:44,  1.45it/s]\u001b[A\n",
      "6675it [1:16:45,  1.45it/s]\u001b[A\n",
      "6676it [1:16:45,  1.45it/s]\u001b[A\n",
      "6677it [1:16:46,  1.45it/s]\u001b[A\n",
      "6678it [1:16:47,  1.45it/s]\u001b[A\n",
      "6679it [1:16:47,  1.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6680it [1:16:48,  1.45it/s]\u001b[A\n",
      "6681it [1:16:49,  1.45it/s]\u001b[A\n",
      "6682it [1:16:50,  1.45it/s]\u001b[A\n",
      "6683it [1:16:50,  1.45it/s]\u001b[A\n",
      "6684it [1:16:51,  1.45it/s]\u001b[A\n",
      "6685it [1:16:52,  1.45it/s]\u001b[A\n",
      "6686it [1:16:52,  1.45it/s]\u001b[A\n",
      "6687it [1:16:53,  1.45it/s]\u001b[A\n",
      "6688it [1:16:54,  1.45it/s]\u001b[A\n",
      "6689it [1:16:54,  1.45it/s]\u001b[A\n",
      "6690it [1:16:55,  1.45it/s]\u001b[A\n",
      "6691it [1:16:56,  1.45it/s]\u001b[A\n",
      "6692it [1:16:56,  1.45it/s]\u001b[A\n",
      "6693it [1:16:57,  1.45it/s]\u001b[A\n",
      "6694it [1:16:58,  1.45it/s]\u001b[A\n",
      "6695it [1:16:58,  1.45it/s]\u001b[A\n",
      "6696it [1:16:59,  1.45it/s]\u001b[A\n",
      "6697it [1:17:00,  1.45it/s]\u001b[A\n",
      "6698it [1:17:01,  1.45it/s]\u001b[A\n",
      "6699it [1:17:01,  1.45it/s]\u001b[A\n",
      "6700it [1:17:02,  1.45it/s]\u001b[A\n",
      "6701it [1:17:03,  1.45it/s]\u001b[A\n",
      "6702it [1:17:03,  1.45it/s]\u001b[A\n",
      "6703it [1:17:04,  1.45it/s]\u001b[A\n",
      "6704it [1:17:05,  1.45it/s]\u001b[A\n",
      "6705it [1:17:05,  1.45it/s]\u001b[A\n",
      "6706it [1:17:06,  1.45it/s]\u001b[A\n",
      "6707it [1:17:07,  1.45it/s]\u001b[A\n",
      "6708it [1:17:07,  1.45it/s]\u001b[A\n",
      "6709it [1:17:08,  1.45it/s]\u001b[A\n",
      "6710it [1:17:09,  1.45it/s]\u001b[A\n",
      "6711it [1:17:09,  1.45it/s]\u001b[A\n",
      "6712it [1:17:10,  1.45it/s]\u001b[A\n",
      "6713it [1:17:11,  1.45it/s]\u001b[A\n",
      "6714it [1:17:12,  1.45it/s]\u001b[A\n",
      "6715it [1:17:12,  1.45it/s]\u001b[A\n",
      "6716it [1:17:13,  1.45it/s]\u001b[A\n",
      "6717it [1:17:14,  1.45it/s]\u001b[A\n",
      "6718it [1:17:14,  1.45it/s]\u001b[A\n",
      "6719it [1:17:15,  1.45it/s]\u001b[A\n",
      "6720it [1:17:16,  1.45it/s]\u001b[A\n",
      "6721it [1:17:16,  1.45it/s]\u001b[A\n",
      "6722it [1:17:17,  1.45it/s]\u001b[A\n",
      "6723it [1:17:18,  1.45it/s]\u001b[A\n",
      "6724it [1:17:18,  1.45it/s]\u001b[A\n",
      "6725it [1:17:19,  1.45it/s]\u001b[A\n",
      "6726it [1:17:20,  1.45it/s]\u001b[A\n",
      "6727it [1:17:21,  1.45it/s]\u001b[A\n",
      "6728it [1:17:21,  1.45it/s]\u001b[A\n",
      "6729it [1:17:22,  1.45it/s]\u001b[A\n",
      "6730it [1:17:23,  1.45it/s]\u001b[A\n",
      "6731it [1:17:23,  1.45it/s]\u001b[A\n",
      "6732it [1:17:24,  1.45it/s]\u001b[A\n",
      "6733it [1:17:25,  1.45it/s]\u001b[A\n",
      "6734it [1:17:25,  1.45it/s]\u001b[A\n",
      "6735it [1:17:26,  1.45it/s]\u001b[A\n",
      "6736it [1:17:27,  1.45it/s]\u001b[A\n",
      "6737it [1:17:27,  1.45it/s]\u001b[A\n",
      "6738it [1:17:28,  1.45it/s]\u001b[A\n",
      "6739it [1:17:29,  1.45it/s]\u001b[A\n",
      "6740it [1:17:29,  1.45it/s]\u001b[A\n",
      "6741it [1:17:30,  1.45it/s]\u001b[A\n",
      "6742it [1:17:31,  1.45it/s]\u001b[A\n",
      "6743it [1:17:32,  1.45it/s]\u001b[A\n",
      "6744it [1:17:32,  1.45it/s]\u001b[A\n",
      "6745it [1:17:33,  1.45it/s]\u001b[A\n",
      "6746it [1:17:34,  1.45it/s]\u001b[A\n",
      "6747it [1:17:34,  1.45it/s]\u001b[A\n",
      "6748it [1:17:35,  1.45it/s]\u001b[A\n",
      "6749it [1:17:36,  1.45it/s]\u001b[A\n",
      "6750it [1:17:36,  1.45it/s]\u001b[A\n",
      "6751it [1:17:37,  1.45it/s]\u001b[A\n",
      "6752it [1:17:38,  1.45it/s]\u001b[A\n",
      "6753it [1:17:38,  1.45it/s]\u001b[A\n",
      "6754it [1:17:39,  1.45it/s]\u001b[A\n",
      "6755it [1:17:40,  1.45it/s]\u001b[A\n",
      "6756it [1:17:40,  1.45it/s]\u001b[A\n",
      "6757it [1:17:41,  1.45it/s]\u001b[A\n",
      "6758it [1:17:42,  1.45it/s]\u001b[A\n",
      "6759it [1:17:43,  1.45it/s]\u001b[A\n",
      "6760it [1:17:43,  1.45it/s]\u001b[A\n",
      "6761it [1:17:44,  1.45it/s]\u001b[A\n",
      "6762it [1:17:45,  1.45it/s]\u001b[A\n",
      "6763it [1:17:45,  1.45it/s]\u001b[A\n",
      "6764it [1:17:46,  1.45it/s]\u001b[A\n",
      "6765it [1:17:47,  1.45it/s]\u001b[A\n",
      "6766it [1:17:47,  1.45it/s]\u001b[A\n",
      "6767it [1:17:48,  1.45it/s]\u001b[A\n",
      "6768it [1:17:49,  1.45it/s]\u001b[A\n",
      "6769it [1:17:49,  1.45it/s]\u001b[A\n",
      "6770it [1:17:50,  1.45it/s]\u001b[A\n",
      "6771it [1:17:51,  1.45it/s]\u001b[A\n",
      "6772it [1:17:51,  1.45it/s]\u001b[A\n",
      "6773it [1:17:52,  1.45it/s]\u001b[A\n",
      "6774it [1:17:53,  1.45it/s]\u001b[A\n",
      "6775it [1:17:54,  1.45it/s]\u001b[A\n",
      "6776it [1:17:54,  1.45it/s]\u001b[A\n",
      "6777it [1:17:55,  1.45it/s]\u001b[A\n",
      "6778it [1:17:56,  1.45it/s]\u001b[A\n",
      "6779it [1:17:56,  1.45it/s]\u001b[A\n",
      "6780it [1:17:57,  1.45it/s]\u001b[A\n",
      "6781it [1:17:58,  1.45it/s]\u001b[A\n",
      "6782it [1:17:58,  1.45it/s]\u001b[A\n",
      "6783it [1:17:59,  1.45it/s]\u001b[A\n",
      "6784it [1:18:00,  1.45it/s]\u001b[A\n",
      "6785it [1:18:00,  1.45it/s]\u001b[A\n",
      "6786it [1:18:01,  1.45it/s]\u001b[A\n",
      "6787it [1:18:02,  1.45it/s]\u001b[A\n",
      "6788it [1:18:03,  1.45it/s]\u001b[A\n",
      "6789it [1:18:03,  1.45it/s]\u001b[A\n",
      "6790it [1:18:04,  1.45it/s]\u001b[A\n",
      "6791it [1:18:05,  1.45it/s]\u001b[A\n",
      "6792it [1:18:05,  1.45it/s]\u001b[A\n",
      "6793it [1:18:06,  1.45it/s]\u001b[A\n",
      "6794it [1:18:07,  1.45it/s]\u001b[A\n",
      "6795it [1:18:07,  1.45it/s]\u001b[A\n",
      "6796it [1:18:08,  1.45it/s]\u001b[A\n",
      "6797it [1:18:09,  1.45it/s]\u001b[A\n",
      "6798it [1:18:09,  1.45it/s]\u001b[A\n",
      "6799it [1:18:10,  1.45it/s]\u001b[A\n",
      "6800it [1:18:11,  1.45it/s]\u001b[A\n",
      "6801it [1:18:11,  1.45it/s]\u001b[A\n",
      "6802it [1:18:12,  1.45it/s]\u001b[A\n",
      "6803it [1:18:13,  1.45it/s]\u001b[A\n",
      "6804it [1:18:14,  1.45it/s]\u001b[A\n",
      "6805it [1:18:14,  1.45it/s]\u001b[A\n",
      "6806it [1:18:15,  1.45it/s]\u001b[A\n",
      "6807it [1:18:16,  1.45it/s]\u001b[A\n",
      "6808it [1:18:16,  1.45it/s]\u001b[A\n",
      "6809it [1:18:17,  1.45it/s]\u001b[A\n",
      "6810it [1:18:18,  1.45it/s]\u001b[A\n",
      "6811it [1:18:18,  1.45it/s]\u001b[A\n",
      "6812it [1:18:19,  1.45it/s]\u001b[A\n",
      "6813it [1:18:20,  1.45it/s]\u001b[A\n",
      "6814it [1:18:20,  1.45it/s]\u001b[A\n",
      "6815it [1:18:21,  1.45it/s]\u001b[A\n",
      "6816it [1:18:22,  1.45it/s]\u001b[A\n",
      "6817it [1:18:22,  1.45it/s]\u001b[A\n",
      "6818it [1:18:23,  1.45it/s]\u001b[A\n",
      "6819it [1:18:24,  1.45it/s]\u001b[A\n",
      "6820it [1:18:25,  1.45it/s]\u001b[A\n",
      "6821it [1:18:25,  1.45it/s]\u001b[A\n",
      "6822it [1:18:26,  1.45it/s]\u001b[A\n",
      "6823it [1:18:27,  1.45it/s]\u001b[A\n",
      "6824it [1:18:27,  1.45it/s]\u001b[A\n",
      "6825it [1:18:28,  1.45it/s]\u001b[A\n",
      "6826it [1:18:29,  1.45it/s]\u001b[A\n",
      "6827it [1:18:29,  1.45it/s]\u001b[A\n",
      "6828it [1:18:30,  1.45it/s]\u001b[A\n",
      "6829it [1:18:31,  1.45it/s]\u001b[A\n",
      "6830it [1:18:31,  1.45it/s]\u001b[A\n",
      "6831it [1:18:32,  1.45it/s]\u001b[A\n",
      "6832it [1:18:33,  1.45it/s]\u001b[A\n",
      "6833it [1:18:33,  1.45it/s]\u001b[A\n",
      "6834it [1:18:34,  1.45it/s]\u001b[A\n",
      "6835it [1:18:35,  1.45it/s]\u001b[A\n",
      "6836it [1:18:36,  1.45it/s]\u001b[A\n",
      "6837it [1:18:36,  1.45it/s]\u001b[A\n",
      "6838it [1:18:37,  1.45it/s]\u001b[A\n",
      "6839it [1:18:38,  1.45it/s]\u001b[A\n",
      "6840it [1:18:38,  1.45it/s]\u001b[A\n",
      "6841it [1:18:39,  1.45it/s]\u001b[A\n",
      "6842it [1:18:40,  1.45it/s]\u001b[A\n",
      "6843it [1:18:40,  1.45it/s]\u001b[A\n",
      "6844it [1:18:41,  1.45it/s]\u001b[A\n",
      "6845it [1:18:42,  1.45it/s]\u001b[A\n",
      "6846it [1:18:42,  1.45it/s]\u001b[A\n",
      "6847it [1:18:43,  1.45it/s]\u001b[A\n",
      "6848it [1:18:44,  1.45it/s]\u001b[A\n",
      "6849it [1:18:45,  1.45it/s]\u001b[A\n",
      "6850it [1:18:45,  1.45it/s]\u001b[A\n",
      "6851it [1:18:46,  1.45it/s]\u001b[A\n",
      "6852it [1:18:47,  1.45it/s]\u001b[A\n",
      "6853it [1:18:47,  1.45it/s]\u001b[A\n",
      "6854it [1:18:48,  1.45it/s]\u001b[A\n",
      "6855it [1:18:49,  1.45it/s]\u001b[A\n",
      "6856it [1:18:49,  1.45it/s]\u001b[A\n",
      "6857it [1:18:50,  1.45it/s]\u001b[A\n",
      "6858it [1:18:51,  1.45it/s]\u001b[A\n",
      "6859it [1:18:51,  1.45it/s]\u001b[A\n",
      "6860it [1:18:52,  1.45it/s]\u001b[A\n",
      "6861it [1:18:53,  1.45it/s]\u001b[A\n",
      "6862it [1:18:53,  1.45it/s]\u001b[A\n",
      "6863it [1:18:54,  1.45it/s]\u001b[A\n",
      "6864it [1:18:55,  1.45it/s]\u001b[A\n",
      "6865it [1:18:56,  1.45it/s]\u001b[A\n",
      "6866it [1:18:56,  1.45it/s]\u001b[A\n",
      "6867it [1:18:57,  1.45it/s]\u001b[A\n",
      "6868it [1:18:58,  1.45it/s]\u001b[A\n",
      "6869it [1:18:58,  1.45it/s]\u001b[A\n",
      "6870it [1:18:59,  1.45it/s]\u001b[A\n",
      "6871it [1:18:59,  1.45it/s]\u001b[A\n",
      "12/02/2022 19:05:27 - INFO - run_classifier -   Loading features from cached file ./data/cached_dev_valid_pytorch_model.bin_200_codesearch\n",
      "12/02/2022 19:05:30 - INFO - run_classifier -   ***** Running evaluation  *****\n",
      "12/02/2022 19:05:30 - INFO - run_classifier -     Num examples = 83397\n",
      "12/02/2022 19:05:30 - INFO - run_classifier -     Batch size = 64\n",
      "\n",
      "Evaluating:   0%|          | 0/1304 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:   0%|          | 1/1304 [00:00<05:02,  4.30it/s]\u001b[A\n",
      "Evaluating:   0%|          | 2/1304 [00:00<04:52,  4.46it/s]\u001b[A\n",
      "Evaluating:   0%|          | 3/1304 [00:00<04:47,  4.52it/s]\u001b[A\n",
      "Evaluating:   0%|          | 4/1304 [00:00<04:45,  4.56it/s]\u001b[A\n",
      "Evaluating:   0%|          | 5/1304 [00:01<04:44,  4.57it/s]\u001b[A\n",
      "Evaluating:   0%|          | 6/1304 [00:01<04:43,  4.58it/s]\u001b[A\n",
      "Evaluating:   1%|          | 7/1304 [00:01<04:42,  4.59it/s]\u001b[A\n",
      "Evaluating:   1%|          | 8/1304 [00:01<04:42,  4.59it/s]\u001b[A\n",
      "Evaluating:   1%|          | 9/1304 [00:01<04:41,  4.60it/s]\u001b[A\n",
      "Evaluating:   1%|          | 10/1304 [00:02<04:41,  4.59it/s]\u001b[A\n",
      "Evaluating:   1%|          | 11/1304 [00:02<04:41,  4.59it/s]\u001b[A\n",
      "Evaluating:   1%|          | 12/1304 [00:02<04:41,  4.59it/s]\u001b[A\n",
      "Evaluating:   1%|          | 13/1304 [00:02<04:41,  4.59it/s]\u001b[A\n",
      "Evaluating:   1%|          | 14/1304 [00:03<04:41,  4.58it/s]\u001b[A\n",
      "Evaluating:   1%|          | 15/1304 [00:03<04:40,  4.59it/s]\u001b[A\n",
      "Evaluating:   1%|          | 16/1304 [00:03<04:40,  4.59it/s]\u001b[A\n",
      "Evaluating:   1%|▏         | 17/1304 [00:03<04:40,  4.59it/s]\u001b[A\n",
      "Evaluating:   1%|▏         | 18/1304 [00:03<04:40,  4.59it/s]\u001b[A\n",
      "Evaluating:   1%|▏         | 19/1304 [00:04<04:40,  4.59it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 20/1304 [00:04<04:39,  4.59it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 21/1304 [00:04<04:39,  4.59it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 22/1304 [00:04<04:39,  4.58it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 23/1304 [00:05<04:39,  4.59it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 24/1304 [00:05<04:38,  4.59it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 25/1304 [00:05<04:38,  4.58it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 26/1304 [00:05<04:38,  4.58it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 27/1304 [00:05<04:38,  4.59it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 28/1304 [00:06<04:37,  4.59it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 29/1304 [00:06<04:37,  4.60it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 30/1304 [00:06<04:36,  4.60it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 31/1304 [00:06<04:37,  4.59it/s]\u001b[A\n",
      "Evaluating:   2%|▏         | 32/1304 [00:06<04:37,  4.58it/s]\u001b[A\n",
      "Evaluating:   3%|▎         | 33/1304 [00:07<04:37,  4.58it/s]\u001b[A\n",
      "Evaluating:   3%|▎         | 34/1304 [00:07<04:36,  4.59it/s]\u001b[A\n",
      "Evaluating:   3%|▎         | 35/1304 [00:07<04:36,  4.59it/s]\u001b[A\n",
      "Evaluating:   3%|▎         | 36/1304 [00:07<04:35,  4.59it/s]\u001b[A\n",
      "Evaluating:   3%|▎         | 37/1304 [00:08<04:35,  4.59it/s]\u001b[A\n",
      "Evaluating:   3%|▎         | 38/1304 [00:08<04:35,  4.59it/s]\u001b[A\n",
      "Evaluating:   3%|▎         | 39/1304 [00:08<04:35,  4.60it/s]\u001b[A\n",
      "Evaluating:   3%|▎         | 40/1304 [00:08<04:35,  4.59it/s]\u001b[A\n",
      "Evaluating:   3%|▎         | 41/1304 [00:08<04:34,  4.60it/s]\u001b[A\n",
      "Evaluating:   3%|▎         | 42/1304 [00:09<04:34,  4.60it/s]\u001b[A\n",
      "Evaluating:   3%|▎         | 43/1304 [00:09<04:34,  4.60it/s]\u001b[A\n",
      "Evaluating:   3%|▎         | 44/1304 [00:09<04:34,  4.59it/s]\u001b[A\n",
      "Evaluating:   3%|▎         | 45/1304 [00:09<04:34,  4.59it/s]\u001b[A\n",
      "Evaluating:   4%|▎         | 46/1304 [00:10<04:33,  4.59it/s]\u001b[A\n",
      "Evaluating:   4%|▎         | 47/1304 [00:10<04:33,  4.59it/s]\u001b[A\n",
      "Evaluating:   4%|▎         | 48/1304 [00:10<04:33,  4.59it/s]\u001b[A\n",
      "Evaluating:   4%|▍         | 49/1304 [00:10<04:33,  4.59it/s]\u001b[A\n",
      "Evaluating:   4%|▍         | 50/1304 [00:10<04:33,  4.58it/s]\u001b[A\n",
      "Evaluating:   4%|▍         | 51/1304 [00:11<04:33,  4.59it/s]\u001b[A\n",
      "Evaluating:   4%|▍         | 52/1304 [00:11<04:32,  4.59it/s]\u001b[A\n",
      "Evaluating:   4%|▍         | 53/1304 [00:11<04:32,  4.59it/s]\u001b[A\n",
      "Evaluating:   4%|▍         | 54/1304 [00:11<04:32,  4.59it/s]\u001b[A\n",
      "Evaluating:   4%|▍         | 55/1304 [00:11<04:31,  4.59it/s]\u001b[A\n",
      "Evaluating:   4%|▍         | 56/1304 [00:12<04:31,  4.60it/s]\u001b[A\n",
      "Evaluating:   4%|▍         | 57/1304 [00:12<04:31,  4.60it/s]\u001b[A\n",
      "Evaluating:   4%|▍         | 58/1304 [00:12<04:30,  4.60it/s]\u001b[A\n",
      "Evaluating:   5%|▍         | 59/1304 [00:12<04:30,  4.60it/s]\u001b[A\n",
      "Evaluating:   5%|▍         | 60/1304 [00:13<04:30,  4.60it/s]\u001b[A\n",
      "Evaluating:   5%|▍         | 61/1304 [00:13<04:30,  4.60it/s]\u001b[A\n",
      "Evaluating:   5%|▍         | 62/1304 [00:13<04:29,  4.60it/s]\u001b[A\n",
      "Evaluating:   5%|▍         | 63/1304 [00:13<04:29,  4.60it/s]\u001b[A\n",
      "Evaluating:   5%|▍         | 64/1304 [00:13<04:29,  4.60it/s]\u001b[A\n",
      "Evaluating:   5%|▍         | 65/1304 [00:14<04:29,  4.60it/s]\u001b[A\n",
      "Evaluating:   5%|▌         | 66/1304 [00:14<04:29,  4.60it/s]\u001b[A\n",
      "Evaluating:   5%|▌         | 67/1304 [00:14<04:29,  4.59it/s]\u001b[A\n",
      "Evaluating:   5%|▌         | 68/1304 [00:14<04:29,  4.59it/s]\u001b[A\n",
      "Evaluating:   5%|▌         | 69/1304 [00:15<04:28,  4.59it/s]\u001b[A\n",
      "Evaluating:   5%|▌         | 70/1304 [00:15<04:29,  4.59it/s]\u001b[A\n",
      "Evaluating:   5%|▌         | 71/1304 [00:15<04:29,  4.58it/s]\u001b[A\n",
      "Evaluating:   6%|▌         | 72/1304 [00:15<04:28,  4.59it/s]\u001b[A\n",
      "Evaluating:   6%|▌         | 73/1304 [00:15<04:28,  4.59it/s]\u001b[A\n",
      "Evaluating:   6%|▌         | 74/1304 [00:16<04:28,  4.59it/s]\u001b[A\n",
      "Evaluating:   6%|▌         | 75/1304 [00:16<04:27,  4.59it/s]\u001b[A\n",
      "Evaluating:   6%|▌         | 76/1304 [00:16<04:27,  4.58it/s]\u001b[A\n",
      "Evaluating:   6%|▌         | 77/1304 [00:16<04:27,  4.58it/s]\u001b[A\n",
      "Evaluating:   6%|▌         | 78/1304 [00:17<04:27,  4.58it/s]\u001b[A\n",
      "Evaluating:   6%|▌         | 79/1304 [00:17<04:27,  4.58it/s]\u001b[A\n",
      "Evaluating:   6%|▌         | 80/1304 [00:17<04:27,  4.58it/s]\u001b[A\n",
      "Evaluating:   6%|▌         | 81/1304 [00:17<04:26,  4.58it/s]\u001b[A\n",
      "Evaluating:   6%|▋         | 82/1304 [00:17<04:26,  4.58it/s]\u001b[A\n",
      "Evaluating:   6%|▋         | 83/1304 [00:18<04:26,  4.58it/s]\u001b[A\n",
      "Evaluating:   6%|▋         | 84/1304 [00:18<04:26,  4.58it/s]\u001b[A\n",
      "Evaluating:   7%|▋         | 85/1304 [00:18<04:26,  4.58it/s]\u001b[A\n",
      "Evaluating:   7%|▋         | 86/1304 [00:18<04:25,  4.58it/s]\u001b[A\n",
      "Evaluating:   7%|▋         | 87/1304 [00:18<04:25,  4.58it/s]\u001b[A\n",
      "Evaluating:   7%|▋         | 88/1304 [00:19<04:25,  4.58it/s]\u001b[A\n",
      "Evaluating:   7%|▋         | 89/1304 [00:19<04:25,  4.58it/s]\u001b[A\n",
      "Evaluating:   7%|▋         | 90/1304 [00:19<04:25,  4.58it/s]\u001b[A\n",
      "Evaluating:   7%|▋         | 91/1304 [00:19<04:25,  4.58it/s]\u001b[A\n",
      "Evaluating:   7%|▋         | 92/1304 [00:20<04:24,  4.58it/s]\u001b[A\n",
      "Evaluating:   7%|▋         | 93/1304 [00:20<04:24,  4.58it/s]\u001b[A\n",
      "Evaluating:   7%|▋         | 94/1304 [00:20<04:24,  4.58it/s]\u001b[A\n",
      "Evaluating:   7%|▋         | 95/1304 [00:20<04:23,  4.59it/s]\u001b[A\n",
      "Evaluating:   7%|▋         | 96/1304 [00:20<04:22,  4.59it/s]\u001b[A\n",
      "Evaluating:   7%|▋         | 97/1304 [00:21<04:22,  4.60it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 98/1304 [00:21<04:22,  4.60it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 99/1304 [00:21<04:21,  4.60it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 100/1304 [00:21<04:21,  4.60it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 101/1304 [00:22<04:21,  4.60it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 102/1304 [00:22<04:21,  4.60it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 103/1304 [00:22<04:21,  4.60it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 104/1304 [00:22<04:21,  4.59it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 105/1304 [00:22<04:21,  4.59it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 106/1304 [00:23<04:21,  4.58it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 107/1304 [00:23<04:21,  4.58it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 108/1304 [00:23<04:20,  4.59it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 109/1304 [00:23<04:20,  4.59it/s]\u001b[A\n",
      "Evaluating:   8%|▊         | 110/1304 [00:23<04:20,  4.58it/s]\u001b[A\n",
      "Evaluating:   9%|▊         | 111/1304 [00:24<04:20,  4.58it/s]\u001b[A\n",
      "Evaluating:   9%|▊         | 112/1304 [00:24<04:20,  4.58it/s]\u001b[A\n",
      "Evaluating:   9%|▊         | 113/1304 [00:24<04:20,  4.58it/s]\u001b[A\n",
      "Evaluating:   9%|▊         | 114/1304 [00:24<04:19,  4.58it/s]\u001b[A\n",
      "Evaluating:   9%|▉         | 115/1304 [00:25<04:19,  4.58it/s]\u001b[A\n",
      "Evaluating:   9%|▉         | 116/1304 [00:25<04:19,  4.58it/s]\u001b[A\n",
      "Evaluating:   9%|▉         | 117/1304 [00:25<04:18,  4.58it/s]\u001b[A\n",
      "Evaluating:   9%|▉         | 118/1304 [00:25<04:18,  4.58it/s]\u001b[A\n",
      "Evaluating:   9%|▉         | 119/1304 [00:25<04:18,  4.58it/s]\u001b[A\n",
      "Evaluating:   9%|▉         | 120/1304 [00:26<04:18,  4.58it/s]\u001b[A\n",
      "Evaluating:   9%|▉         | 121/1304 [00:26<04:18,  4.58it/s]\u001b[A\n",
      "Evaluating:   9%|▉         | 122/1304 [00:26<04:18,  4.58it/s]\u001b[A\n",
      "Evaluating:   9%|▉         | 123/1304 [00:26<04:18,  4.58it/s]\u001b[A\n",
      "Evaluating:  10%|▉         | 124/1304 [00:27<04:17,  4.58it/s]\u001b[A\n",
      "Evaluating:  10%|▉         | 125/1304 [00:27<04:17,  4.58it/s]\u001b[A\n",
      "Evaluating:  10%|▉         | 126/1304 [00:27<04:17,  4.57it/s]\u001b[A\n",
      "Evaluating:  10%|▉         | 127/1304 [00:27<04:17,  4.57it/s]\u001b[A\n",
      "Evaluating:  10%|▉         | 128/1304 [00:27<04:16,  4.58it/s]\u001b[A\n",
      "Evaluating:  10%|▉         | 129/1304 [00:28<04:16,  4.58it/s]\u001b[A\n",
      "Evaluating:  10%|▉         | 130/1304 [00:28<04:16,  4.58it/s]\u001b[A\n",
      "Evaluating:  10%|█         | 131/1304 [00:28<04:15,  4.58it/s]\u001b[A\n",
      "Evaluating:  10%|█         | 132/1304 [00:28<04:15,  4.58it/s]\u001b[A\n",
      "Evaluating:  10%|█         | 133/1304 [00:29<04:15,  4.58it/s]\u001b[A\n",
      "Evaluating:  10%|█         | 134/1304 [00:29<04:15,  4.58it/s]\u001b[A\n",
      "Evaluating:  10%|█         | 135/1304 [00:29<04:15,  4.58it/s]\u001b[A\n",
      "Evaluating:  10%|█         | 136/1304 [00:29<04:15,  4.58it/s]\u001b[A\n",
      "Evaluating:  11%|█         | 137/1304 [00:29<04:14,  4.58it/s]\u001b[A\n",
      "Evaluating:  11%|█         | 138/1304 [00:30<04:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  11%|█         | 139/1304 [00:30<04:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  11%|█         | 140/1304 [00:30<04:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  11%|█         | 141/1304 [00:30<04:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  11%|█         | 142/1304 [00:30<04:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  11%|█         | 143/1304 [00:31<04:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  11%|█         | 144/1304 [00:31<04:13,  4.58it/s]\u001b[A\n",
      "Evaluating:  11%|█         | 145/1304 [00:31<04:12,  4.58it/s]\u001b[A\n",
      "Evaluating:  11%|█         | 146/1304 [00:31<04:12,  4.58it/s]\u001b[A\n",
      "Evaluating:  11%|█▏        | 147/1304 [00:32<04:12,  4.58it/s]\u001b[A\n",
      "Evaluating:  11%|█▏        | 148/1304 [00:32<04:12,  4.58it/s]\u001b[A\n",
      "Evaluating:  11%|█▏        | 149/1304 [00:32<04:12,  4.57it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 150/1304 [00:32<04:12,  4.58it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 151/1304 [00:32<04:11,  4.58it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 152/1304 [00:33<04:11,  4.57it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 153/1304 [00:33<04:11,  4.57it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  12%|█▏        | 154/1304 [00:33<04:11,  4.57it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 155/1304 [00:33<04:11,  4.57it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 156/1304 [00:34<04:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 157/1304 [00:34<04:10,  4.58it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 158/1304 [00:34<04:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 159/1304 [00:34<04:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 160/1304 [00:34<04:09,  4.58it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 161/1304 [00:35<04:09,  4.59it/s]\u001b[A\n",
      "Evaluating:  12%|█▏        | 162/1304 [00:35<04:08,  4.59it/s]\u001b[A\n",
      "Evaluating:  12%|█▎        | 163/1304 [00:35<04:09,  4.58it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 164/1304 [00:35<04:09,  4.58it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 165/1304 [00:35<04:08,  4.58it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 166/1304 [00:36<04:08,  4.58it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 167/1304 [00:36<04:08,  4.58it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 168/1304 [00:36<04:07,  4.59it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 169/1304 [00:36<04:07,  4.59it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 170/1304 [00:37<04:07,  4.59it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 171/1304 [00:37<04:07,  4.59it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 172/1304 [00:37<04:06,  4.58it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 173/1304 [00:37<04:06,  4.58it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 174/1304 [00:37<04:06,  4.58it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 175/1304 [00:38<04:06,  4.58it/s]\u001b[A\n",
      "Evaluating:  13%|█▎        | 176/1304 [00:38<04:06,  4.58it/s]\u001b[A\n",
      "Evaluating:  14%|█▎        | 177/1304 [00:38<04:05,  4.58it/s]\u001b[A\n",
      "Evaluating:  14%|█▎        | 178/1304 [00:38<04:05,  4.58it/s]\u001b[A\n",
      "Evaluating:  14%|█▎        | 179/1304 [00:39<04:05,  4.58it/s]\u001b[A\n",
      "Evaluating:  14%|█▍        | 180/1304 [00:39<04:05,  4.57it/s]\u001b[A\n",
      "Evaluating:  14%|█▍        | 181/1304 [00:39<04:05,  4.57it/s]\u001b[A\n",
      "Evaluating:  14%|█▍        | 182/1304 [00:39<04:05,  4.57it/s]\u001b[A\n",
      "Evaluating:  14%|█▍        | 183/1304 [00:39<04:04,  4.58it/s]\u001b[A\n",
      "Evaluating:  14%|█▍        | 184/1304 [00:40<04:04,  4.58it/s]\u001b[A\n",
      "Evaluating:  14%|█▍        | 185/1304 [00:40<04:04,  4.58it/s]\u001b[A\n",
      "Evaluating:  14%|█▍        | 186/1304 [00:40<04:04,  4.58it/s]\u001b[A\n",
      "Evaluating:  14%|█▍        | 187/1304 [00:40<04:04,  4.57it/s]\u001b[A\n",
      "Evaluating:  14%|█▍        | 188/1304 [00:41<04:03,  4.57it/s]\u001b[A\n",
      "Evaluating:  14%|█▍        | 189/1304 [00:41<04:03,  4.58it/s]\u001b[A\n",
      "Evaluating:  15%|█▍        | 190/1304 [00:41<04:03,  4.57it/s]\u001b[A\n",
      "Evaluating:  15%|█▍        | 191/1304 [00:41<04:03,  4.58it/s]\u001b[A\n",
      "Evaluating:  15%|█▍        | 192/1304 [00:41<04:02,  4.58it/s]\u001b[A\n",
      "Evaluating:  15%|█▍        | 193/1304 [00:42<04:02,  4.58it/s]\u001b[A\n",
      "Evaluating:  15%|█▍        | 194/1304 [00:42<04:02,  4.58it/s]\u001b[A\n",
      "Evaluating:  15%|█▍        | 195/1304 [00:42<04:02,  4.58it/s]\u001b[A\n",
      "Evaluating:  15%|█▌        | 196/1304 [00:42<04:02,  4.58it/s]\u001b[A\n",
      "Evaluating:  15%|█▌        | 197/1304 [00:42<04:01,  4.58it/s]\u001b[A\n",
      "Evaluating:  15%|█▌        | 198/1304 [00:43<04:01,  4.59it/s]\u001b[A\n",
      "Evaluating:  15%|█▌        | 199/1304 [00:43<04:01,  4.58it/s]\u001b[A\n",
      "Evaluating:  15%|█▌        | 200/1304 [00:43<04:01,  4.57it/s]\u001b[A\n",
      "Evaluating:  15%|█▌        | 201/1304 [00:43<04:01,  4.57it/s]\u001b[A\n",
      "Evaluating:  15%|█▌        | 202/1304 [00:44<04:00,  4.58it/s]\u001b[A\n",
      "Evaluating:  16%|█▌        | 203/1304 [00:44<04:00,  4.58it/s]\u001b[A\n",
      "Evaluating:  16%|█▌        | 204/1304 [00:44<03:59,  4.59it/s]\u001b[A\n",
      "Evaluating:  16%|█▌        | 205/1304 [00:44<03:59,  4.59it/s]\u001b[A\n",
      "Evaluating:  16%|█▌        | 206/1304 [00:44<03:58,  4.60it/s]\u001b[A\n",
      "Evaluating:  16%|█▌        | 207/1304 [00:45<03:58,  4.60it/s]\u001b[A\n",
      "Evaluating:  16%|█▌        | 208/1304 [00:45<03:58,  4.59it/s]\u001b[A\n",
      "Evaluating:  16%|█▌        | 209/1304 [00:45<03:59,  4.58it/s]\u001b[A\n",
      "Evaluating:  16%|█▌        | 210/1304 [00:45<03:58,  4.58it/s]\u001b[A\n",
      "Evaluating:  16%|█▌        | 211/1304 [00:46<03:58,  4.58it/s]\u001b[A\n",
      "Evaluating:  16%|█▋        | 212/1304 [00:46<03:58,  4.58it/s]\u001b[A\n",
      "Evaluating:  16%|█▋        | 213/1304 [00:46<03:58,  4.57it/s]\u001b[A\n",
      "Evaluating:  16%|█▋        | 214/1304 [00:46<03:58,  4.58it/s]\u001b[A\n",
      "Evaluating:  16%|█▋        | 215/1304 [00:46<03:57,  4.58it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 216/1304 [00:47<03:57,  4.58it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 217/1304 [00:47<03:57,  4.58it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 218/1304 [00:47<03:57,  4.58it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 219/1304 [00:47<03:57,  4.57it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 220/1304 [00:48<03:56,  4.57it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 221/1304 [00:48<03:56,  4.58it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 222/1304 [00:48<03:56,  4.57it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 223/1304 [00:48<03:56,  4.57it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 224/1304 [00:48<03:56,  4.57it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 225/1304 [00:49<03:56,  4.57it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 226/1304 [00:49<03:55,  4.57it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 227/1304 [00:49<03:55,  4.57it/s]\u001b[A\n",
      "Evaluating:  17%|█▋        | 228/1304 [00:49<03:55,  4.57it/s]\u001b[A\n",
      "Evaluating:  18%|█▊        | 229/1304 [00:49<03:55,  4.57it/s]\u001b[A\n",
      "Evaluating:  18%|█▊        | 230/1304 [00:50<03:55,  4.57it/s]\u001b[A\n",
      "Evaluating:  18%|█▊        | 231/1304 [00:50<03:54,  4.57it/s]\u001b[A\n",
      "Evaluating:  18%|█▊        | 232/1304 [00:50<03:54,  4.57it/s]\u001b[A\n",
      "Evaluating:  18%|█▊        | 233/1304 [00:50<03:54,  4.57it/s]\u001b[A\n",
      "Evaluating:  18%|█▊        | 234/1304 [00:51<03:53,  4.58it/s]\u001b[A\n",
      "Evaluating:  18%|█▊        | 235/1304 [00:51<03:53,  4.58it/s]\u001b[A\n",
      "Evaluating:  18%|█▊        | 236/1304 [00:51<03:53,  4.58it/s]\u001b[A\n",
      "Evaluating:  18%|█▊        | 237/1304 [00:51<03:52,  4.59it/s]\u001b[A\n",
      "Evaluating:  18%|█▊        | 238/1304 [00:51<03:52,  4.59it/s]\u001b[A\n",
      "Evaluating:  18%|█▊        | 239/1304 [00:52<03:52,  4.58it/s]\u001b[A\n",
      "Evaluating:  18%|█▊        | 240/1304 [00:52<03:52,  4.58it/s]\u001b[A\n",
      "Evaluating:  18%|█▊        | 241/1304 [00:52<03:52,  4.58it/s]\u001b[A\n",
      "Evaluating:  19%|█▊        | 242/1304 [00:52<03:51,  4.59it/s]\u001b[A\n",
      "Evaluating:  19%|█▊        | 243/1304 [00:53<03:51,  4.59it/s]\u001b[A\n",
      "Evaluating:  19%|█▊        | 244/1304 [00:53<03:51,  4.58it/s]\u001b[A\n",
      "Evaluating:  19%|█▉        | 245/1304 [00:53<03:51,  4.58it/s]\u001b[A\n",
      "Evaluating:  19%|█▉        | 246/1304 [00:53<03:51,  4.58it/s]\u001b[A\n",
      "Evaluating:  19%|█▉        | 247/1304 [00:53<03:50,  4.59it/s]\u001b[A\n",
      "Evaluating:  19%|█▉        | 248/1304 [00:54<03:50,  4.58it/s]\u001b[A\n",
      "Evaluating:  19%|█▉        | 249/1304 [00:54<03:50,  4.58it/s]\u001b[A\n",
      "Evaluating:  19%|█▉        | 250/1304 [00:54<03:50,  4.57it/s]\u001b[A\n",
      "Evaluating:  19%|█▉        | 251/1304 [00:54<03:50,  4.57it/s]\u001b[A\n",
      "Evaluating:  19%|█▉        | 252/1304 [00:54<03:50,  4.57it/s]\u001b[A\n",
      "Evaluating:  19%|█▉        | 253/1304 [00:55<03:49,  4.57it/s]\u001b[A\n",
      "Evaluating:  19%|█▉        | 254/1304 [00:55<03:49,  4.58it/s]\u001b[A\n",
      "Evaluating:  20%|█▉        | 255/1304 [00:55<03:48,  4.59it/s]\u001b[A\n",
      "Evaluating:  20%|█▉        | 256/1304 [00:55<03:48,  4.58it/s]\u001b[A\n",
      "Evaluating:  20%|█▉        | 257/1304 [00:56<03:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  20%|█▉        | 258/1304 [00:56<03:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  20%|█▉        | 259/1304 [00:56<03:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  20%|█▉        | 260/1304 [00:56<03:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  20%|██        | 261/1304 [00:56<03:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  20%|██        | 262/1304 [00:57<03:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  20%|██        | 263/1304 [00:57<03:47,  4.57it/s]\u001b[A\n",
      "Evaluating:  20%|██        | 264/1304 [00:57<03:47,  4.57it/s]\u001b[A\n",
      "Evaluating:  20%|██        | 265/1304 [00:57<03:47,  4.57it/s]\u001b[A\n",
      "Evaluating:  20%|██        | 266/1304 [00:58<03:47,  4.57it/s]\u001b[A\n",
      "Evaluating:  20%|██        | 267/1304 [00:58<03:46,  4.57it/s]\u001b[A\n",
      "Evaluating:  21%|██        | 268/1304 [00:58<03:46,  4.57it/s]\u001b[A\n",
      "Evaluating:  21%|██        | 269/1304 [00:58<03:46,  4.57it/s]\u001b[A\n",
      "Evaluating:  21%|██        | 270/1304 [00:58<03:46,  4.57it/s]\u001b[A\n",
      "Evaluating:  21%|██        | 271/1304 [00:59<03:46,  4.57it/s]\u001b[A\n",
      "Evaluating:  21%|██        | 272/1304 [00:59<03:45,  4.57it/s]\u001b[A\n",
      "Evaluating:  21%|██        | 273/1304 [00:59<03:45,  4.57it/s]\u001b[A\n",
      "Evaluating:  21%|██        | 274/1304 [00:59<03:45,  4.56it/s]\u001b[A\n",
      "Evaluating:  21%|██        | 275/1304 [01:00<03:45,  4.56it/s]\u001b[A\n",
      "Evaluating:  21%|██        | 276/1304 [01:00<03:45,  4.57it/s]\u001b[A\n",
      "Evaluating:  21%|██        | 277/1304 [01:00<03:45,  4.56it/s]\u001b[A\n",
      "Evaluating:  21%|██▏       | 278/1304 [01:00<03:44,  4.57it/s]\u001b[A\n",
      "Evaluating:  21%|██▏       | 279/1304 [01:00<03:44,  4.57it/s]\u001b[A\n",
      "Evaluating:  21%|██▏       | 280/1304 [01:01<03:44,  4.57it/s]\u001b[A\n",
      "Evaluating:  22%|██▏       | 281/1304 [01:01<03:44,  4.56it/s]\u001b[A\n",
      "Evaluating:  22%|██▏       | 282/1304 [01:01<03:43,  4.56it/s]\u001b[A\n",
      "Evaluating:  22%|██▏       | 283/1304 [01:01<03:43,  4.57it/s]\u001b[A\n",
      "Evaluating:  22%|██▏       | 284/1304 [01:01<03:43,  4.57it/s]\u001b[A\n",
      "Evaluating:  22%|██▏       | 285/1304 [01:02<03:42,  4.57it/s]\u001b[A\n",
      "Evaluating:  22%|██▏       | 286/1304 [01:02<03:42,  4.57it/s]\u001b[A\n",
      "Evaluating:  22%|██▏       | 287/1304 [01:02<03:42,  4.57it/s]\u001b[A\n",
      "Evaluating:  22%|██▏       | 288/1304 [01:02<03:42,  4.57it/s]\u001b[A\n",
      "Evaluating:  22%|██▏       | 289/1304 [01:03<03:41,  4.58it/s]\u001b[A\n",
      "Evaluating:  22%|██▏       | 290/1304 [01:03<03:41,  4.57it/s]\u001b[A\n",
      "Evaluating:  22%|██▏       | 291/1304 [01:03<03:41,  4.57it/s]\u001b[A\n",
      "Evaluating:  22%|██▏       | 292/1304 [01:03<03:41,  4.57it/s]\u001b[A\n",
      "Evaluating:  22%|██▏       | 293/1304 [01:03<03:41,  4.57it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 294/1304 [01:04<03:41,  4.57it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 295/1304 [01:04<03:40,  4.57it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 296/1304 [01:04<03:40,  4.57it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 297/1304 [01:04<03:40,  4.57it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 298/1304 [01:05<03:40,  4.57it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 299/1304 [01:05<03:39,  4.57it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 300/1304 [01:05<03:39,  4.57it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 301/1304 [01:05<03:39,  4.58it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 302/1304 [01:05<03:39,  4.57it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 303/1304 [01:06<03:39,  4.57it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 304/1304 [01:06<03:38,  4.57it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 305/1304 [01:06<03:38,  4.57it/s]\u001b[A\n",
      "Evaluating:  23%|██▎       | 306/1304 [01:06<03:37,  4.58it/s]\u001b[A\n",
      "Evaluating:  24%|██▎       | 307/1304 [01:07<03:37,  4.58it/s]\u001b[A\n",
      "Evaluating:  24%|██▎       | 308/1304 [01:07<03:37,  4.58it/s]\u001b[A\n",
      "Evaluating:  24%|██▎       | 309/1304 [01:07<03:37,  4.58it/s]\u001b[A\n",
      "Evaluating:  24%|██▍       | 310/1304 [01:07<03:37,  4.57it/s]\u001b[A\n",
      "Evaluating:  24%|██▍       | 311/1304 [01:07<03:37,  4.57it/s]\u001b[A\n",
      "Evaluating:  24%|██▍       | 312/1304 [01:08<03:36,  4.58it/s]\u001b[A\n",
      "Evaluating:  24%|██▍       | 313/1304 [01:08<03:36,  4.58it/s]\u001b[A\n",
      "Evaluating:  24%|██▍       | 314/1304 [01:08<03:36,  4.57it/s]\u001b[A\n",
      "Evaluating:  24%|██▍       | 315/1304 [01:08<03:36,  4.57it/s]\u001b[A\n",
      "Evaluating:  24%|██▍       | 316/1304 [01:08<03:35,  4.57it/s]\u001b[A\n",
      "Evaluating:  24%|██▍       | 317/1304 [01:09<03:35,  4.57it/s]\u001b[A\n",
      "Evaluating:  24%|██▍       | 318/1304 [01:09<03:35,  4.57it/s]\u001b[A\n",
      "Evaluating:  24%|██▍       | 319/1304 [01:09<03:34,  4.58it/s]\u001b[A\n",
      "Evaluating:  25%|██▍       | 320/1304 [01:09<03:34,  4.58it/s]\u001b[A\n",
      "Evaluating:  25%|██▍       | 321/1304 [01:10<03:34,  4.58it/s]\u001b[A\n",
      "Evaluating:  25%|██▍       | 322/1304 [01:10<03:34,  4.58it/s]\u001b[A\n",
      "Evaluating:  25%|██▍       | 323/1304 [01:10<03:34,  4.58it/s]\u001b[A\n",
      "Evaluating:  25%|██▍       | 324/1304 [01:10<03:34,  4.58it/s]\u001b[A\n",
      "Evaluating:  25%|██▍       | 325/1304 [01:10<03:33,  4.58it/s]\u001b[A\n",
      "Evaluating:  25%|██▌       | 326/1304 [01:11<03:33,  4.59it/s]\u001b[A\n",
      "Evaluating:  25%|██▌       | 327/1304 [01:11<03:33,  4.58it/s]\u001b[A\n",
      "Evaluating:  25%|██▌       | 328/1304 [01:11<03:33,  4.58it/s]\u001b[A\n",
      "Evaluating:  25%|██▌       | 329/1304 [01:11<03:33,  4.58it/s]\u001b[A\n",
      "Evaluating:  25%|██▌       | 330/1304 [01:12<03:32,  4.58it/s]\u001b[A\n",
      "Evaluating:  25%|██▌       | 331/1304 [01:12<03:32,  4.58it/s]\u001b[A\n",
      "Evaluating:  25%|██▌       | 332/1304 [01:12<03:32,  4.57it/s]\u001b[A\n",
      "Evaluating:  26%|██▌       | 333/1304 [01:12<03:32,  4.57it/s]\u001b[A\n",
      "Evaluating:  26%|██▌       | 334/1304 [01:12<03:32,  4.57it/s]\u001b[A\n",
      "Evaluating:  26%|██▌       | 335/1304 [01:13<03:32,  4.57it/s]\u001b[A\n",
      "Evaluating:  26%|██▌       | 336/1304 [01:13<03:31,  4.57it/s]\u001b[A\n",
      "Evaluating:  26%|██▌       | 337/1304 [01:13<03:31,  4.57it/s]\u001b[A\n",
      "Evaluating:  26%|██▌       | 338/1304 [01:13<03:31,  4.58it/s]\u001b[A\n",
      "Evaluating:  26%|██▌       | 339/1304 [01:14<03:30,  4.58it/s]\u001b[A\n",
      "Evaluating:  26%|██▌       | 340/1304 [01:14<03:30,  4.58it/s]\u001b[A\n",
      "Evaluating:  26%|██▌       | 341/1304 [01:14<03:30,  4.58it/s]\u001b[A\n",
      "Evaluating:  26%|██▌       | 342/1304 [01:14<03:29,  4.58it/s]\u001b[A\n",
      "Evaluating:  26%|██▋       | 343/1304 [01:14<03:29,  4.58it/s]\u001b[A\n",
      "Evaluating:  26%|██▋       | 344/1304 [01:15<03:29,  4.58it/s]\u001b[A\n",
      "Evaluating:  26%|██▋       | 345/1304 [01:15<03:29,  4.57it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 346/1304 [01:15<03:29,  4.57it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 347/1304 [01:15<03:28,  4.58it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 348/1304 [01:15<03:28,  4.58it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 349/1304 [01:16<03:28,  4.58it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 350/1304 [01:16<03:28,  4.59it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 351/1304 [01:16<03:28,  4.58it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 352/1304 [01:16<03:27,  4.58it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 353/1304 [01:17<03:27,  4.58it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 354/1304 [01:17<03:27,  4.58it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 355/1304 [01:17<03:27,  4.57it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 356/1304 [01:17<03:27,  4.58it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 357/1304 [01:17<03:26,  4.58it/s]\u001b[A\n",
      "Evaluating:  27%|██▋       | 358/1304 [01:18<03:26,  4.57it/s]\u001b[A\n",
      "Evaluating:  28%|██▊       | 359/1304 [01:18<03:26,  4.57it/s]\u001b[A\n",
      "Evaluating:  28%|██▊       | 360/1304 [01:18<03:26,  4.58it/s]\u001b[A\n",
      "Evaluating:  28%|██▊       | 361/1304 [01:18<03:25,  4.58it/s]\u001b[A\n",
      "Evaluating:  28%|██▊       | 362/1304 [01:19<03:25,  4.58it/s]\u001b[A\n",
      "Evaluating:  28%|██▊       | 363/1304 [01:19<03:25,  4.58it/s]\u001b[A\n",
      "Evaluating:  28%|██▊       | 364/1304 [01:19<03:25,  4.58it/s]\u001b[A\n",
      "Evaluating:  28%|██▊       | 365/1304 [01:19<03:25,  4.57it/s]\u001b[A\n",
      "Evaluating:  28%|██▊       | 366/1304 [01:19<03:24,  4.58it/s]\u001b[A\n",
      "Evaluating:  28%|██▊       | 367/1304 [01:20<03:24,  4.58it/s]\u001b[A\n",
      "Evaluating:  28%|██▊       | 368/1304 [01:20<03:24,  4.58it/s]\u001b[A\n",
      "Evaluating:  28%|██▊       | 369/1304 [01:20<03:24,  4.58it/s]\u001b[A\n",
      "Evaluating:  28%|██▊       | 370/1304 [01:20<03:24,  4.58it/s]\u001b[A\n",
      "Evaluating:  28%|██▊       | 371/1304 [01:21<03:24,  4.57it/s]\u001b[A\n",
      "Evaluating:  29%|██▊       | 372/1304 [01:21<03:23,  4.57it/s]\u001b[A\n",
      "Evaluating:  29%|██▊       | 373/1304 [01:21<03:23,  4.57it/s]\u001b[A\n",
      "Evaluating:  29%|██▊       | 374/1304 [01:21<03:23,  4.57it/s]\u001b[A\n",
      "Evaluating:  29%|██▉       | 375/1304 [01:21<03:23,  4.57it/s]\u001b[A\n",
      "Evaluating:  29%|██▉       | 376/1304 [01:22<03:22,  4.57it/s]\u001b[A\n",
      "Evaluating:  29%|██▉       | 377/1304 [01:22<03:22,  4.57it/s]\u001b[A\n",
      "Evaluating:  29%|██▉       | 378/1304 [01:22<03:22,  4.57it/s]\u001b[A\n",
      "Evaluating:  29%|██▉       | 379/1304 [01:22<03:22,  4.57it/s]\u001b[A\n",
      "Evaluating:  29%|██▉       | 380/1304 [01:22<03:22,  4.57it/s]\u001b[A\n",
      "Evaluating:  29%|██▉       | 381/1304 [01:23<03:21,  4.57it/s]\u001b[A\n",
      "Evaluating:  29%|██▉       | 382/1304 [01:23<03:21,  4.58it/s]\u001b[A\n",
      "Evaluating:  29%|██▉       | 383/1304 [01:23<03:21,  4.57it/s]\u001b[A\n",
      "Evaluating:  29%|██▉       | 384/1304 [01:23<03:21,  4.57it/s]\u001b[A\n",
      "Evaluating:  30%|██▉       | 385/1304 [01:24<03:21,  4.57it/s]\u001b[A\n",
      "Evaluating:  30%|██▉       | 386/1304 [01:24<03:20,  4.57it/s]\u001b[A\n",
      "Evaluating:  30%|██▉       | 387/1304 [01:24<03:20,  4.57it/s]\u001b[A\n",
      "Evaluating:  30%|██▉       | 388/1304 [01:24<03:20,  4.57it/s]\u001b[A\n",
      "Evaluating:  30%|██▉       | 389/1304 [01:24<03:20,  4.57it/s]\u001b[A\n",
      "Evaluating:  30%|██▉       | 390/1304 [01:25<03:19,  4.57it/s]\u001b[A\n",
      "Evaluating:  30%|██▉       | 391/1304 [01:25<03:19,  4.57it/s]\u001b[A\n",
      "Evaluating:  30%|███       | 392/1304 [01:25<03:19,  4.58it/s]\u001b[A\n",
      "Evaluating:  30%|███       | 393/1304 [01:25<03:19,  4.58it/s]\u001b[A\n",
      "Evaluating:  30%|███       | 394/1304 [01:26<03:18,  4.58it/s]\u001b[A\n",
      "Evaluating:  30%|███       | 395/1304 [01:26<03:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  30%|███       | 396/1304 [01:26<03:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  30%|███       | 397/1304 [01:26<03:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  31%|███       | 398/1304 [01:26<03:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  31%|███       | 399/1304 [01:27<03:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  31%|███       | 400/1304 [01:27<03:17,  4.57it/s]\u001b[A\n",
      "Evaluating:  31%|███       | 401/1304 [01:27<03:17,  4.57it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  31%|███       | 402/1304 [01:27<03:17,  4.58it/s]\u001b[A\n",
      "Evaluating:  31%|███       | 403/1304 [01:28<03:16,  4.58it/s]\u001b[A\n",
      "Evaluating:  31%|███       | 404/1304 [01:28<03:16,  4.58it/s]\u001b[A\n",
      "Evaluating:  31%|███       | 405/1304 [01:28<03:16,  4.58it/s]\u001b[A\n",
      "Evaluating:  31%|███       | 406/1304 [01:28<03:16,  4.58it/s]\u001b[A\n",
      "Evaluating:  31%|███       | 407/1304 [01:28<03:15,  4.58it/s]\u001b[A\n",
      "Evaluating:  31%|███▏      | 408/1304 [01:29<03:15,  4.58it/s]\u001b[A\n",
      "Evaluating:  31%|███▏      | 409/1304 [01:29<03:15,  4.58it/s]\u001b[A\n",
      "Evaluating:  31%|███▏      | 410/1304 [01:29<03:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  32%|███▏      | 411/1304 [01:29<03:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  32%|███▏      | 412/1304 [01:29<03:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  32%|███▏      | 413/1304 [01:30<03:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  32%|███▏      | 414/1304 [01:30<03:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  32%|███▏      | 415/1304 [01:30<03:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  32%|███▏      | 416/1304 [01:30<03:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  32%|███▏      | 417/1304 [01:31<03:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  32%|███▏      | 418/1304 [01:31<03:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  32%|███▏      | 419/1304 [01:31<03:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  32%|███▏      | 420/1304 [01:31<03:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  32%|███▏      | 421/1304 [01:31<03:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  32%|███▏      | 422/1304 [01:32<03:12,  4.58it/s]\u001b[A\n",
      "Evaluating:  32%|███▏      | 423/1304 [01:32<03:12,  4.58it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 424/1304 [01:32<03:12,  4.58it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 425/1304 [01:32<03:11,  4.58it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 426/1304 [01:33<03:11,  4.58it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 427/1304 [01:33<03:11,  4.58it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 428/1304 [01:33<03:11,  4.58it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 429/1304 [01:33<03:11,  4.58it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 430/1304 [01:33<03:11,  4.57it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 431/1304 [01:34<03:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 432/1304 [01:34<03:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 433/1304 [01:34<03:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 434/1304 [01:34<03:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 435/1304 [01:35<03:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  33%|███▎      | 436/1304 [01:35<03:09,  4.57it/s]\u001b[A\n",
      "Evaluating:  34%|███▎      | 437/1304 [01:35<03:09,  4.57it/s]\u001b[A\n",
      "Evaluating:  34%|███▎      | 438/1304 [01:35<03:09,  4.57it/s]\u001b[A\n",
      "Evaluating:  34%|███▎      | 439/1304 [01:35<03:09,  4.57it/s]\u001b[A\n",
      "Evaluating:  34%|███▎      | 440/1304 [01:36<03:09,  4.57it/s]\u001b[A\n",
      "Evaluating:  34%|███▍      | 441/1304 [01:36<03:08,  4.57it/s]\u001b[A\n",
      "Evaluating:  34%|███▍      | 442/1304 [01:36<03:08,  4.57it/s]\u001b[A\n",
      "Evaluating:  34%|███▍      | 443/1304 [01:36<03:08,  4.57it/s]\u001b[A\n",
      "Evaluating:  34%|███▍      | 444/1304 [01:36<03:08,  4.57it/s]\u001b[A\n",
      "Evaluating:  34%|███▍      | 445/1304 [01:37<03:08,  4.57it/s]\u001b[A\n",
      "Evaluating:  34%|███▍      | 446/1304 [01:37<03:07,  4.57it/s]\u001b[A\n",
      "Evaluating:  34%|███▍      | 447/1304 [01:37<03:07,  4.57it/s]\u001b[A\n",
      "Evaluating:  34%|███▍      | 448/1304 [01:37<03:07,  4.57it/s]\u001b[A\n",
      "Evaluating:  34%|███▍      | 449/1304 [01:38<03:06,  4.57it/s]\u001b[A\n",
      "Evaluating:  35%|███▍      | 450/1304 [01:38<03:06,  4.57it/s]\u001b[A\n",
      "Evaluating:  35%|███▍      | 451/1304 [01:38<03:06,  4.57it/s]\u001b[A\n",
      "Evaluating:  35%|███▍      | 452/1304 [01:38<03:06,  4.57it/s]\u001b[A\n",
      "Evaluating:  35%|███▍      | 453/1304 [01:38<03:06,  4.57it/s]\u001b[A\n",
      "Evaluating:  35%|███▍      | 454/1304 [01:39<03:05,  4.57it/s]\u001b[A\n",
      "Evaluating:  35%|███▍      | 455/1304 [01:39<03:05,  4.58it/s]\u001b[A\n",
      "Evaluating:  35%|███▍      | 456/1304 [01:39<03:05,  4.58it/s]\u001b[A\n",
      "Evaluating:  35%|███▌      | 457/1304 [01:39<03:05,  4.58it/s]\u001b[A\n",
      "Evaluating:  35%|███▌      | 458/1304 [01:40<03:04,  4.57it/s]\u001b[A\n",
      "Evaluating:  35%|███▌      | 459/1304 [01:40<03:04,  4.58it/s]\u001b[A\n",
      "Evaluating:  35%|███▌      | 460/1304 [01:40<03:04,  4.58it/s]\u001b[A\n",
      "Evaluating:  35%|███▌      | 461/1304 [01:40<03:04,  4.58it/s]\u001b[A\n",
      "Evaluating:  35%|███▌      | 462/1304 [01:40<03:03,  4.58it/s]\u001b[A\n",
      "Evaluating:  36%|███▌      | 463/1304 [01:41<03:03,  4.58it/s]\u001b[A\n",
      "Evaluating:  36%|███▌      | 464/1304 [01:41<03:03,  4.57it/s]\u001b[A\n",
      "Evaluating:  36%|███▌      | 465/1304 [01:41<03:03,  4.57it/s]\u001b[A\n",
      "Evaluating:  36%|███▌      | 466/1304 [01:41<03:03,  4.57it/s]\u001b[A\n",
      "Evaluating:  36%|███▌      | 467/1304 [01:42<03:03,  4.57it/s]\u001b[A\n",
      "Evaluating:  36%|███▌      | 468/1304 [01:42<03:03,  4.57it/s]\u001b[A\n",
      "Evaluating:  36%|███▌      | 469/1304 [01:42<03:02,  4.57it/s]\u001b[A\n",
      "Evaluating:  36%|███▌      | 470/1304 [01:42<03:02,  4.57it/s]\u001b[A\n",
      "Evaluating:  36%|███▌      | 471/1304 [01:42<03:02,  4.57it/s]\u001b[A\n",
      "Evaluating:  36%|███▌      | 472/1304 [01:43<03:02,  4.57it/s]\u001b[A\n",
      "Evaluating:  36%|███▋      | 473/1304 [01:43<03:01,  4.57it/s]\u001b[A\n",
      "Evaluating:  36%|███▋      | 474/1304 [01:43<03:01,  4.57it/s]\u001b[A\n",
      "Evaluating:  36%|███▋      | 475/1304 [01:43<03:01,  4.57it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 476/1304 [01:43<03:01,  4.57it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 477/1304 [01:44<03:01,  4.57it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 478/1304 [01:44<03:00,  4.57it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 479/1304 [01:44<03:00,  4.57it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 480/1304 [01:44<03:00,  4.58it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 481/1304 [01:45<02:59,  4.58it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 482/1304 [01:45<02:59,  4.57it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 483/1304 [01:45<02:59,  4.57it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 484/1304 [01:45<02:59,  4.58it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 485/1304 [01:45<02:58,  4.58it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 486/1304 [01:46<02:58,  4.57it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 487/1304 [01:46<02:58,  4.58it/s]\u001b[A\n",
      "Evaluating:  37%|███▋      | 488/1304 [01:46<02:58,  4.58it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 489/1304 [01:46<02:58,  4.57it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 490/1304 [01:47<02:58,  4.57it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 491/1304 [01:47<02:57,  4.57it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 492/1304 [01:47<02:57,  4.57it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 493/1304 [01:47<02:57,  4.57it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 494/1304 [01:47<02:57,  4.57it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 495/1304 [01:48<02:56,  4.58it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 496/1304 [01:48<02:56,  4.57it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 497/1304 [01:48<02:56,  4.57it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 498/1304 [01:48<02:56,  4.57it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 499/1304 [01:49<02:56,  4.57it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 500/1304 [01:49<02:55,  4.57it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 501/1304 [01:49<02:55,  4.57it/s]\u001b[A\n",
      "Evaluating:  38%|███▊      | 502/1304 [01:49<02:56,  4.55it/s]\u001b[A\n",
      "Evaluating:  39%|███▊      | 503/1304 [01:49<02:55,  4.56it/s]\u001b[A\n",
      "Evaluating:  39%|███▊      | 504/1304 [01:50<02:55,  4.56it/s]\u001b[A\n",
      "Evaluating:  39%|███▊      | 505/1304 [01:50<02:55,  4.56it/s]\u001b[A\n",
      "Evaluating:  39%|███▉      | 506/1304 [01:50<02:54,  4.56it/s]\u001b[A\n",
      "Evaluating:  39%|███▉      | 507/1304 [01:50<02:54,  4.57it/s]\u001b[A\n",
      "Evaluating:  39%|███▉      | 508/1304 [01:50<02:53,  4.58it/s]\u001b[A\n",
      "Evaluating:  39%|███▉      | 509/1304 [01:51<02:53,  4.57it/s]\u001b[A\n",
      "Evaluating:  39%|███▉      | 510/1304 [01:51<02:53,  4.57it/s]\u001b[A\n",
      "Evaluating:  39%|███▉      | 511/1304 [01:51<02:53,  4.58it/s]\u001b[A\n",
      "Evaluating:  39%|███▉      | 512/1304 [01:51<02:53,  4.57it/s]\u001b[A\n",
      "Evaluating:  39%|███▉      | 513/1304 [01:52<02:53,  4.57it/s]\u001b[A\n",
      "Evaluating:  39%|███▉      | 514/1304 [01:52<02:52,  4.57it/s]\u001b[A\n",
      "Evaluating:  39%|███▉      | 515/1304 [01:52<02:52,  4.57it/s]\u001b[A\n",
      "Evaluating:  40%|███▉      | 516/1304 [01:52<02:52,  4.57it/s]\u001b[A\n",
      "Evaluating:  40%|███▉      | 517/1304 [01:52<02:52,  4.57it/s]\u001b[A\n",
      "Evaluating:  40%|███▉      | 518/1304 [01:53<02:52,  4.57it/s]\u001b[A\n",
      "Evaluating:  40%|███▉      | 519/1304 [01:53<02:51,  4.57it/s]\u001b[A\n",
      "Evaluating:  40%|███▉      | 520/1304 [01:53<02:51,  4.57it/s]\u001b[A\n",
      "Evaluating:  40%|███▉      | 521/1304 [01:53<02:51,  4.57it/s]\u001b[A\n",
      "Evaluating:  40%|████      | 522/1304 [01:54<02:51,  4.57it/s]\u001b[A\n",
      "Evaluating:  40%|████      | 523/1304 [01:54<02:50,  4.57it/s]\u001b[A\n",
      "Evaluating:  40%|████      | 524/1304 [01:54<02:50,  4.57it/s]\u001b[A\n",
      "Evaluating:  40%|████      | 525/1304 [01:54<02:50,  4.57it/s]\u001b[A\n",
      "Evaluating:  40%|████      | 526/1304 [01:54<02:50,  4.57it/s]\u001b[A\n",
      "Evaluating:  40%|████      | 527/1304 [01:55<02:49,  4.57it/s]\u001b[A\n",
      "Evaluating:  40%|████      | 528/1304 [01:55<02:49,  4.57it/s]\u001b[A\n",
      "Evaluating:  41%|████      | 529/1304 [01:55<02:49,  4.56it/s]\u001b[A\n",
      "Evaluating:  41%|████      | 530/1304 [01:55<02:49,  4.57it/s]\u001b[A\n",
      "Evaluating:  41%|████      | 531/1304 [01:56<02:49,  4.57it/s]\u001b[A\n",
      "Evaluating:  41%|████      | 532/1304 [01:56<02:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  41%|████      | 533/1304 [01:56<02:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  41%|████      | 534/1304 [01:56<02:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  41%|████      | 535/1304 [01:56<02:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  41%|████      | 536/1304 [01:57<02:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  41%|████      | 537/1304 [01:57<02:47,  4.57it/s]\u001b[A\n",
      "Evaluating:  41%|████▏     | 538/1304 [01:57<02:47,  4.58it/s]\u001b[A\n",
      "Evaluating:  41%|████▏     | 539/1304 [01:57<02:47,  4.57it/s]\u001b[A\n",
      "Evaluating:  41%|████▏     | 540/1304 [01:57<02:47,  4.57it/s]\u001b[A\n",
      "Evaluating:  41%|████▏     | 541/1304 [01:58<02:47,  4.57it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 542/1304 [01:58<02:46,  4.57it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 543/1304 [01:58<02:46,  4.58it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 544/1304 [01:58<02:46,  4.57it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 545/1304 [01:59<02:46,  4.57it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 546/1304 [01:59<02:45,  4.57it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 547/1304 [01:59<02:45,  4.57it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 548/1304 [01:59<02:45,  4.57it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 549/1304 [01:59<02:45,  4.58it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 550/1304 [02:00<02:44,  4.57it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 551/1304 [02:00<02:44,  4.57it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 552/1304 [02:00<02:44,  4.57it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 553/1304 [02:00<02:44,  4.58it/s]\u001b[A\n",
      "Evaluating:  42%|████▏     | 554/1304 [02:01<02:43,  4.58it/s]\u001b[A\n",
      "Evaluating:  43%|████▎     | 555/1304 [02:01<02:43,  4.58it/s]\u001b[A\n",
      "Evaluating:  43%|████▎     | 556/1304 [02:01<02:43,  4.58it/s]\u001b[A\n",
      "Evaluating:  43%|████▎     | 557/1304 [02:01<02:43,  4.57it/s]\u001b[A\n",
      "Evaluating:  43%|████▎     | 558/1304 [02:01<02:43,  4.57it/s]\u001b[A\n",
      "Evaluating:  43%|████▎     | 559/1304 [02:02<02:43,  4.57it/s]\u001b[A\n",
      "Evaluating:  43%|████▎     | 560/1304 [02:02<02:42,  4.57it/s]\u001b[A\n",
      "Evaluating:  43%|████▎     | 561/1304 [02:02<02:42,  4.57it/s]\u001b[A\n",
      "Evaluating:  43%|████▎     | 562/1304 [02:02<02:42,  4.56it/s]\u001b[A\n",
      "Evaluating:  43%|████▎     | 563/1304 [02:03<02:42,  4.56it/s]\u001b[A\n",
      "Evaluating:  43%|████▎     | 564/1304 [02:03<02:42,  4.56it/s]\u001b[A\n",
      "Evaluating:  43%|████▎     | 565/1304 [02:03<02:41,  4.56it/s]\u001b[A\n",
      "Evaluating:  43%|████▎     | 566/1304 [02:03<02:41,  4.56it/s]\u001b[A\n",
      "Evaluating:  43%|████▎     | 567/1304 [02:03<02:41,  4.57it/s]\u001b[A\n",
      "Evaluating:  44%|████▎     | 568/1304 [02:04<02:41,  4.57it/s]\u001b[A\n",
      "Evaluating:  44%|████▎     | 569/1304 [02:04<02:40,  4.57it/s]\u001b[A\n",
      "Evaluating:  44%|████▎     | 570/1304 [02:04<02:40,  4.57it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 571/1304 [02:04<02:40,  4.57it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 572/1304 [02:04<02:39,  4.58it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 573/1304 [02:05<02:39,  4.57it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 574/1304 [02:05<02:39,  4.58it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 575/1304 [02:05<02:39,  4.58it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 576/1304 [02:05<02:39,  4.57it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 577/1304 [02:06<02:39,  4.57it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 578/1304 [02:06<02:38,  4.57it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 579/1304 [02:06<02:38,  4.57it/s]\u001b[A\n",
      "Evaluating:  44%|████▍     | 580/1304 [02:06<02:38,  4.57it/s]\u001b[A\n",
      "Evaluating:  45%|████▍     | 581/1304 [02:06<02:38,  4.57it/s]\u001b[A\n",
      "Evaluating:  45%|████▍     | 582/1304 [02:07<02:37,  4.57it/s]\u001b[A\n",
      "Evaluating:  45%|████▍     | 583/1304 [02:07<02:37,  4.57it/s]\u001b[A\n",
      "Evaluating:  45%|████▍     | 584/1304 [02:07<02:37,  4.57it/s]\u001b[A\n",
      "Evaluating:  45%|████▍     | 585/1304 [02:07<02:37,  4.57it/s]\u001b[A\n",
      "Evaluating:  45%|████▍     | 586/1304 [02:08<02:37,  4.57it/s]\u001b[A\n",
      "Evaluating:  45%|████▌     | 587/1304 [02:08<02:36,  4.57it/s]\u001b[A\n",
      "Evaluating:  45%|████▌     | 588/1304 [02:08<02:36,  4.58it/s]\u001b[A\n",
      "Evaluating:  45%|████▌     | 589/1304 [02:08<02:36,  4.58it/s]\u001b[A\n",
      "Evaluating:  45%|████▌     | 590/1304 [02:08<02:36,  4.57it/s]\u001b[A\n",
      "Evaluating:  45%|████▌     | 591/1304 [02:09<02:35,  4.57it/s]\u001b[A\n",
      "Evaluating:  45%|████▌     | 592/1304 [02:09<02:35,  4.58it/s]\u001b[A\n",
      "Evaluating:  45%|████▌     | 593/1304 [02:09<02:35,  4.58it/s]\u001b[A\n",
      "Evaluating:  46%|████▌     | 594/1304 [02:09<02:35,  4.58it/s]\u001b[A\n",
      "Evaluating:  46%|████▌     | 595/1304 [02:10<02:34,  4.58it/s]\u001b[A\n",
      "Evaluating:  46%|████▌     | 596/1304 [02:10<02:34,  4.57it/s]\u001b[A\n",
      "Evaluating:  46%|████▌     | 597/1304 [02:10<02:34,  4.57it/s]\u001b[A\n",
      "Evaluating:  46%|████▌     | 598/1304 [02:10<02:34,  4.57it/s]\u001b[A\n",
      "Evaluating:  46%|████▌     | 599/1304 [02:10<02:33,  4.58it/s]\u001b[A\n",
      "Evaluating:  46%|████▌     | 600/1304 [02:11<02:33,  4.57it/s]\u001b[A\n",
      "Evaluating:  46%|████▌     | 601/1304 [02:11<02:33,  4.57it/s]\u001b[A\n",
      "Evaluating:  46%|████▌     | 602/1304 [02:11<02:33,  4.57it/s]\u001b[A\n",
      "Evaluating:  46%|████▌     | 603/1304 [02:11<02:33,  4.57it/s]\u001b[A\n",
      "Evaluating:  46%|████▋     | 604/1304 [02:11<02:33,  4.57it/s]\u001b[A\n",
      "Evaluating:  46%|████▋     | 605/1304 [02:12<02:32,  4.57it/s]\u001b[A\n",
      "Evaluating:  46%|████▋     | 606/1304 [02:12<02:32,  4.57it/s]\u001b[A\n",
      "Evaluating:  47%|████▋     | 607/1304 [02:12<02:32,  4.57it/s]\u001b[A\n",
      "Evaluating:  47%|████▋     | 608/1304 [02:12<02:32,  4.57it/s]\u001b[A\n",
      "Evaluating:  47%|████▋     | 609/1304 [02:13<02:32,  4.57it/s]\u001b[A\n",
      "Evaluating:  47%|████▋     | 610/1304 [02:13<02:31,  4.57it/s]\u001b[A\n",
      "Evaluating:  47%|████▋     | 611/1304 [02:13<02:31,  4.57it/s]\u001b[A\n",
      "Evaluating:  47%|████▋     | 612/1304 [02:13<02:31,  4.57it/s]\u001b[A\n",
      "Evaluating:  47%|████▋     | 613/1304 [02:13<02:30,  4.58it/s]\u001b[A\n",
      "Evaluating:  47%|████▋     | 614/1304 [02:14<02:30,  4.57it/s]\u001b[A\n",
      "Evaluating:  47%|████▋     | 615/1304 [02:14<02:30,  4.57it/s]\u001b[A\n",
      "Evaluating:  47%|████▋     | 616/1304 [02:14<02:30,  4.57it/s]\u001b[A\n",
      "Evaluating:  47%|████▋     | 617/1304 [02:14<02:30,  4.57it/s]\u001b[A\n",
      "Evaluating:  47%|████▋     | 618/1304 [02:15<02:29,  4.58it/s]\u001b[A\n",
      "Evaluating:  47%|████▋     | 619/1304 [02:15<02:29,  4.57it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 620/1304 [02:15<02:29,  4.58it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 621/1304 [02:15<02:29,  4.57it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 622/1304 [02:15<02:29,  4.57it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 623/1304 [02:16<02:29,  4.57it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 624/1304 [02:16<02:28,  4.57it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 625/1304 [02:16<02:28,  4.58it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 626/1304 [02:16<02:27,  4.59it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 627/1304 [02:17<02:27,  4.59it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 628/1304 [02:17<02:27,  4.58it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 629/1304 [02:17<02:27,  4.58it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 630/1304 [02:17<02:27,  4.57it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 631/1304 [02:17<02:27,  4.57it/s]\u001b[A\n",
      "Evaluating:  48%|████▊     | 632/1304 [02:18<02:26,  4.57it/s]\u001b[A\n",
      "Evaluating:  49%|████▊     | 633/1304 [02:18<02:26,  4.58it/s]\u001b[A\n",
      "Evaluating:  49%|████▊     | 634/1304 [02:18<02:26,  4.57it/s]\u001b[A\n",
      "Evaluating:  49%|████▊     | 635/1304 [02:18<02:26,  4.58it/s]\u001b[A\n",
      "Evaluating:  49%|████▉     | 636/1304 [02:18<02:25,  4.58it/s]\u001b[A\n",
      "Evaluating:  49%|████▉     | 637/1304 [02:19<02:25,  4.57it/s]\u001b[A\n",
      "Evaluating:  49%|████▉     | 638/1304 [02:19<02:25,  4.57it/s]\u001b[A\n",
      "Evaluating:  49%|████▉     | 639/1304 [02:19<02:25,  4.57it/s]\u001b[A\n",
      "Evaluating:  49%|████▉     | 640/1304 [02:19<02:25,  4.57it/s]\u001b[A\n",
      "Evaluating:  49%|████▉     | 641/1304 [02:20<02:24,  4.57it/s]\u001b[A\n",
      "Evaluating:  49%|████▉     | 642/1304 [02:20<02:24,  4.58it/s]\u001b[A\n",
      "Evaluating:  49%|████▉     | 643/1304 [02:20<02:24,  4.57it/s]\u001b[A\n",
      "Evaluating:  49%|████▉     | 644/1304 [02:20<02:24,  4.57it/s]\u001b[A\n",
      "Evaluating:  49%|████▉     | 645/1304 [02:20<02:23,  4.58it/s]\u001b[A\n",
      "Evaluating:  50%|████▉     | 646/1304 [02:21<02:23,  4.57it/s]\u001b[A\n",
      "Evaluating:  50%|████▉     | 647/1304 [02:21<02:23,  4.57it/s]\u001b[A\n",
      "Evaluating:  50%|████▉     | 648/1304 [02:21<02:23,  4.58it/s]\u001b[A\n",
      "Evaluating:  50%|████▉     | 649/1304 [02:21<02:22,  4.58it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|████▉     | 650/1304 [02:22<02:22,  4.59it/s]\u001b[A\n",
      "Evaluating:  50%|████▉     | 651/1304 [02:22<02:22,  4.58it/s]\u001b[A\n",
      "Evaluating:  50%|█████     | 652/1304 [02:22<02:22,  4.58it/s]\u001b[A\n",
      "Evaluating:  50%|█████     | 653/1304 [02:22<02:22,  4.58it/s]\u001b[A\n",
      "Evaluating:  50%|█████     | 654/1304 [02:22<02:22,  4.58it/s]\u001b[A\n",
      "Evaluating:  50%|█████     | 655/1304 [02:23<02:21,  4.58it/s]\u001b[A\n",
      "Evaluating:  50%|█████     | 656/1304 [02:23<02:21,  4.57it/s]\u001b[A\n",
      "Evaluating:  50%|█████     | 657/1304 [02:23<02:21,  4.58it/s]\u001b[A\n",
      "Evaluating:  50%|█████     | 658/1304 [02:23<02:21,  4.58it/s]\u001b[A\n",
      "Evaluating:  51%|█████     | 659/1304 [02:23<02:21,  4.57it/s]\u001b[A\n",
      "Evaluating:  51%|█████     | 660/1304 [02:24<02:20,  4.57it/s]\u001b[A\n",
      "Evaluating:  51%|█████     | 661/1304 [02:24<02:20,  4.58it/s]\u001b[A\n",
      "Evaluating:  51%|█████     | 662/1304 [02:24<02:20,  4.58it/s]\u001b[A\n",
      "Evaluating:  51%|█████     | 663/1304 [02:24<02:20,  4.57it/s]\u001b[A\n",
      "Evaluating:  51%|█████     | 664/1304 [02:25<02:19,  4.58it/s]\u001b[A\n",
      "Evaluating:  51%|█████     | 665/1304 [02:25<02:19,  4.58it/s]\u001b[A\n",
      "Evaluating:  51%|█████     | 666/1304 [02:25<02:19,  4.58it/s]\u001b[A\n",
      "Evaluating:  51%|█████     | 667/1304 [02:25<02:19,  4.57it/s]\u001b[A\n",
      "Evaluating:  51%|█████     | 668/1304 [02:25<02:19,  4.57it/s]\u001b[A\n",
      "Evaluating:  51%|█████▏    | 669/1304 [02:26<02:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  51%|█████▏    | 670/1304 [02:26<02:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  51%|█████▏    | 671/1304 [02:26<02:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 672/1304 [02:26<02:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 673/1304 [02:27<02:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 674/1304 [02:27<02:17,  4.57it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 675/1304 [02:27<02:17,  4.57it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 676/1304 [02:27<02:17,  4.57it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 677/1304 [02:27<02:17,  4.57it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 678/1304 [02:28<02:17,  4.57it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 679/1304 [02:28<02:16,  4.57it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 680/1304 [02:28<02:16,  4.58it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 681/1304 [02:28<02:16,  4.58it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 682/1304 [02:29<02:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 683/1304 [02:29<02:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  52%|█████▏    | 684/1304 [02:29<02:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  53%|█████▎    | 685/1304 [02:29<02:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  53%|█████▎    | 686/1304 [02:29<02:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  53%|█████▎    | 687/1304 [02:30<02:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  53%|█████▎    | 688/1304 [02:30<02:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  53%|█████▎    | 689/1304 [02:30<02:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  53%|█████▎    | 690/1304 [02:30<02:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  53%|█████▎    | 691/1304 [02:30<02:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  53%|█████▎    | 692/1304 [02:31<02:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  53%|█████▎    | 693/1304 [02:31<02:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  53%|█████▎    | 694/1304 [02:31<02:13,  4.58it/s]\u001b[A\n",
      "Evaluating:  53%|█████▎    | 695/1304 [02:31<02:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  53%|█████▎    | 696/1304 [02:32<02:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  53%|█████▎    | 697/1304 [02:32<02:12,  4.57it/s]\u001b[A\n",
      "Evaluating:  54%|█████▎    | 698/1304 [02:32<02:12,  4.57it/s]\u001b[A\n",
      "Evaluating:  54%|█████▎    | 699/1304 [02:32<02:12,  4.57it/s]\u001b[A\n",
      "Evaluating:  54%|█████▎    | 700/1304 [02:32<02:12,  4.57it/s]\u001b[A\n",
      "Evaluating:  54%|█████▍    | 701/1304 [02:33<02:11,  4.57it/s]\u001b[A\n",
      "Evaluating:  54%|█████▍    | 702/1304 [02:33<02:11,  4.57it/s]\u001b[A\n",
      "Evaluating:  54%|█████▍    | 703/1304 [02:33<02:11,  4.57it/s]\u001b[A\n",
      "Evaluating:  54%|█████▍    | 704/1304 [02:33<02:11,  4.57it/s]\u001b[A\n",
      "Evaluating:  54%|█████▍    | 705/1304 [02:34<02:11,  4.57it/s]\u001b[A\n",
      "Evaluating:  54%|█████▍    | 706/1304 [02:34<02:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  54%|█████▍    | 707/1304 [02:34<02:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  54%|█████▍    | 708/1304 [02:34<02:10,  4.58it/s]\u001b[A\n",
      "Evaluating:  54%|█████▍    | 709/1304 [02:34<02:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  54%|█████▍    | 710/1304 [02:35<02:09,  4.57it/s]\u001b[A\n",
      "Evaluating:  55%|█████▍    | 711/1304 [02:35<02:09,  4.58it/s]\u001b[A\n",
      "Evaluating:  55%|█████▍    | 712/1304 [02:35<02:09,  4.58it/s]\u001b[A\n",
      "Evaluating:  55%|█████▍    | 713/1304 [02:35<02:09,  4.57it/s]\u001b[A\n",
      "Evaluating:  55%|█████▍    | 714/1304 [02:36<02:09,  4.57it/s]\u001b[A\n",
      "Evaluating:  55%|█████▍    | 715/1304 [02:36<02:08,  4.58it/s]\u001b[A\n",
      "Evaluating:  55%|█████▍    | 716/1304 [02:36<02:08,  4.57it/s]\u001b[A\n",
      "Evaluating:  55%|█████▍    | 717/1304 [02:36<02:08,  4.57it/s]\u001b[A\n",
      "Evaluating:  55%|█████▌    | 718/1304 [02:36<02:08,  4.57it/s]\u001b[A\n",
      "Evaluating:  55%|█████▌    | 719/1304 [02:37<02:08,  4.57it/s]\u001b[A\n",
      "Evaluating:  55%|█████▌    | 720/1304 [02:37<02:07,  4.57it/s]\u001b[A\n",
      "Evaluating:  55%|█████▌    | 721/1304 [02:37<02:07,  4.58it/s]\u001b[A\n",
      "Evaluating:  55%|█████▌    | 722/1304 [02:37<02:07,  4.57it/s]\u001b[A\n",
      "Evaluating:  55%|█████▌    | 723/1304 [02:37<02:07,  4.57it/s]\u001b[A\n",
      "Evaluating:  56%|█████▌    | 724/1304 [02:38<02:06,  4.58it/s]\u001b[A\n",
      "Evaluating:  56%|█████▌    | 725/1304 [02:38<02:06,  4.57it/s]\u001b[A\n",
      "Evaluating:  56%|█████▌    | 726/1304 [02:38<02:06,  4.57it/s]\u001b[A\n",
      "Evaluating:  56%|█████▌    | 727/1304 [02:38<02:06,  4.58it/s]\u001b[A\n",
      "Evaluating:  56%|█████▌    | 728/1304 [02:39<02:05,  4.58it/s]\u001b[A\n",
      "Evaluating:  56%|█████▌    | 729/1304 [02:39<02:05,  4.57it/s]\u001b[A\n",
      "Evaluating:  56%|█████▌    | 730/1304 [02:39<02:05,  4.57it/s]\u001b[A\n",
      "Evaluating:  56%|█████▌    | 731/1304 [02:39<02:05,  4.57it/s]\u001b[A\n",
      "Evaluating:  56%|█████▌    | 732/1304 [02:39<02:05,  4.57it/s]\u001b[A\n",
      "Evaluating:  56%|█████▌    | 733/1304 [02:40<02:05,  4.57it/s]\u001b[A\n",
      "Evaluating:  56%|█████▋    | 734/1304 [02:40<02:04,  4.57it/s]\u001b[A\n",
      "Evaluating:  56%|█████▋    | 735/1304 [02:40<02:04,  4.57it/s]\u001b[A\n",
      "Evaluating:  56%|█████▋    | 736/1304 [02:40<02:04,  4.57it/s]\u001b[A\n",
      "Evaluating:  57%|█████▋    | 737/1304 [02:41<02:04,  4.57it/s]\u001b[A\n",
      "Evaluating:  57%|█████▋    | 738/1304 [02:41<02:04,  4.55it/s]\u001b[A\n",
      "Evaluating:  57%|█████▋    | 739/1304 [02:41<02:03,  4.56it/s]\u001b[A\n",
      "Evaluating:  57%|█████▋    | 740/1304 [02:41<02:03,  4.57it/s]\u001b[A\n",
      "Evaluating:  57%|█████▋    | 741/1304 [02:41<02:03,  4.57it/s]\u001b[A\n",
      "Evaluating:  57%|█████▋    | 742/1304 [02:42<02:02,  4.57it/s]\u001b[A\n",
      "Evaluating:  57%|█████▋    | 743/1304 [02:42<02:02,  4.57it/s]\u001b[A\n",
      "Evaluating:  57%|█████▋    | 744/1304 [02:42<02:02,  4.57it/s]\u001b[A\n",
      "Evaluating:  57%|█████▋    | 745/1304 [02:42<02:02,  4.58it/s]\u001b[A\n",
      "Evaluating:  57%|█████▋    | 746/1304 [02:43<02:01,  4.58it/s]\u001b[A\n",
      "Evaluating:  57%|█████▋    | 747/1304 [02:43<02:01,  4.58it/s]\u001b[A\n",
      "Evaluating:  57%|█████▋    | 748/1304 [02:43<02:01,  4.57it/s]\u001b[A\n",
      "Evaluating:  57%|█████▋    | 749/1304 [02:43<02:01,  4.58it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 750/1304 [02:43<02:01,  4.58it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 751/1304 [02:44<02:00,  4.57it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 752/1304 [02:44<02:00,  4.58it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 753/1304 [02:44<02:00,  4.57it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 754/1304 [02:44<02:00,  4.57it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 755/1304 [02:44<01:59,  4.58it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 756/1304 [02:45<01:59,  4.58it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 757/1304 [02:45<01:59,  4.58it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 758/1304 [02:45<01:59,  4.58it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 759/1304 [02:45<01:59,  4.57it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 760/1304 [02:46<01:59,  4.57it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 761/1304 [02:46<01:58,  4.57it/s]\u001b[A\n",
      "Evaluating:  58%|█████▊    | 762/1304 [02:46<01:58,  4.57it/s]\u001b[A\n",
      "Evaluating:  59%|█████▊    | 763/1304 [02:46<01:58,  4.57it/s]\u001b[A\n",
      "Evaluating:  59%|█████▊    | 764/1304 [02:46<01:58,  4.57it/s]\u001b[A\n",
      "Evaluating:  59%|█████▊    | 765/1304 [02:47<01:57,  4.57it/s]\u001b[A\n",
      "Evaluating:  59%|█████▊    | 766/1304 [02:47<01:57,  4.57it/s]\u001b[A\n",
      "Evaluating:  59%|█████▉    | 767/1304 [02:47<01:57,  4.57it/s]\u001b[A\n",
      "Evaluating:  59%|█████▉    | 768/1304 [02:47<01:57,  4.57it/s]\u001b[A\n",
      "Evaluating:  59%|█████▉    | 769/1304 [02:48<01:57,  4.57it/s]\u001b[A\n",
      "Evaluating:  59%|█████▉    | 770/1304 [02:48<01:56,  4.57it/s]\u001b[A\n",
      "Evaluating:  59%|█████▉    | 771/1304 [02:48<01:56,  4.57it/s]\u001b[A\n",
      "Evaluating:  59%|█████▉    | 772/1304 [02:48<01:56,  4.57it/s]\u001b[A\n",
      "Evaluating:  59%|█████▉    | 773/1304 [02:48<01:56,  4.57it/s]\u001b[A\n",
      "Evaluating:  59%|█████▉    | 774/1304 [02:49<01:55,  4.58it/s]\u001b[A\n",
      "Evaluating:  59%|█████▉    | 775/1304 [02:49<01:55,  4.58it/s]\u001b[A\n",
      "Evaluating:  60%|█████▉    | 776/1304 [02:49<01:55,  4.57it/s]\u001b[A\n",
      "Evaluating:  60%|█████▉    | 777/1304 [02:49<01:55,  4.57it/s]\u001b[A\n",
      "Evaluating:  60%|█████▉    | 778/1304 [02:50<01:55,  4.57it/s]\u001b[A\n",
      "Evaluating:  60%|█████▉    | 779/1304 [02:50<01:54,  4.58it/s]\u001b[A\n",
      "Evaluating:  60%|█████▉    | 780/1304 [02:50<01:54,  4.57it/s]\u001b[A\n",
      "Evaluating:  60%|█████▉    | 781/1304 [02:50<01:54,  4.57it/s]\u001b[A\n",
      "Evaluating:  60%|█████▉    | 782/1304 [02:50<01:54,  4.57it/s]\u001b[A\n",
      "Evaluating:  60%|██████    | 783/1304 [02:51<01:54,  4.57it/s]\u001b[A\n",
      "Evaluating:  60%|██████    | 784/1304 [02:51<01:53,  4.57it/s]\u001b[A\n",
      "Evaluating:  60%|██████    | 785/1304 [02:51<01:53,  4.57it/s]\u001b[A\n",
      "Evaluating:  60%|██████    | 786/1304 [02:51<01:53,  4.58it/s]\u001b[A\n",
      "Evaluating:  60%|██████    | 787/1304 [02:51<01:52,  4.58it/s]\u001b[A\n",
      "Evaluating:  60%|██████    | 788/1304 [02:52<01:52,  4.57it/s]\u001b[A\n",
      "Evaluating:  61%|██████    | 789/1304 [02:52<01:52,  4.57it/s]\u001b[A\n",
      "Evaluating:  61%|██████    | 790/1304 [02:52<01:52,  4.57it/s]\u001b[A\n",
      "Evaluating:  61%|██████    | 791/1304 [02:52<01:52,  4.57it/s]\u001b[A\n",
      "Evaluating:  61%|██████    | 792/1304 [02:53<01:52,  4.57it/s]\u001b[A\n",
      "Evaluating:  61%|██████    | 793/1304 [02:53<01:51,  4.57it/s]\u001b[A\n",
      "Evaluating:  61%|██████    | 794/1304 [02:53<01:51,  4.57it/s]\u001b[A\n",
      "Evaluating:  61%|██████    | 795/1304 [02:53<01:51,  4.57it/s]\u001b[A\n",
      "Evaluating:  61%|██████    | 796/1304 [02:53<01:51,  4.57it/s]\u001b[A\n",
      "Evaluating:  61%|██████    | 797/1304 [02:54<01:51,  4.57it/s]\u001b[A\n",
      "Evaluating:  61%|██████    | 798/1304 [02:54<01:50,  4.57it/s]\u001b[A\n",
      "Evaluating:  61%|██████▏   | 799/1304 [02:54<01:50,  4.57it/s]\u001b[A\n",
      "Evaluating:  61%|██████▏   | 800/1304 [02:54<01:50,  4.57it/s]\u001b[A\n",
      "Evaluating:  61%|██████▏   | 801/1304 [02:55<01:50,  4.56it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 802/1304 [02:55<01:50,  4.56it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 803/1304 [02:55<01:49,  4.57it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 804/1304 [02:55<01:49,  4.57it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 805/1304 [02:55<01:49,  4.57it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 806/1304 [02:56<01:49,  4.57it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 807/1304 [02:56<01:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 808/1304 [02:56<01:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 809/1304 [02:56<01:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 810/1304 [02:57<01:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 811/1304 [02:57<01:47,  4.57it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 812/1304 [02:57<01:47,  4.57it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 813/1304 [02:57<01:47,  4.57it/s]\u001b[A\n",
      "Evaluating:  62%|██████▏   | 814/1304 [02:57<01:47,  4.58it/s]\u001b[A\n",
      "Evaluating:  62%|██████▎   | 815/1304 [02:58<01:46,  4.58it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 816/1304 [02:58<01:46,  4.57it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 817/1304 [02:58<01:46,  4.57it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 818/1304 [02:58<01:46,  4.57it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 819/1304 [02:58<01:46,  4.57it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 820/1304 [02:59<01:45,  4.57it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 821/1304 [02:59<01:45,  4.57it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 822/1304 [02:59<01:45,  4.57it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 823/1304 [02:59<01:45,  4.57it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 824/1304 [03:00<01:45,  4.57it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 825/1304 [03:00<01:44,  4.57it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 826/1304 [03:00<01:44,  4.57it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 827/1304 [03:00<01:44,  4.58it/s]\u001b[A\n",
      "Evaluating:  63%|██████▎   | 828/1304 [03:00<01:44,  4.57it/s]\u001b[A\n",
      "Evaluating:  64%|██████▎   | 829/1304 [03:01<01:43,  4.57it/s]\u001b[A\n",
      "Evaluating:  64%|██████▎   | 830/1304 [03:01<01:43,  4.57it/s]\u001b[A\n",
      "Evaluating:  64%|██████▎   | 831/1304 [03:01<01:43,  4.57it/s]\u001b[A\n",
      "Evaluating:  64%|██████▍   | 832/1304 [03:01<01:43,  4.57it/s]\u001b[A\n",
      "Evaluating:  64%|██████▍   | 833/1304 [03:02<01:43,  4.57it/s]\u001b[A\n",
      "Evaluating:  64%|██████▍   | 834/1304 [03:02<01:42,  4.57it/s]\u001b[A\n",
      "Evaluating:  64%|██████▍   | 835/1304 [03:02<01:42,  4.57it/s]\u001b[A\n",
      "Evaluating:  64%|██████▍   | 836/1304 [03:02<01:42,  4.57it/s]\u001b[A\n",
      "Evaluating:  64%|██████▍   | 837/1304 [03:02<01:42,  4.57it/s]\u001b[A\n",
      "Evaluating:  64%|██████▍   | 838/1304 [03:03<01:42,  4.57it/s]\u001b[A\n",
      "Evaluating:  64%|██████▍   | 839/1304 [03:03<01:41,  4.56it/s]\u001b[A\n",
      "Evaluating:  64%|██████▍   | 840/1304 [03:03<01:41,  4.57it/s]\u001b[A\n",
      "Evaluating:  64%|██████▍   | 841/1304 [03:03<01:41,  4.57it/s]\u001b[A\n",
      "Evaluating:  65%|██████▍   | 842/1304 [03:04<01:41,  4.56it/s]\u001b[A\n",
      "Evaluating:  65%|██████▍   | 843/1304 [03:04<01:40,  4.57it/s]\u001b[A\n",
      "Evaluating:  65%|██████▍   | 844/1304 [03:04<01:40,  4.57it/s]\u001b[A\n",
      "Evaluating:  65%|██████▍   | 845/1304 [03:04<01:40,  4.57it/s]\u001b[A\n",
      "Evaluating:  65%|██████▍   | 846/1304 [03:04<01:40,  4.57it/s]\u001b[A\n",
      "Evaluating:  65%|██████▍   | 847/1304 [03:05<01:39,  4.57it/s]\u001b[A\n",
      "Evaluating:  65%|██████▌   | 848/1304 [03:05<01:39,  4.57it/s]\u001b[A\n",
      "Evaluating:  65%|██████▌   | 849/1304 [03:05<01:39,  4.57it/s]\u001b[A\n",
      "Evaluating:  65%|██████▌   | 850/1304 [03:05<01:39,  4.58it/s]\u001b[A\n",
      "Evaluating:  65%|██████▌   | 851/1304 [03:06<01:38,  4.58it/s]\u001b[A\n",
      "Evaluating:  65%|██████▌   | 852/1304 [03:06<01:38,  4.57it/s]\u001b[A\n",
      "Evaluating:  65%|██████▌   | 853/1304 [03:06<01:38,  4.58it/s]\u001b[A\n",
      "Evaluating:  65%|██████▌   | 854/1304 [03:06<01:38,  4.58it/s]\u001b[A\n",
      "Evaluating:  66%|██████▌   | 855/1304 [03:06<01:38,  4.58it/s]\u001b[A\n",
      "Evaluating:  66%|██████▌   | 856/1304 [03:07<01:37,  4.58it/s]\u001b[A\n",
      "Evaluating:  66%|██████▌   | 857/1304 [03:07<01:37,  4.58it/s]\u001b[A\n",
      "Evaluating:  66%|██████▌   | 858/1304 [03:07<01:37,  4.57it/s]\u001b[A\n",
      "Evaluating:  66%|██████▌   | 859/1304 [03:07<01:37,  4.58it/s]\u001b[A\n",
      "Evaluating:  66%|██████▌   | 860/1304 [03:07<01:37,  4.57it/s]\u001b[A\n",
      "Evaluating:  66%|██████▌   | 861/1304 [03:08<01:36,  4.57it/s]\u001b[A\n",
      "Evaluating:  66%|██████▌   | 862/1304 [03:08<01:36,  4.57it/s]\u001b[A\n",
      "Evaluating:  66%|██████▌   | 863/1304 [03:08<01:36,  4.57it/s]\u001b[A\n",
      "Evaluating:  66%|██████▋   | 864/1304 [03:08<01:36,  4.57it/s]\u001b[A\n",
      "Evaluating:  66%|██████▋   | 865/1304 [03:09<01:36,  4.57it/s]\u001b[A\n",
      "Evaluating:  66%|██████▋   | 866/1304 [03:09<01:35,  4.57it/s]\u001b[A\n",
      "Evaluating:  66%|██████▋   | 867/1304 [03:09<01:35,  4.57it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 868/1304 [03:09<01:35,  4.57it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 869/1304 [03:09<01:35,  4.57it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 870/1304 [03:10<01:35,  4.57it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 871/1304 [03:10<01:34,  4.57it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 872/1304 [03:10<01:34,  4.57it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 873/1304 [03:10<01:34,  4.57it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 874/1304 [03:11<01:34,  4.57it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 875/1304 [03:11<01:33,  4.57it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 876/1304 [03:11<01:33,  4.57it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 877/1304 [03:11<01:33,  4.57it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 878/1304 [03:11<01:33,  4.57it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 879/1304 [03:12<01:33,  4.57it/s]\u001b[A\n",
      "Evaluating:  67%|██████▋   | 880/1304 [03:12<01:32,  4.57it/s]\u001b[A\n",
      "Evaluating:  68%|██████▊   | 881/1304 [03:12<01:32,  4.57it/s]\u001b[A\n",
      "Evaluating:  68%|██████▊   | 882/1304 [03:12<01:32,  4.57it/s]\u001b[A\n",
      "Evaluating:  68%|██████▊   | 883/1304 [03:13<01:32,  4.57it/s]\u001b[A\n",
      "Evaluating:  68%|██████▊   | 884/1304 [03:13<01:31,  4.57it/s]\u001b[A\n",
      "Evaluating:  68%|██████▊   | 885/1304 [03:13<01:31,  4.57it/s]\u001b[A\n",
      "Evaluating:  68%|██████▊   | 886/1304 [03:13<01:31,  4.57it/s]\u001b[A\n",
      "Evaluating:  68%|██████▊   | 887/1304 [03:13<01:31,  4.57it/s]\u001b[A\n",
      "Evaluating:  68%|██████▊   | 888/1304 [03:14<01:30,  4.57it/s]\u001b[A\n",
      "Evaluating:  68%|██████▊   | 889/1304 [03:14<01:30,  4.57it/s]\u001b[A\n",
      "Evaluating:  68%|██████▊   | 890/1304 [03:14<01:30,  4.57it/s]\u001b[A\n",
      "Evaluating:  68%|██████▊   | 891/1304 [03:14<01:30,  4.57it/s]\u001b[A\n",
      "Evaluating:  68%|██████▊   | 892/1304 [03:14<01:30,  4.57it/s]\u001b[A\n",
      "Evaluating:  68%|██████▊   | 893/1304 [03:15<01:29,  4.57it/s]\u001b[A\n",
      "Evaluating:  69%|██████▊   | 894/1304 [03:15<01:29,  4.57it/s]\u001b[A\n",
      "Evaluating:  69%|██████▊   | 895/1304 [03:15<01:29,  4.58it/s]\u001b[A\n",
      "Evaluating:  69%|██████▊   | 896/1304 [03:15<01:29,  4.57it/s]\u001b[A\n",
      "Evaluating:  69%|██████▉   | 897/1304 [03:16<01:29,  4.57it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  69%|██████▉   | 898/1304 [03:16<01:28,  4.57it/s]\u001b[A\n",
      "Evaluating:  69%|██████▉   | 899/1304 [03:16<01:28,  4.57it/s]\u001b[A\n",
      "Evaluating:  69%|██████▉   | 900/1304 [03:16<01:28,  4.57it/s]\u001b[A\n",
      "Evaluating:  69%|██████▉   | 901/1304 [03:16<01:28,  4.57it/s]\u001b[A\n",
      "Evaluating:  69%|██████▉   | 902/1304 [03:17<01:27,  4.57it/s]\u001b[A\n",
      "Evaluating:  69%|██████▉   | 903/1304 [03:17<01:27,  4.57it/s]\u001b[A\n",
      "Evaluating:  69%|██████▉   | 904/1304 [03:17<01:27,  4.57it/s]\u001b[A\n",
      "Evaluating:  69%|██████▉   | 905/1304 [03:17<01:27,  4.57it/s]\u001b[A\n",
      "Evaluating:  69%|██████▉   | 906/1304 [03:18<01:27,  4.57it/s]\u001b[A\n",
      "Evaluating:  70%|██████▉   | 907/1304 [03:18<01:26,  4.57it/s]\u001b[A\n",
      "Evaluating:  70%|██████▉   | 908/1304 [03:18<01:26,  4.57it/s]\u001b[A\n",
      "Evaluating:  70%|██████▉   | 909/1304 [03:18<01:26,  4.57it/s]\u001b[A\n",
      "Evaluating:  70%|██████▉   | 910/1304 [03:18<01:26,  4.57it/s]\u001b[A\n",
      "Evaluating:  70%|██████▉   | 911/1304 [03:19<01:25,  4.57it/s]\u001b[A\n",
      "Evaluating:  70%|██████▉   | 912/1304 [03:19<01:25,  4.57it/s]\u001b[A\n",
      "Evaluating:  70%|███████   | 913/1304 [03:19<01:25,  4.57it/s]\u001b[A\n",
      "Evaluating:  70%|███████   | 914/1304 [03:19<01:25,  4.57it/s]\u001b[A\n",
      "Evaluating:  70%|███████   | 915/1304 [03:20<01:24,  4.58it/s]\u001b[A\n",
      "Evaluating:  70%|███████   | 916/1304 [03:20<01:24,  4.58it/s]\u001b[A\n",
      "Evaluating:  70%|███████   | 917/1304 [03:20<01:24,  4.58it/s]\u001b[A\n",
      "Evaluating:  70%|███████   | 918/1304 [03:20<01:24,  4.58it/s]\u001b[A\n",
      "Evaluating:  70%|███████   | 919/1304 [03:20<01:24,  4.57it/s]\u001b[A\n",
      "Evaluating:  71%|███████   | 920/1304 [03:21<01:24,  4.57it/s]\u001b[A\n",
      "Evaluating:  71%|███████   | 921/1304 [03:21<01:23,  4.57it/s]\u001b[A\n",
      "Evaluating:  71%|███████   | 922/1304 [03:21<01:23,  4.57it/s]\u001b[A\n",
      "Evaluating:  71%|███████   | 923/1304 [03:21<01:23,  4.57it/s]\u001b[A\n",
      "Evaluating:  71%|███████   | 924/1304 [03:21<01:23,  4.57it/s]\u001b[A\n",
      "Evaluating:  71%|███████   | 925/1304 [03:22<01:22,  4.58it/s]\u001b[A\n",
      "Evaluating:  71%|███████   | 926/1304 [03:22<01:22,  4.58it/s]\u001b[A\n",
      "Evaluating:  71%|███████   | 927/1304 [03:22<01:22,  4.57it/s]\u001b[A\n",
      "Evaluating:  71%|███████   | 928/1304 [03:22<01:22,  4.58it/s]\u001b[A\n",
      "Evaluating:  71%|███████   | 929/1304 [03:23<01:21,  4.57it/s]\u001b[A\n",
      "Evaluating:  71%|███████▏  | 930/1304 [03:23<01:21,  4.57it/s]\u001b[A\n",
      "Evaluating:  71%|███████▏  | 931/1304 [03:23<01:21,  4.57it/s]\u001b[A\n",
      "Evaluating:  71%|███████▏  | 932/1304 [03:23<01:21,  4.57it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 933/1304 [03:23<01:21,  4.57it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 934/1304 [03:24<01:20,  4.58it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 935/1304 [03:24<01:20,  4.57it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 936/1304 [03:24<01:20,  4.57it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 937/1304 [03:24<01:20,  4.57it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 938/1304 [03:25<01:20,  4.57it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 939/1304 [03:25<01:19,  4.57it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 940/1304 [03:25<01:19,  4.57it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 941/1304 [03:25<01:19,  4.57it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 942/1304 [03:25<01:19,  4.57it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 943/1304 [03:26<01:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 944/1304 [03:26<01:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  72%|███████▏  | 945/1304 [03:26<01:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 946/1304 [03:26<01:18,  4.56it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 947/1304 [03:27<01:18,  4.56it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 948/1304 [03:27<01:17,  4.57it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 949/1304 [03:27<01:17,  4.57it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 950/1304 [03:27<01:17,  4.57it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 951/1304 [03:27<01:17,  4.57it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 952/1304 [03:28<01:16,  4.58it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 953/1304 [03:28<01:16,  4.58it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 954/1304 [03:28<01:16,  4.58it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 955/1304 [03:28<01:16,  4.58it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 956/1304 [03:28<01:15,  4.58it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 957/1304 [03:29<01:15,  4.58it/s]\u001b[A\n",
      "Evaluating:  73%|███████▎  | 958/1304 [03:29<01:15,  4.58it/s]\u001b[A\n",
      "Evaluating:  74%|███████▎  | 959/1304 [03:29<01:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  74%|███████▎  | 960/1304 [03:29<01:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  74%|███████▎  | 961/1304 [03:30<01:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  74%|███████▍  | 962/1304 [03:30<01:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  74%|███████▍  | 963/1304 [03:30<01:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  74%|███████▍  | 964/1304 [03:30<01:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  74%|███████▍  | 965/1304 [03:30<01:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  74%|███████▍  | 966/1304 [03:31<01:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  74%|███████▍  | 967/1304 [03:31<01:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  74%|███████▍  | 968/1304 [03:31<01:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  74%|███████▍  | 969/1304 [03:31<01:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  74%|███████▍  | 970/1304 [03:32<01:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  74%|███████▍  | 971/1304 [03:32<01:12,  4.58it/s]\u001b[A\n",
      "Evaluating:  75%|███████▍  | 972/1304 [03:32<01:12,  4.58it/s]\u001b[A\n",
      "Evaluating:  75%|███████▍  | 973/1304 [03:32<01:12,  4.57it/s]\u001b[A\n",
      "Evaluating:  75%|███████▍  | 974/1304 [03:32<01:12,  4.57it/s]\u001b[A\n",
      "Evaluating:  75%|███████▍  | 975/1304 [03:33<01:11,  4.57it/s]\u001b[A\n",
      "Evaluating:  75%|███████▍  | 976/1304 [03:33<01:11,  4.57it/s]\u001b[A\n",
      "Evaluating:  75%|███████▍  | 977/1304 [03:33<01:11,  4.57it/s]\u001b[A\n",
      "Evaluating:  75%|███████▌  | 978/1304 [03:33<01:11,  4.57it/s]\u001b[A\n",
      "Evaluating:  75%|███████▌  | 979/1304 [03:34<01:11,  4.57it/s]\u001b[A\n",
      "Evaluating:  75%|███████▌  | 980/1304 [03:34<01:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  75%|███████▌  | 981/1304 [03:34<01:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  75%|███████▌  | 982/1304 [03:34<01:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  75%|███████▌  | 983/1304 [03:34<01:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  75%|███████▌  | 984/1304 [03:35<01:09,  4.58it/s]\u001b[A\n",
      "Evaluating:  76%|███████▌  | 985/1304 [03:35<01:09,  4.57it/s]\u001b[A\n",
      "Evaluating:  76%|███████▌  | 986/1304 [03:35<01:09,  4.57it/s]\u001b[A\n",
      "Evaluating:  76%|███████▌  | 987/1304 [03:35<01:09,  4.58it/s]\u001b[A\n",
      "Evaluating:  76%|███████▌  | 988/1304 [03:35<01:09,  4.57it/s]\u001b[A\n",
      "Evaluating:  76%|███████▌  | 989/1304 [03:36<01:09,  4.56it/s]\u001b[A\n",
      "Evaluating:  76%|███████▌  | 990/1304 [03:36<01:08,  4.57it/s]\u001b[A\n",
      "Evaluating:  76%|███████▌  | 991/1304 [03:36<01:08,  4.58it/s]\u001b[A\n",
      "Evaluating:  76%|███████▌  | 992/1304 [03:36<01:08,  4.58it/s]\u001b[A\n",
      "Evaluating:  76%|███████▌  | 993/1304 [03:37<01:07,  4.58it/s]\u001b[A\n",
      "Evaluating:  76%|███████▌  | 994/1304 [03:37<01:07,  4.58it/s]\u001b[A\n",
      "Evaluating:  76%|███████▋  | 995/1304 [03:37<01:07,  4.58it/s]\u001b[A\n",
      "Evaluating:  76%|███████▋  | 996/1304 [03:37<01:07,  4.58it/s]\u001b[A\n",
      "Evaluating:  76%|███████▋  | 997/1304 [03:37<01:06,  4.58it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 998/1304 [03:38<01:06,  4.58it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 999/1304 [03:38<01:06,  4.58it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 1000/1304 [03:38<01:06,  4.58it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 1001/1304 [03:38<01:06,  4.58it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 1002/1304 [03:39<01:05,  4.58it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 1003/1304 [03:39<01:05,  4.58it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 1004/1304 [03:39<01:05,  4.59it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 1005/1304 [03:39<01:05,  4.58it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 1006/1304 [03:39<01:05,  4.58it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 1007/1304 [03:40<01:04,  4.58it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 1008/1304 [03:40<01:04,  4.57it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 1009/1304 [03:40<01:04,  4.57it/s]\u001b[A\n",
      "Evaluating:  77%|███████▋  | 1010/1304 [03:40<01:04,  4.57it/s]\u001b[A\n",
      "Evaluating:  78%|███████▊  | 1011/1304 [03:40<01:03,  4.58it/s]\u001b[A\n",
      "Evaluating:  78%|███████▊  | 1012/1304 [03:41<01:03,  4.59it/s]\u001b[A\n",
      "Evaluating:  78%|███████▊  | 1013/1304 [03:41<01:03,  4.58it/s]\u001b[A\n",
      "Evaluating:  78%|███████▊  | 1014/1304 [03:41<01:03,  4.58it/s]\u001b[A\n",
      "Evaluating:  78%|███████▊  | 1015/1304 [03:41<01:03,  4.57it/s]\u001b[A\n",
      "Evaluating:  78%|███████▊  | 1016/1304 [03:42<01:02,  4.57it/s]\u001b[A\n",
      "Evaluating:  78%|███████▊  | 1017/1304 [03:42<01:02,  4.57it/s]\u001b[A\n",
      "Evaluating:  78%|███████▊  | 1018/1304 [03:42<01:02,  4.57it/s]\u001b[A\n",
      "Evaluating:  78%|███████▊  | 1019/1304 [03:42<01:02,  4.57it/s]\u001b[A\n",
      "Evaluating:  78%|███████▊  | 1020/1304 [03:42<01:02,  4.57it/s]\u001b[A\n",
      "Evaluating:  78%|███████▊  | 1021/1304 [03:43<01:01,  4.58it/s]\u001b[A\n",
      "Evaluating:  78%|███████▊  | 1022/1304 [03:43<01:01,  4.59it/s]\u001b[A\n",
      "Evaluating:  78%|███████▊  | 1023/1304 [03:43<01:01,  4.59it/s]\u001b[A\n",
      "Evaluating:  79%|███████▊  | 1024/1304 [03:43<01:01,  4.59it/s]\u001b[A\n",
      "Evaluating:  79%|███████▊  | 1025/1304 [03:44<01:00,  4.58it/s]\u001b[A\n",
      "Evaluating:  79%|███████▊  | 1026/1304 [03:44<01:00,  4.58it/s]\u001b[A\n",
      "Evaluating:  79%|███████▉  | 1027/1304 [03:44<01:00,  4.58it/s]\u001b[A\n",
      "Evaluating:  79%|███████▉  | 1028/1304 [03:44<01:00,  4.59it/s]\u001b[A\n",
      "Evaluating:  79%|███████▉  | 1029/1304 [03:44<00:59,  4.59it/s]\u001b[A\n",
      "Evaluating:  79%|███████▉  | 1030/1304 [03:45<00:59,  4.58it/s]\u001b[A\n",
      "Evaluating:  79%|███████▉  | 1031/1304 [03:45<00:59,  4.58it/s]\u001b[A\n",
      "Evaluating:  79%|███████▉  | 1032/1304 [03:45<00:59,  4.58it/s]\u001b[A\n",
      "Evaluating:  79%|███████▉  | 1033/1304 [03:45<00:59,  4.58it/s]\u001b[A\n",
      "Evaluating:  79%|███████▉  | 1034/1304 [03:46<00:59,  4.57it/s]\u001b[A\n",
      "Evaluating:  79%|███████▉  | 1035/1304 [03:46<00:58,  4.57it/s]\u001b[A\n",
      "Evaluating:  79%|███████▉  | 1036/1304 [03:46<00:58,  4.57it/s]\u001b[A\n",
      "Evaluating:  80%|███████▉  | 1037/1304 [03:46<00:58,  4.57it/s]\u001b[A\n",
      "Evaluating:  80%|███████▉  | 1038/1304 [03:46<00:58,  4.57it/s]\u001b[A\n",
      "Evaluating:  80%|███████▉  | 1039/1304 [03:47<00:58,  4.57it/s]\u001b[A\n",
      "Evaluating:  80%|███████▉  | 1040/1304 [03:47<00:57,  4.57it/s]\u001b[A\n",
      "Evaluating:  80%|███████▉  | 1041/1304 [03:47<00:57,  4.57it/s]\u001b[A\n",
      "Evaluating:  80%|███████▉  | 1042/1304 [03:47<00:57,  4.57it/s]\u001b[A\n",
      "Evaluating:  80%|███████▉  | 1043/1304 [03:47<00:56,  4.58it/s]\u001b[A\n",
      "Evaluating:  80%|████████  | 1044/1304 [03:48<00:56,  4.58it/s]\u001b[A\n",
      "Evaluating:  80%|████████  | 1045/1304 [03:48<00:56,  4.57it/s]\u001b[A\n",
      "Evaluating:  80%|████████  | 1046/1304 [03:48<00:56,  4.58it/s]\u001b[A\n",
      "Evaluating:  80%|████████  | 1047/1304 [03:48<00:56,  4.58it/s]\u001b[A\n",
      "Evaluating:  80%|████████  | 1048/1304 [03:49<00:55,  4.57it/s]\u001b[A\n",
      "Evaluating:  80%|████████  | 1049/1304 [03:49<00:55,  4.57it/s]\u001b[A\n",
      "Evaluating:  81%|████████  | 1050/1304 [03:49<00:55,  4.58it/s]\u001b[A\n",
      "Evaluating:  81%|████████  | 1051/1304 [03:49<00:55,  4.57it/s]\u001b[A\n",
      "Evaluating:  81%|████████  | 1052/1304 [03:49<00:55,  4.57it/s]\u001b[A\n",
      "Evaluating:  81%|████████  | 1053/1304 [03:50<00:54,  4.57it/s]\u001b[A\n",
      "Evaluating:  81%|████████  | 1054/1304 [03:50<00:54,  4.57it/s]\u001b[A\n",
      "Evaluating:  81%|████████  | 1055/1304 [03:50<00:54,  4.56it/s]\u001b[A\n",
      "Evaluating:  81%|████████  | 1056/1304 [03:50<00:54,  4.57it/s]\u001b[A\n",
      "Evaluating:  81%|████████  | 1057/1304 [03:51<00:54,  4.57it/s]\u001b[A\n",
      "Evaluating:  81%|████████  | 1058/1304 [03:51<00:53,  4.57it/s]\u001b[A\n",
      "Evaluating:  81%|████████  | 1059/1304 [03:51<00:53,  4.57it/s]\u001b[A\n",
      "Evaluating:  81%|████████▏ | 1060/1304 [03:51<00:53,  4.57it/s]\u001b[A\n",
      "Evaluating:  81%|████████▏ | 1061/1304 [03:51<00:53,  4.57it/s]\u001b[A\n",
      "Evaluating:  81%|████████▏ | 1062/1304 [03:52<00:53,  4.56it/s]\u001b[A\n",
      "Evaluating:  82%|████████▏ | 1063/1304 [03:52<00:52,  4.56it/s]\u001b[A\n",
      "Evaluating:  82%|████████▏ | 1064/1304 [03:52<00:52,  4.56it/s]\u001b[A\n",
      "Evaluating:  82%|████████▏ | 1065/1304 [03:52<00:52,  4.57it/s]\u001b[A\n",
      "Evaluating:  82%|████████▏ | 1066/1304 [03:53<00:52,  4.57it/s]\u001b[A\n",
      "Evaluating:  82%|████████▏ | 1067/1304 [03:53<00:51,  4.57it/s]\u001b[A\n",
      "Evaluating:  82%|████████▏ | 1068/1304 [03:53<00:51,  4.57it/s]\u001b[A\n",
      "Evaluating:  82%|████████▏ | 1069/1304 [03:53<00:51,  4.57it/s]\u001b[A\n",
      "Evaluating:  82%|████████▏ | 1070/1304 [03:53<00:51,  4.57it/s]\u001b[A\n",
      "Evaluating:  82%|████████▏ | 1071/1304 [03:54<00:50,  4.57it/s]\u001b[A\n",
      "Evaluating:  82%|████████▏ | 1072/1304 [03:54<00:50,  4.57it/s]\u001b[A\n",
      "Evaluating:  82%|████████▏ | 1073/1304 [03:54<00:50,  4.58it/s]\u001b[A\n",
      "Evaluating:  82%|████████▏ | 1074/1304 [03:54<00:50,  4.59it/s]\u001b[A\n",
      "Evaluating:  82%|████████▏ | 1075/1304 [03:54<00:49,  4.59it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 1076/1304 [03:55<00:49,  4.59it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 1077/1304 [03:55<00:49,  4.58it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 1078/1304 [03:55<00:49,  4.58it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 1079/1304 [03:55<00:49,  4.58it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 1080/1304 [03:56<00:48,  4.58it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 1081/1304 [03:56<00:48,  4.57it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 1082/1304 [03:56<00:48,  4.58it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 1083/1304 [03:56<00:48,  4.58it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 1084/1304 [03:56<00:48,  4.58it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 1085/1304 [03:57<00:47,  4.58it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 1086/1304 [03:57<00:47,  4.58it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 1087/1304 [03:57<00:47,  4.58it/s]\u001b[A\n",
      "Evaluating:  83%|████████▎ | 1088/1304 [03:57<00:47,  4.58it/s]\u001b[A\n",
      "Evaluating:  84%|████████▎ | 1089/1304 [03:58<00:46,  4.58it/s]\u001b[A\n",
      "Evaluating:  84%|████████▎ | 1090/1304 [03:58<00:46,  4.58it/s]\u001b[A\n",
      "Evaluating:  84%|████████▎ | 1091/1304 [03:58<00:46,  4.58it/s]\u001b[A\n",
      "Evaluating:  84%|████████▎ | 1092/1304 [03:58<00:46,  4.59it/s]\u001b[A\n",
      "Evaluating:  84%|████████▍ | 1093/1304 [03:58<00:46,  4.58it/s]\u001b[A\n",
      "Evaluating:  84%|████████▍ | 1094/1304 [03:59<00:45,  4.58it/s]\u001b[A\n",
      "Evaluating:  84%|████████▍ | 1095/1304 [03:59<00:45,  4.58it/s]\u001b[A\n",
      "Evaluating:  84%|████████▍ | 1096/1304 [03:59<00:45,  4.58it/s]\u001b[A\n",
      "Evaluating:  84%|████████▍ | 1097/1304 [03:59<00:45,  4.58it/s]\u001b[A\n",
      "Evaluating:  84%|████████▍ | 1098/1304 [04:00<00:44,  4.58it/s]\u001b[A\n",
      "Evaluating:  84%|████████▍ | 1099/1304 [04:00<00:44,  4.57it/s]\u001b[A\n",
      "Evaluating:  84%|████████▍ | 1100/1304 [04:00<00:44,  4.58it/s]\u001b[A\n",
      "Evaluating:  84%|████████▍ | 1101/1304 [04:00<00:44,  4.57it/s]\u001b[A\n",
      "Evaluating:  85%|████████▍ | 1102/1304 [04:00<00:44,  4.57it/s]\u001b[A\n",
      "Evaluating:  85%|████████▍ | 1103/1304 [04:01<00:44,  4.57it/s]\u001b[A\n",
      "Evaluating:  85%|████████▍ | 1104/1304 [04:01<00:43,  4.57it/s]\u001b[A\n",
      "Evaluating:  85%|████████▍ | 1105/1304 [04:01<00:43,  4.58it/s]\u001b[A\n",
      "Evaluating:  85%|████████▍ | 1106/1304 [04:01<00:43,  4.57it/s]\u001b[A\n",
      "Evaluating:  85%|████████▍ | 1107/1304 [04:01<00:43,  4.57it/s]\u001b[A\n",
      "Evaluating:  85%|████████▍ | 1108/1304 [04:02<00:42,  4.57it/s]\u001b[A\n",
      "Evaluating:  85%|████████▌ | 1109/1304 [04:02<00:42,  4.57it/s]\u001b[A\n",
      "Evaluating:  85%|████████▌ | 1110/1304 [04:02<00:42,  4.58it/s]\u001b[A\n",
      "Evaluating:  85%|████████▌ | 1111/1304 [04:02<00:42,  4.58it/s]\u001b[A\n",
      "Evaluating:  85%|████████▌ | 1112/1304 [04:03<00:41,  4.58it/s]\u001b[A\n",
      "Evaluating:  85%|████████▌ | 1113/1304 [04:03<00:41,  4.57it/s]\u001b[A\n",
      "Evaluating:  85%|████████▌ | 1114/1304 [04:03<00:41,  4.57it/s]\u001b[A\n",
      "Evaluating:  86%|████████▌ | 1115/1304 [04:03<00:41,  4.58it/s]\u001b[A\n",
      "Evaluating:  86%|████████▌ | 1116/1304 [04:03<00:41,  4.58it/s]\u001b[A\n",
      "Evaluating:  86%|████████▌ | 1117/1304 [04:04<00:40,  4.58it/s]\u001b[A\n",
      "Evaluating:  86%|████████▌ | 1118/1304 [04:04<00:40,  4.58it/s]\u001b[A\n",
      "Evaluating:  86%|████████▌ | 1119/1304 [04:04<00:40,  4.57it/s]\u001b[A\n",
      "Evaluating:  86%|████████▌ | 1120/1304 [04:04<00:40,  4.57it/s]\u001b[A\n",
      "Evaluating:  86%|████████▌ | 1121/1304 [04:05<00:40,  4.57it/s]\u001b[A\n",
      "Evaluating:  86%|████████▌ | 1122/1304 [04:05<00:39,  4.57it/s]\u001b[A\n",
      "Evaluating:  86%|████████▌ | 1123/1304 [04:05<00:39,  4.58it/s]\u001b[A\n",
      "Evaluating:  86%|████████▌ | 1124/1304 [04:05<00:39,  4.57it/s]\u001b[A\n",
      "Evaluating:  86%|████████▋ | 1125/1304 [04:05<00:39,  4.57it/s]\u001b[A\n",
      "Evaluating:  86%|████████▋ | 1126/1304 [04:06<00:38,  4.57it/s]\u001b[A\n",
      "Evaluating:  86%|████████▋ | 1127/1304 [04:06<00:38,  4.57it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 1128/1304 [04:06<00:38,  4.57it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 1129/1304 [04:06<00:38,  4.57it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 1130/1304 [04:07<00:38,  4.57it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 1131/1304 [04:07<00:37,  4.57it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 1132/1304 [04:07<00:37,  4.57it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 1133/1304 [04:07<00:37,  4.57it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 1134/1304 [04:07<00:37,  4.57it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 1135/1304 [04:08<00:36,  4.57it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 1136/1304 [04:08<00:36,  4.57it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 1137/1304 [04:08<00:36,  4.57it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 1138/1304 [04:08<00:36,  4.57it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 1139/1304 [04:08<00:36,  4.57it/s]\u001b[A\n",
      "Evaluating:  87%|████████▋ | 1140/1304 [04:09<00:35,  4.57it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 1141/1304 [04:09<00:35,  4.57it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 1142/1304 [04:09<00:35,  4.57it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  88%|████████▊ | 1143/1304 [04:09<00:35,  4.57it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 1144/1304 [04:10<00:35,  4.57it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 1145/1304 [04:10<00:34,  4.57it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 1146/1304 [04:10<00:34,  4.57it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 1147/1304 [04:10<00:34,  4.57it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 1148/1304 [04:10<00:34,  4.58it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 1149/1304 [04:11<00:33,  4.57it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 1150/1304 [04:11<00:33,  4.57it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 1151/1304 [04:11<00:33,  4.57it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 1152/1304 [04:11<00:33,  4.57it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 1153/1304 [04:12<00:32,  4.58it/s]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 1154/1304 [04:12<00:32,  4.59it/s]\u001b[A\n",
      "Evaluating:  89%|████████▊ | 1155/1304 [04:12<00:32,  4.59it/s]\u001b[A\n",
      "Evaluating:  89%|████████▊ | 1156/1304 [04:12<00:32,  4.58it/s]\u001b[A\n",
      "Evaluating:  89%|████████▊ | 1157/1304 [04:12<00:32,  4.58it/s]\u001b[A\n",
      "Evaluating:  89%|████████▉ | 1158/1304 [04:13<00:31,  4.58it/s]\u001b[A\n",
      "Evaluating:  89%|████████▉ | 1159/1304 [04:13<00:31,  4.58it/s]\u001b[A\n",
      "Evaluating:  89%|████████▉ | 1160/1304 [04:13<00:31,  4.57it/s]\u001b[A\n",
      "Evaluating:  89%|████████▉ | 1161/1304 [04:13<00:31,  4.57it/s]\u001b[A\n",
      "Evaluating:  89%|████████▉ | 1162/1304 [04:14<00:31,  4.57it/s]\u001b[A\n",
      "Evaluating:  89%|████████▉ | 1163/1304 [04:14<00:30,  4.57it/s]\u001b[A\n",
      "Evaluating:  89%|████████▉ | 1164/1304 [04:14<00:30,  4.57it/s]\u001b[A\n",
      "Evaluating:  89%|████████▉ | 1165/1304 [04:14<00:30,  4.56it/s]\u001b[A\n",
      "Evaluating:  89%|████████▉ | 1166/1304 [04:14<00:30,  4.57it/s]\u001b[A\n",
      "Evaluating:  89%|████████▉ | 1167/1304 [04:15<00:29,  4.57it/s]\u001b[A\n",
      "Evaluating:  90%|████████▉ | 1168/1304 [04:15<00:29,  4.57it/s]\u001b[A\n",
      "Evaluating:  90%|████████▉ | 1169/1304 [04:15<00:29,  4.57it/s]\u001b[A\n",
      "Evaluating:  90%|████████▉ | 1170/1304 [04:15<00:29,  4.58it/s]\u001b[A\n",
      "Evaluating:  90%|████████▉ | 1171/1304 [04:15<00:29,  4.57it/s]\u001b[A\n",
      "Evaluating:  90%|████████▉ | 1172/1304 [04:16<00:28,  4.57it/s]\u001b[A\n",
      "Evaluating:  90%|████████▉ | 1173/1304 [04:16<00:28,  4.58it/s]\u001b[A\n",
      "Evaluating:  90%|█████████ | 1174/1304 [04:16<00:28,  4.58it/s]\u001b[A\n",
      "Evaluating:  90%|█████████ | 1175/1304 [04:16<00:28,  4.58it/s]\u001b[A\n",
      "Evaluating:  90%|█████████ | 1176/1304 [04:17<00:27,  4.57it/s]\u001b[A\n",
      "Evaluating:  90%|█████████ | 1177/1304 [04:17<00:27,  4.57it/s]\u001b[A\n",
      "Evaluating:  90%|█████████ | 1178/1304 [04:17<00:27,  4.57it/s]\u001b[A\n",
      "Evaluating:  90%|█████████ | 1179/1304 [04:17<00:27,  4.57it/s]\u001b[A\n",
      "Evaluating:  90%|█████████ | 1180/1304 [04:17<00:27,  4.58it/s]\u001b[A\n",
      "Evaluating:  91%|█████████ | 1181/1304 [04:18<00:26,  4.57it/s]\u001b[A\n",
      "Evaluating:  91%|█████████ | 1182/1304 [04:18<00:26,  4.57it/s]\u001b[A\n",
      "Evaluating:  91%|█████████ | 1183/1304 [04:18<00:26,  4.57it/s]\u001b[A\n",
      "Evaluating:  91%|█████████ | 1184/1304 [04:18<00:26,  4.57it/s]\u001b[A\n",
      "Evaluating:  91%|█████████ | 1185/1304 [04:19<00:25,  4.58it/s]\u001b[A\n",
      "Evaluating:  91%|█████████ | 1186/1304 [04:19<00:25,  4.58it/s]\u001b[A\n",
      "Evaluating:  91%|█████████ | 1187/1304 [04:19<00:25,  4.58it/s]\u001b[A\n",
      "Evaluating:  91%|█████████ | 1188/1304 [04:19<00:25,  4.58it/s]\u001b[A\n",
      "Evaluating:  91%|█████████ | 1189/1304 [04:19<00:25,  4.57it/s]\u001b[A\n",
      "Evaluating:  91%|█████████▏| 1190/1304 [04:20<00:24,  4.58it/s]\u001b[A\n",
      "Evaluating:  91%|█████████▏| 1191/1304 [04:20<00:24,  4.58it/s]\u001b[A\n",
      "Evaluating:  91%|█████████▏| 1192/1304 [04:20<00:24,  4.58it/s]\u001b[A\n",
      "Evaluating:  91%|█████████▏| 1193/1304 [04:20<00:24,  4.58it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 1194/1304 [04:20<00:24,  4.57it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 1195/1304 [04:21<00:23,  4.57it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 1196/1304 [04:21<00:23,  4.57it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 1197/1304 [04:21<00:23,  4.58it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 1198/1304 [04:21<00:23,  4.58it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 1199/1304 [04:22<00:22,  4.57it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 1200/1304 [04:22<00:22,  4.57it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 1201/1304 [04:22<00:22,  4.57it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 1202/1304 [04:22<00:22,  4.57it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 1203/1304 [04:22<00:22,  4.58it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 1204/1304 [04:23<00:21,  4.59it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 1205/1304 [04:23<00:21,  4.59it/s]\u001b[A\n",
      "Evaluating:  92%|█████████▏| 1206/1304 [04:23<00:21,  4.58it/s]\u001b[A\n",
      "Evaluating:  93%|█████████▎| 1207/1304 [04:23<00:21,  4.58it/s]\u001b[A\n",
      "Evaluating:  93%|█████████▎| 1208/1304 [04:24<00:20,  4.58it/s]\u001b[A\n",
      "Evaluating:  93%|█████████▎| 1209/1304 [04:24<00:20,  4.58it/s]\u001b[A\n",
      "Evaluating:  93%|█████████▎| 1210/1304 [04:24<00:20,  4.57it/s]\u001b[A\n",
      "Evaluating:  93%|█████████▎| 1211/1304 [04:24<00:20,  4.57it/s]\u001b[A\n",
      "Evaluating:  93%|█████████▎| 1212/1304 [04:24<00:20,  4.57it/s]\u001b[A\n",
      "Evaluating:  93%|█████████▎| 1213/1304 [04:25<00:19,  4.57it/s]\u001b[A\n",
      "Evaluating:  93%|█████████▎| 1214/1304 [04:25<00:19,  4.57it/s]\u001b[A\n",
      "Evaluating:  93%|█████████▎| 1215/1304 [04:25<00:19,  4.57it/s]\u001b[A\n",
      "Evaluating:  93%|█████████▎| 1216/1304 [04:25<00:19,  4.57it/s]\u001b[A\n",
      "Evaluating:  93%|█████████▎| 1217/1304 [04:26<00:19,  4.57it/s]\u001b[A\n",
      "Evaluating:  93%|█████████▎| 1218/1304 [04:26<00:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  93%|█████████▎| 1219/1304 [04:26<00:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▎| 1220/1304 [04:26<00:18,  4.56it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▎| 1221/1304 [04:26<00:18,  4.57it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▎| 1222/1304 [04:27<00:17,  4.57it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▍| 1223/1304 [04:27<00:17,  4.57it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▍| 1224/1304 [04:27<00:17,  4.56it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▍| 1225/1304 [04:27<00:17,  4.55it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▍| 1226/1304 [04:27<00:17,  4.55it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▍| 1227/1304 [04:28<00:16,  4.56it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▍| 1228/1304 [04:28<00:16,  4.57it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▍| 1229/1304 [04:28<00:16,  4.57it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▍| 1230/1304 [04:28<00:16,  4.56it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▍| 1231/1304 [04:29<00:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  94%|█████████▍| 1232/1304 [04:29<00:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  95%|█████████▍| 1233/1304 [04:29<00:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  95%|█████████▍| 1234/1304 [04:29<00:15,  4.57it/s]\u001b[A\n",
      "Evaluating:  95%|█████████▍| 1235/1304 [04:29<00:15,  4.58it/s]\u001b[A\n",
      "Evaluating:  95%|█████████▍| 1236/1304 [04:30<00:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  95%|█████████▍| 1237/1304 [04:30<00:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  95%|█████████▍| 1238/1304 [04:30<00:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  95%|█████████▌| 1239/1304 [04:30<00:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  95%|█████████▌| 1240/1304 [04:31<00:14,  4.57it/s]\u001b[A\n",
      "Evaluating:  95%|█████████▌| 1241/1304 [04:31<00:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  95%|█████████▌| 1242/1304 [04:31<00:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  95%|█████████▌| 1243/1304 [04:31<00:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  95%|█████████▌| 1244/1304 [04:31<00:13,  4.57it/s]\u001b[A\n",
      "Evaluating:  95%|█████████▌| 1245/1304 [04:32<00:12,  4.58it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▌| 1246/1304 [04:32<00:12,  4.58it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▌| 1247/1304 [04:32<00:12,  4.58it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▌| 1248/1304 [04:32<00:12,  4.58it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▌| 1249/1304 [04:33<00:12,  4.58it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▌| 1250/1304 [04:33<00:11,  4.58it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▌| 1251/1304 [04:33<00:11,  4.59it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▌| 1252/1304 [04:33<00:11,  4.58it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▌| 1253/1304 [04:33<00:11,  4.57it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▌| 1254/1304 [04:34<00:10,  4.58it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▌| 1255/1304 [04:34<00:10,  4.58it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▋| 1256/1304 [04:34<00:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▋| 1257/1304 [04:34<00:10,  4.57it/s]\u001b[A\n",
      "Evaluating:  96%|█████████▋| 1258/1304 [04:34<00:10,  4.58it/s]\u001b[A\n",
      "Evaluating:  97%|█████████▋| 1259/1304 [04:35<00:09,  4.58it/s]\u001b[A\n",
      "Evaluating:  97%|█████████▋| 1260/1304 [04:35<00:09,  4.57it/s]\u001b[A\n",
      "Evaluating:  97%|█████████▋| 1261/1304 [04:35<00:09,  4.57it/s]\u001b[A\n",
      "Evaluating:  97%|█████████▋| 1262/1304 [04:35<00:09,  4.57it/s]\u001b[A\n",
      "Evaluating:  97%|█████████▋| 1263/1304 [04:36<00:08,  4.57it/s]\u001b[A\n",
      "Evaluating:  97%|█████████▋| 1264/1304 [04:36<00:08,  4.57it/s]\u001b[A\n",
      "Evaluating:  97%|█████████▋| 1265/1304 [04:36<00:08,  4.57it/s]\u001b[A\n",
      "Evaluating:  97%|█████████▋| 1266/1304 [04:36<00:08,  4.57it/s]\u001b[A\n",
      "Evaluating:  97%|█████████▋| 1267/1304 [04:36<00:08,  4.57it/s]\u001b[A\n",
      "Evaluating:  97%|█████████▋| 1268/1304 [04:37<00:07,  4.57it/s]\u001b[A\n",
      "Evaluating:  97%|█████████▋| 1269/1304 [04:37<00:07,  4.57it/s]\u001b[A\n",
      "Evaluating:  97%|█████████▋| 1270/1304 [04:37<00:07,  4.58it/s]\u001b[A\n",
      "Evaluating:  97%|█████████▋| 1271/1304 [04:37<00:07,  4.58it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 1272/1304 [04:38<00:06,  4.57it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 1273/1304 [04:38<00:06,  4.57it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 1274/1304 [04:38<00:06,  4.57it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 1275/1304 [04:38<00:06,  4.57it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 1276/1304 [04:38<00:06,  4.57it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 1277/1304 [04:39<00:05,  4.57it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 1278/1304 [04:39<00:05,  4.57it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 1279/1304 [04:39<00:05,  4.57it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 1280/1304 [04:39<00:05,  4.57it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 1281/1304 [04:40<00:05,  4.57it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 1282/1304 [04:40<00:04,  4.57it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 1283/1304 [04:40<00:04,  4.57it/s]\u001b[A\n",
      "Evaluating:  98%|█████████▊| 1284/1304 [04:40<00:04,  4.58it/s]\u001b[A\n",
      "Evaluating:  99%|█████████▊| 1285/1304 [04:40<00:04,  4.57it/s]\u001b[A\n",
      "Evaluating:  99%|█████████▊| 1286/1304 [04:41<00:03,  4.57it/s]\u001b[A\n",
      "Evaluating:  99%|█████████▊| 1287/1304 [04:41<00:03,  4.57it/s]\u001b[A\n",
      "Evaluating:  99%|█████████▉| 1288/1304 [04:41<00:03,  4.57it/s]\u001b[A\n",
      "Evaluating:  99%|█████████▉| 1289/1304 [04:41<00:03,  4.56it/s]\u001b[A\n",
      "Evaluating:  99%|█████████▉| 1290/1304 [04:41<00:03,  4.56it/s]\u001b[A\n",
      "Evaluating:  99%|█████████▉| 1291/1304 [04:42<00:02,  4.56it/s]\u001b[A\n",
      "Evaluating:  99%|█████████▉| 1292/1304 [04:42<00:02,  4.57it/s]\u001b[A\n",
      "Evaluating:  99%|█████████▉| 1293/1304 [04:42<00:02,  4.57it/s]\u001b[A\n",
      "Evaluating:  99%|█████████▉| 1294/1304 [04:42<00:02,  4.57it/s]\u001b[A\n",
      "Evaluating:  99%|█████████▉| 1295/1304 [04:43<00:01,  4.58it/s]\u001b[A\n",
      "Evaluating:  99%|█████████▉| 1296/1304 [04:43<00:01,  4.57it/s]\u001b[A\n",
      "Evaluating:  99%|█████████▉| 1297/1304 [04:43<00:01,  4.58it/s]\u001b[A\n",
      "Evaluating: 100%|█████████▉| 1298/1304 [04:43<00:01,  4.57it/s]\u001b[A\n",
      "Evaluating: 100%|█████████▉| 1299/1304 [04:43<00:01,  4.57it/s]\u001b[A\n",
      "Evaluating: 100%|█████████▉| 1300/1304 [04:44<00:00,  4.57it/s]\u001b[A\n",
      "Evaluating: 100%|█████████▉| 1301/1304 [04:44<00:00,  4.56it/s]\u001b[A\n",
      "Evaluating: 100%|█████████▉| 1302/1304 [04:44<00:00,  4.57it/s]\u001b[A\n",
      "Evaluating: 100%|██████████| 1304/1304 [04:44<00:00,  4.58it/s]\u001b[A\n",
      "12/02/2022 19:10:22 - INFO - __main__ -   Saving model checkpoint to ./models/checkpoint-last\n",
      "12/02/2022 19:10:29 - INFO - __main__ -   Saving optimizer and scheduler states to ./models/checkpoint-last\n",
      "12/02/2022 19:10:36 - INFO - __main__ -   Saving model checkpoint to ./models/checkpoint-best\n",
      "12/02/2022 19:10:48 - INFO - __main__ -   Saving optimizer and scheduler states to ./models/checkpoint-best\n",
      "Epoch: 100%|██████████| 1/1 [1:24:21<00:00, 5061.61s/it]\n",
      "12/02/2022 19:10:48 - INFO - __main__ -    global_step = 20613, average loss = 0.26260399696105535\n",
      "12/02/2022 19:10:48 - INFO - __main__ -   Saving model checkpoint to ./models\n",
      "Some weights of the model checkpoint at ./models were not used when initializing RobertaModel: ['query_encoder.encoder.layer.8.output.dense.bias', 'code_encoder.encoder.layer.6.output.dense.weight', 'code_encoder.encoder.layer.6.attention.output.dense.weight', 'query_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.attention.self.query.weight', 'query_encoder.encoder.layer.2.attention.self.query.weight', 'query_encoder.encoder.layer.7.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.intermediate.dense.bias', 'query_encoder.encoder.layer.4.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.output.dense.bias', 'query_encoder.encoder.layer.10.attention.self.value.weight', 'query_encoder.encoder.layer.2.attention.self.value.bias', 'query_encoder.encoder.layer.1.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.attention.self.query.weight', 'query_encoder.encoder.layer.3.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.self.key.bias', 'code_encoder.encoder.layer.4.attention.output.dense.weight', 'query_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.1.attention.output.dense.weight', 'code_encoder.encoder.layer.5.attention.self.key.weight', 'code_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'query_encoder.embeddings.LayerNorm.weight', 'code_encoder.encoder.layer.9.output.LayerNorm.bias', 'code_encoder.embeddings.LayerNorm.bias', 'code_encoder.encoder.layer.2.intermediate.dense.weight', 'code_encoder.encoder.layer.0.intermediate.dense.weight', 'code_encoder.encoder.layer.3.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.attention.self.query.bias', 'query_encoder.encoder.layer.0.attention.self.key.weight', 'query_encoder.encoder.layer.7.attention.self.query.weight', 'query_encoder.encoder.layer.2.intermediate.dense.bias', 'code_encoder.encoder.layer.10.attention.self.key.weight', 'query_encoder.encoder.layer.7.intermediate.dense.weight', 'query_encoder.encoder.layer.2.attention.output.dense.weight', 'query_encoder.encoder.layer.5.output.LayerNorm.bias', 'query_encoder.pooler.dense.weight', 'query_encoder.encoder.layer.0.output.dense.bias', 'code_encoder.encoder.layer.10.attention.self.query.bias', 'query_encoder.encoder.layer.11.output.LayerNorm.weight', 'code_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.3.output.dense.bias', 'query_encoder.encoder.layer.2.intermediate.dense.weight', 'query_encoder.encoder.layer.1.attention.self.query.bias', 'code_encoder.encoder.layer.3.output.LayerNorm.weight', 'query_encoder.encoder.layer.11.output.dense.weight', 'code_encoder.encoder.layer.9.attention.self.key.weight', 'code_encoder.encoder.layer.4.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.output.dense.bias', 'query_encoder.encoder.layer.1.attention.output.dense.weight', 'query_encoder.encoder.layer.3.intermediate.dense.bias', 'query_encoder.embeddings.word_embeddings.weight', 'query_encoder.encoder.layer.5.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.9.output.dense.bias', 'code_encoder.encoder.layer.11.attention.self.key.bias', 'query_encoder.encoder.layer.8.attention.self.value.bias', 'code_encoder.embeddings.LayerNorm.weight', 'query_encoder.encoder.layer.8.attention.self.query.bias', 'code_encoder.encoder.layer.1.attention.self.query.weight', 'code_encoder.encoder.layer.1.output.dense.bias', 'query_encoder.encoder.layer.10.output.LayerNorm.bias', 'query_encoder.encoder.layer.9.attention.self.key.weight', 'query_encoder.encoder.layer.7.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.intermediate.dense.bias', 'query_encoder.encoder.layer.0.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.output.dense.bias', 'query_encoder.encoder.layer.4.attention.self.query.bias', 'query_encoder.embeddings.position_embeddings.weight', 'query_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'code_encoder.embeddings.token_type_embeddings.weight', 'query_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.2.attention.self.query.bias', 'query_encoder.encoder.layer.11.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.key.bias', 'code_encoder.encoder.layer.4.attention.self.key.bias', 'code_encoder.encoder.layer.6.output.dense.bias', 'code_encoder.encoder.layer.10.intermediate.dense.weight', 'code_encoder.encoder.layer.5.attention.output.dense.weight', 'query_encoder.encoder.layer.9.attention.output.dense.bias', 'code_encoder.encoder.layer.11.attention.self.value.bias', 'query_encoder.encoder.layer.1.attention.self.query.weight', 'code_encoder.encoder.layer.2.attention.self.key.bias', 'code_encoder.encoder.layer.2.attention.output.dense.bias', 'query_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.6.output.LayerNorm.weight', 'code_encoder.encoder.layer.6.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.output.dense.weight', 'code_encoder.encoder.layer.1.attention.self.value.weight', 'code_encoder.encoder.layer.2.output.dense.bias', 'code_encoder.encoder.layer.0.attention.self.key.bias', 'code_encoder.encoder.layer.2.output.LayerNorm.weight', 'query_encoder.encoder.layer.5.attention.output.dense.bias', 'code_encoder.encoder.layer.6.attention.self.value.bias', 'code_encoder.encoder.layer.7.attention.self.value.weight', 'code_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.9.attention.output.dense.weight', 'query_encoder.encoder.layer.7.attention.self.value.bias', 'query_encoder.encoder.layer.9.attention.self.query.weight', 'code_encoder.encoder.layer.2.attention.self.query.bias', 'query_encoder.encoder.layer.10.output.LayerNorm.weight', 'query_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.output.dense.weight', 'code_encoder.encoder.layer.11.intermediate.dense.bias', 'query_encoder.encoder.layer.10.attention.self.query.bias', 'code_encoder.encoder.layer.0.intermediate.dense.bias', 'query_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.0.attention.self.value.weight', 'code_encoder.encoder.layer.1.attention.self.key.weight', 'query_encoder.encoder.layer.0.attention.self.query.weight', 'code_encoder.encoder.layer.10.attention.self.value.weight', 'query_encoder.encoder.layer.11.intermediate.dense.weight', 'query_encoder.encoder.layer.11.output.dense.bias', 'code_encoder.encoder.layer.0.output.LayerNorm.bias', 'code_encoder.encoder.layer.3.attention.output.dense.weight', 'code_encoder.encoder.layer.9.output.dense.weight', 'code_encoder.encoder.layer.9.intermediate.dense.bias', 'query_encoder.encoder.layer.0.attention.output.dense.bias', 'code_encoder.encoder.layer.7.output.dense.bias', 'query_encoder.encoder.layer.10.attention.output.dense.bias', 'query_encoder.encoder.layer.4.intermediate.dense.bias', 'query_encoder.encoder.layer.11.attention.self.query.weight', 'query_encoder.encoder.layer.8.output.dense.weight', 'code_encoder.encoder.layer.7.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.intermediate.dense.weight', 'code_encoder.encoder.layer.4.attention.self.value.weight', 'code_encoder.encoder.layer.8.attention.self.query.weight', 'query_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.1.intermediate.dense.weight', 'code_encoder.encoder.layer.2.attention.self.value.weight', 'query_encoder.encoder.layer.8.attention.self.value.weight', 'query_encoder.encoder.layer.11.attention.self.key.weight', 'code_encoder.encoder.layer.10.attention.self.key.bias', 'query_encoder.encoder.layer.4.attention.self.value.weight', 'query_encoder.encoder.layer.9.intermediate.dense.bias', 'query_encoder.encoder.layer.5.intermediate.dense.bias', 'code_encoder.encoder.layer.3.attention.self.value.bias', 'query_encoder.encoder.layer.9.attention.self.key.bias', 'query_encoder.encoder.layer.6.attention.self.value.bias', 'code_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'query_encoder.pooler.dense.bias', 'code_encoder.encoder.layer.3.attention.self.query.bias', 'query_encoder.embeddings.LayerNorm.bias', 'query_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.self.value.weight', 'query_encoder.encoder.layer.11.attention.output.dense.bias', 'code_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.3.attention.self.key.weight', 'query_encoder.encoder.layer.11.intermediate.dense.bias', 'query_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.output.dense.bias', 'code_encoder.encoder.layer.2.attention.self.key.weight', 'code_encoder.encoder.layer.10.output.dense.weight', 'query_encoder.encoder.layer.5.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.output.dense.weight', 'code_encoder.encoder.layer.6.attention.output.dense.bias', 'code_encoder.encoder.layer.2.output.dense.weight', 'query_encoder.encoder.layer.7.attention.self.key.bias', 'code_encoder.encoder.layer.3.attention.self.key.bias', 'code_encoder.encoder.layer.5.attention.self.value.bias', 'query_encoder.encoder.layer.6.output.dense.weight', 'query_encoder.encoder.layer.3.attention.self.value.weight', 'query_encoder.encoder.layer.8.output.LayerNorm.weight', 'code_encoder.encoder.layer.7.attention.self.query.bias', 'query_encoder.encoder.layer.7.attention.output.dense.weight', 'code_encoder.encoder.layer.11.output.dense.weight', 'code_encoder.encoder.layer.5.output.LayerNorm.bias', 'code_encoder.encoder.layer.8.attention.self.key.weight', 'query_encoder.encoder.layer.0.output.dense.weight', 'query_encoder.encoder.layer.4.output.dense.weight', 'code_encoder.encoder.layer.7.attention.output.dense.weight', 'code_encoder.encoder.layer.8.intermediate.dense.bias', 'code_encoder.encoder.layer.8.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.attention.self.query.weight', 'query_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.1.attention.self.value.bias', 'query_encoder.encoder.layer.8.attention.self.key.weight', 'query_encoder.encoder.layer.6.attention.output.dense.bias', 'code_encoder.encoder.layer.11.attention.self.value.weight', 'query_encoder.encoder.layer.4.attention.output.dense.weight', 'code_encoder.encoder.layer.0.attention.output.dense.bias', 'query_encoder.encoder.layer.1.intermediate.dense.weight', 'code_encoder.encoder.layer.4.intermediate.dense.weight', 'code_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.output.dense.bias', 'query_encoder.encoder.layer.4.intermediate.dense.weight', 'query_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.attention.self.value.weight', 'query_encoder.encoder.layer.2.attention.self.key.weight', 'code_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'query_encoder.embeddings.token_type_embeddings.weight', 'code_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.intermediate.dense.weight', 'code_encoder.encoder.layer.11.output.LayerNorm.weight', 'query_encoder.encoder.layer.2.output.dense.bias', 'code_encoder.encoder.layer.9.attention.output.dense.weight', 'code_encoder.encoder.layer.8.output.dense.weight', 'code_encoder.encoder.layer.6.intermediate.dense.bias', 'query_encoder.encoder.layer.10.attention.self.value.bias', 'code_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.8.output.dense.bias', 'query_encoder.encoder.layer.1.intermediate.dense.bias', 'query_encoder.encoder.layer.6.attention.self.query.bias', 'code_encoder.encoder.layer.1.intermediate.dense.bias', 'query_encoder.encoder.layer.9.output.dense.bias', 'code_encoder.encoder.layer.4.attention.self.value.bias', 'query_encoder.encoder.layer.7.output.dense.bias', 'query_encoder.encoder.layer.6.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.attention.self.value.weight', 'query_encoder.encoder.layer.11.attention.self.query.bias', 'query_encoder.encoder.layer.5.attention.self.value.bias', 'code_encoder.encoder.layer.5.intermediate.dense.bias', 'code_encoder.encoder.layer.8.attention.self.value.bias', 'code_encoder.embeddings.word_embeddings.weight', 'query_encoder.encoder.layer.2.attention.output.dense.bias', 'code_encoder.embeddings.position_embeddings.weight', 'code_encoder.encoder.layer.4.attention.self.query.bias', 'code_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.self.value.bias', 'code_encoder.encoder.layer.4.output.LayerNorm.bias', 'query_encoder.encoder.layer.2.output.LayerNorm.weight', 'query_encoder.encoder.layer.9.output.LayerNorm.bias', 'query_encoder.encoder.layer.0.attention.self.query.bias', 'code_encoder.encoder.layer.2.attention.output.dense.weight', 'code_encoder.encoder.layer.4.attention.self.key.weight', 'query_encoder.encoder.layer.11.attention.output.dense.weight', 'code_encoder.encoder.layer.0.attention.output.dense.weight', 'query_encoder.encoder.layer.3.attention.output.dense.weight', 'query_encoder.encoder.layer.7.attention.self.query.bias', 'code_encoder.encoder.layer.9.attention.output.dense.bias', 'query_encoder.encoder.layer.3.attention.self.key.bias', 'query_encoder.encoder.layer.6.attention.self.query.weight', 'query_encoder.encoder.layer.1.attention.self.key.bias', 'code_encoder.encoder.layer.9.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.output.dense.bias', 'query_encoder.encoder.layer.7.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.attention.self.key.weight', 'code_encoder.encoder.layer.1.attention.output.dense.bias', 'query_encoder.encoder.layer.0.attention.self.key.bias', 'query_encoder.encoder.layer.9.attention.self.query.bias', 'code_encoder.encoder.layer.11.attention.self.key.weight', 'query_encoder.encoder.layer.10.attention.output.dense.weight', 'query_encoder.encoder.layer.0.output.LayerNorm.weight', 'code_encoder.encoder.layer.6.attention.self.query.weight', 'code_encoder.encoder.layer.8.intermediate.dense.weight', 'query_encoder.encoder.layer.2.attention.self.key.bias', 'code_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.2.attention.self.value.weight', 'query_encoder.encoder.layer.4.attention.self.key.weight', 'code_encoder.encoder.layer.5.attention.self.query.bias', 'code_encoder.encoder.layer.0.attention.self.query.bias', 'query_encoder.encoder.layer.7.intermediate.dense.bias', 'query_encoder.encoder.layer.2.output.dense.weight', 'code_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.6.intermediate.dense.weight', 'query_encoder.encoder.layer.1.output.dense.bias', 'code_encoder.encoder.layer.9.attention.self.value.bias', 'code_encoder.encoder.layer.7.attention.self.query.weight', 'code_encoder.encoder.layer.1.attention.self.query.bias', 'query_encoder.encoder.layer.6.attention.self.value.weight', 'code_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.7.attention.output.dense.bias', 'query_encoder.encoder.layer.2.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.self.key.weight', 'query_encoder.encoder.layer.1.attention.output.dense.bias', 'query_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.9.attention.self.key.bias', 'query_encoder.encoder.layer.9.output.dense.weight', 'code_encoder.encoder.layer.8.attention.output.dense.weight', 'code_encoder.encoder.layer.5.attention.self.query.weight', 'code_encoder.encoder.layer.8.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.key.weight', 'query_encoder.encoder.layer.0.intermediate.dense.weight', 'code_encoder.encoder.layer.1.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.attention.self.value.bias', 'code_encoder.encoder.layer.5.output.dense.bias', 'query_encoder.encoder.layer.0.attention.output.dense.weight', 'code_encoder.encoder.layer.11.attention.output.dense.bias', 'code_encoder.encoder.layer.4.output.dense.bias', 'query_encoder.encoder.layer.5.intermediate.dense.weight', 'code_encoder.encoder.layer.11.attention.output.dense.weight', 'query_encoder.encoder.layer.4.attention.self.query.weight', 'code_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.output.LayerNorm.bias', 'query_encoder.encoder.layer.8.attention.output.dense.bias', 'code_encoder.encoder.layer.4.output.dense.weight', 'query_encoder.encoder.layer.6.attention.output.dense.weight', 'code_encoder.encoder.layer.3.attention.self.query.weight', 'code_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.self.value.bias', 'code_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.5.output.dense.weight', 'code_encoder.encoder.layer.3.output.dense.weight', 'code_encoder.encoder.layer.7.attention.self.value.bias', 'code_encoder.encoder.layer.6.intermediate.dense.weight', 'code_encoder.encoder.layer.10.output.LayerNorm.weight', 'query_encoder.encoder.layer.1.output.dense.weight', 'code_encoder.encoder.layer.2.intermediate.dense.bias', 'query_encoder.encoder.layer.5.output.LayerNorm.weight', 'code_encoder.encoder.layer.1.output.dense.weight', 'code_encoder.encoder.layer.8.attention.self.query.bias', 'query_encoder.encoder.layer.4.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.output.dense.bias', 'query_encoder.encoder.layer.11.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.query.bias', 'query_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.9.attention.self.query.weight', 'code_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.7.output.dense.weight', 'query_encoder.embeddings.position_ids', 'query_encoder.encoder.layer.7.attention.output.dense.bias', 'query_encoder.encoder.layer.8.attention.output.dense.weight', 'code_encoder.encoder.layer.6.attention.self.query.bias', 'query_encoder.encoder.layer.6.intermediate.dense.bias', 'query_encoder.encoder.layer.9.attention.self.value.bias', 'code_encoder.encoder.layer.8.attention.self.key.bias', 'code_encoder.encoder.layer.0.attention.self.key.weight', 'code_encoder.encoder.layer.7.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.intermediate.dense.weight', 'code_encoder.encoder.layer.5.attention.output.dense.bias', 'code_encoder.pooler.dense.bias', 'code_encoder.encoder.layer.9.attention.self.value.weight', 'code_encoder.encoder.layer.10.output.dense.bias', 'code_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.key.weight', 'code_encoder.encoder.layer.11.attention.self.query.weight', 'query_encoder.encoder.layer.9.intermediate.dense.weight', 'code_encoder.encoder.layer.6.attention.self.value.weight', 'query_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.output.LayerNorm.bias', 'code_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.0.attention.self.value.bias', 'query_encoder.encoder.layer.8.attention.self.query.weight', 'query_encoder.encoder.layer.3.attention.self.query.weight', 'query_encoder.encoder.layer.5.attention.self.query.bias', 'code_encoder.encoder.layer.10.output.LayerNorm.bias', 'code_encoder.encoder.layer.3.intermediate.dense.bias', 'query_encoder.encoder.layer.8.intermediate.dense.bias', 'code_encoder.encoder.layer.10.attention.self.value.bias', 'query_encoder.encoder.layer.11.attention.self.key.bias', 'query_encoder.encoder.layer.7.attention.self.key.weight', 'code_encoder.encoder.layer.5.attention.self.value.weight', 'code_encoder.encoder.layer.7.output.dense.weight', 'query_encoder.encoder.layer.5.attention.self.query.weight', 'code_encoder.encoder.layer.8.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.self.value.bias', 'query_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.0.intermediate.dense.bias', 'query_encoder.encoder.layer.9.output.LayerNorm.weight', 'code_encoder.encoder.layer.7.intermediate.dense.bias', 'code_encoder.encoder.layer.2.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.output.dense.weight', 'query_encoder.encoder.layer.10.attention.self.key.bias', 'query_encoder.encoder.layer.6.output.LayerNorm.weight', 'query_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.11.intermediate.dense.weight', 'query_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.3.attention.output.dense.bias', 'code_encoder.encoder.layer.10.intermediate.dense.bias', 'query_encoder.encoder.layer.9.attention.self.value.weight', 'code_encoder.encoder.layer.3.attention.self.value.weight', 'query_encoder.encoder.layer.6.output.dense.bias', 'code_encoder.encoder.layer.9.attention.self.query.bias', 'code_encoder.encoder.layer.10.attention.output.dense.bias', 'query_encoder.encoder.layer.11.attention.self.value.bias', 'code_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.output.dense.weight', 'code_encoder.embeddings.position_ids', 'code_encoder.encoder.layer.5.attention.self.key.bias', 'code_encoder.encoder.layer.4.attention.self.query.weight', 'query_encoder.encoder.layer.5.attention.self.key.weight', 'code_encoder.encoder.layer.5.intermediate.dense.weight', 'query_encoder.encoder.layer.3.output.dense.weight', 'code_encoder.encoder.layer.5.output.LayerNorm.weight', 'code_encoder.encoder.layer.3.intermediate.dense.weight', 'code_encoder.pooler.dense.weight', 'query_encoder.encoder.layer.10.attention.self.key.weight', 'code_encoder.encoder.layer.2.attention.self.value.bias', 'code_encoder.encoder.layer.2.attention.self.query.weight', 'query_encoder.encoder.layer.6.attention.self.key.weight', 'code_encoder.encoder.layer.0.output.LayerNorm.weight', 'query_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.intermediate.dense.weight', 'query_encoder.encoder.layer.6.attention.self.key.bias', 'code_encoder.encoder.layer.8.attention.output.dense.bias', 'code_encoder.encoder.layer.1.output.LayerNorm.weight', 'code_encoder.encoder.layer.11.output.dense.bias', 'code_encoder.encoder.layer.11.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.attention.self.key.bias', 'query_encoder.encoder.layer.10.intermediate.dense.weight', 'code_encoder.encoder.layer.1.attention.self.key.bias', 'query_encoder.encoder.layer.8.attention.self.key.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./models and are newly initialized: ['encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'pooler.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'pooler.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "# Load in the training dataset. Here, we've handled most of the data preprocessing for you\n",
    "train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, ttype='train')\n",
    "\n",
    "# Call the training function that we defined above\n",
    "global_step, tr_loss = train(args, train_dataset, model, tokenizer, optimizer)\n",
    "logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "\n",
    "# Save the trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  \n",
    "model_to_save.save_pretrained(args.output_dir)\n",
    "tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = AutoModel.from_pretrained(args.output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the best model on the test data\n",
    "\n",
    "Our code in training keeps track of how the model is doing and currently keeps around the files for the \"best performing\" model on the training data. How well does this model do on the test data? Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/02/2022 19:11:03 - INFO - __main__ -   Evaluate the following checkpoint: ./models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./models were not used when initializing CodeSearchBiencoderModel: ['query_encoder.pooler.dense.weight', 'query_encoder.pooler.dense.bias', 'code_encoder.pooler.dense.weight', 'code_encoder.pooler.dense.bias']\n",
      "- This IS expected if you are initializing CodeSearchBiencoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeSearchBiencoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "12/02/2022 19:11:06 - INFO - run_classifier -   Loading features from cached file ./data/cached_dev_valid_pytorch_model.bin_200_codesearch\n",
      "12/02/2022 19:11:10 - INFO - run_classifier -   ***** Running evaluation  *****\n",
      "12/02/2022 19:11:10 - INFO - run_classifier -     Num examples = 83397\n",
      "12/02/2022 19:11:10 - INFO - run_classifier -     Batch size = 64\n",
      "Evaluating: 100%|██████████| 1304/1304 [04:41<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_': 0.5005815556914517, 'f1_': 0.667183404717765, 'acc_and_f1_': 0.5838824802046083}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "checkpoint = args.output_dir\n",
    "\n",
    "logger.info(\"Evaluate the following checkpoint: %s\", checkpoint)\n",
    "\n",
    "print(checkpoint)\n",
    "global_step = \"\"\n",
    "model = model_class.from_pretrained(checkpoint)\n",
    "model.to(args.device)\n",
    "result = evaluate(args, model, tokenizer, checkpoint=checkpoint, prefix=global_step)\n",
    "result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
    "print(result)\n",
    "\n",
    "    \n",
    "# Optional part 3 goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qixrKejmJ-N5"
   },
   "source": [
    "# Doing Inference on the Test Dataset\n",
    "\n",
    "Finally, let's estimate the relevance scores for the query-document pairs in our test dataset. The test dataset **test_data.csv** contains each pair of 99 queries and 958 documents, which in total adds up to 94,842 query-document annotations (compare that with the project update number!). \n",
    "\n",
    "For ease of this exercise, we've already processed the data into a ready-to-go format in **test_data.txt** which is required by the model. In the following block, for each query-document pair, we generate a prediction score that measure the relevance of that pair by feeding the pair as inputs to the model's `forward` function (note that in pytorch if you have some model, doing `model(inputs)` and `model.forward(inputs)` is the same--it's trying to emphasize thinking of these as functions!). \n",
    "\n",
    "Once we have the model predictions, we'll create a new dataframe that contains (1) the query id, (2) the document id, and (3) the relevance score for that pair. We'll hand this dataframe off to Part 2 so you can finish up your GPU work.\n",
    "\n",
    "You should adjust the directory path accordingly in order to successfully do the inference. Check what you got in the result.txt file and write this back into the **test_data.csv** that adds an additional column \"sim\". Later In Part 2, you will incorporate the prediction score into the learning to rank model to see if it can improve the performance of ranking,\n",
    "\n",
    "### Implementation notes\n",
    "\n",
    "This implementation works because we've aligned the `test_data.txt` and `test_data.csv` files so they're in the same order. That means you can write a long list of similarities and then add it back to the test data's DataFrame and it will Just Work™. However, in production settings, it's often useful to keep identifiers with the data as much as possible so that you don't just have a file of predictions and instead can write the predictions with the query/document identifiers (or whatever data you're working with).\n",
    "\n",
    "In this homework's setup, we're precomputed the relevance scores for later integration with some overall ranking function (done in Part 2). To get these, we re-encode everything for each step. In commercial systems, what is typically done is the documents are encoded once and then cached, much like how our inverted index caches the terms in each document. Then when a new queries arrives, we only have to encode it and compare it with the cached document emeddings. This saves a lot of time! Thankfully our dataset is quite small here so we don't need to do that, but the ability to precompute and cache embeddings is worth remembering why bi-encoders are helpful and efficient for ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/02/2022 19:15:51 - INFO - run_classifier -   Loading features from cached file ./data/cached_test_test_data_pytorch_model.bin_200_codesearch\n",
      "12/02/2022 19:15:51 - INFO - run_classifier -   Creating features from dataset file at ./data\n",
      "12/02/2022 19:15:51 - INFO - utils -   LOOKING AT ./data/test_data.txt\n",
      "12/02/2022 19:15:53 - INFO - utils -   Writing example 0 of 94842\n",
      "12/02/2022 19:15:53 - INFO - utils -   *** Example ***\n",
      "12/02/2022 19:15:53 - INFO - utils -   guid: test-0\n",
      "12/02/2022 19:15:53 - INFO - utils -   query_token_ids: 0 29631 740 36245 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/02/2022 19:15:53 - INFO - utils -   code_token_ids: 0 9232 3116 1215 49079 1640 49451 6 414 6 45382 8660 5214 6842 846 1215 495 3721 3755 2068 2076 3256 49434 21062 2274 414 7 47896 2870 4832 46669 48786 35 766 9 47896 2870 7 3116 414 7 4832 12528 48786 35 7031 4832 46669 414 35 2274 414 7 3116 7 47896 2870 4832 12528 414 35 295 35187 8932 4832 46669 45382 8660 35 45382 8660 341 11 47896 2870 4 35364 16 45518 131 49519 4832 12528 45382 8660 35 7031 49434 19 490 1640 49451 6 128 605 27645 25 2870 35 740 36245 1215 9408 5457 740 36245 4 9408 1640 21710 6 45382 8660 5214 10273 757 8660 43 13 516 11 414 35 740 36245 1215 9408 4 9408 1722 1640 1902 43 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/02/2022 19:15:53 - INFO - utils -   label: 0 (id = 0)\n",
      "12/02/2022 19:15:53 - INFO - utils -   *** Example ***\n",
      "12/02/2022 19:15:53 - INFO - utils -   guid: test-1\n",
      "12/02/2022 19:15:53 - INFO - utils -   query_token_ids: 0 29631 740 36245 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/02/2022 19:15:53 - INFO - utils -   code_token_ids: 0 9232 3116 1215 560 1215 49079 1640 506 13650 6 12734 6 22162 3256 19 490 1640 506 13650 6 128 41161 27645 25 740 36245 21710 35 740 36245 9408 5457 740 36245 4 9408 1640 49079 21710 6 45382 8660 5214 3934 3934 9740 24262 47579 15483 3934 17977 5214 49079 4 15513 28002 1215 24765 3755 2118 43 740 36245 9408 4 9408 1722 1640 24419 43 13 3236 11 22162 35 740 36245 9408 4 9408 1722 1640 646 29 4 225 20414 46469 49118 12 398 18653 44654 1640 3934 3934 12801 322 44654 45803 37457 282 3934 12801 43 13 579 11 3236 45587 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/02/2022 19:15:53 - INFO - utils -   label: 0 (id = 0)\n",
      "12/02/2022 19:15:53 - INFO - utils -   *** Example ***\n",
      "12/02/2022 19:15:53 - INFO - utils -   guid: test-2\n",
      "12/02/2022 19:15:53 - INFO - utils -   query_token_ids: 0 29631 740 36245 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/02/2022 19:15:53 - INFO - utils -   code_token_ids: 0 9232 3116 1215 49079 1640 13367 3256 849 2315 78 7510 18 2870 2718 98 52 3116 47896 7 276 31826 25 84 414 740 36245 1215 22609 5457 1403 48030 22609 73 108 16008 22597 4 49079 108 19 490 1640 49079 1215 22609 6 128 605 27645 25 856 35 740 36245 1215 9408 5457 740 36245 4 9408 1640 506 43 740 36245 1215 9408 4 9408 1722 1640 48759 13650 47429 33480 108 45587 13 1763 11 1403 48030 49079 1215 25867 4 46740 49536 1763 5457 646 366 4 22609 4 5982 22609 1640 44170 10975 288 7479 1403 48030 22609 238 1763 10975 134 48392 740 36245 1215 9408 4 9408 1722 1640 44170 43 671 740 36245 1215 22609 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/02/2022 19:15:53 - INFO - utils -   label: 0 (id = 0)\n",
      "12/02/2022 19:15:53 - INFO - utils -   *** Example ***\n",
      "12/02/2022 19:15:53 - INFO - utils -   guid: test-3\n",
      "12/02/2022 19:15:53 - INFO - utils -   query_token_ids: 0 29631 740 36245 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/02/2022 19:15:53 - INFO - utils -   code_token_ids: 0 9232 7 1215 49079 1640 13367 6 856 48939 6 9740 24262 47579 113 3934 45382 8660 5214 47429 3256 740 36245 9408 5457 740 36245 4 9408 1640 856 48939 6 9740 24262 5214 43948 24262 6 45382 8660 5214 10273 757 8660 6 24248 39938 2630 40635 37457 282 1297 17977 5214 49079 4 15513 28002 1215 7981 4839 114 1403 4 47288 35 740 36245 9408 4 9408 1722 1640 13367 4 47288 43 13 516 11 1403 4 8628 462 330 1215 45291 4 8656 1215 13415 49536 740 36245 9408 4 9408 1722 1640 1902 43 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/02/2022 19:15:53 - INFO - utils -   label: 0 (id = 0)\n",
      "12/02/2022 19:15:53 - INFO - utils -   *** Example ***\n",
      "12/02/2022 19:15:53 - INFO - utils -   guid: test-4\n",
      "12/02/2022 19:15:53 - INFO - utils -   query_token_ids: 0 29631 740 36245 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/02/2022 19:15:53 - INFO - utils -   code_token_ids: 0 9232 3116 1215 560 1215 49079 1640 13367 6 48786 3256 128 17809 3015 8303 7 2007 740 36245 4832 46669 7031 48786 35 16234 7 2870 13 5376 128 17809 31243 5457 490 1640 49451 6 128 43192 27645 849 21384 12734 889 12734 1215 23999 5457 47052 21001 11964 3934 128 41933 11964 3934 128 45566 3934 128 40089 13539 12440 3934 128 23996 9210 4183 14064 3934 128 428 12 19434 44403 3331 5457 740 36245 4 495 11726 45489 1640 506 808 6 882 37815 5214 24419 1215 23999 43 42211 5457 28700 48461 13650 288 6 766 288 43 13 766 288 11 12734 1215 23999 43 849 21062 7 2870 3331 4 9408 1722 1640 47288 43 13 3236 11 1403 4 23687 35 849 14619 2976 26640 30 1021 37740 332 19 117 38686 24414 849 8 602 2093 9 5 2\n",
      "12/02/2022 19:15:53 - INFO - utils -   label: 0 (id = 0)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (665 > 512). Running this sequence through the model will result in indexing errors\n",
      "12/02/2022 19:15:58 - INFO - utils -   Writing example 10000 of 94842\n",
      "12/02/2022 19:16:04 - INFO - utils -   Writing example 20000 of 94842\n",
      "12/02/2022 19:16:10 - INFO - utils -   Writing example 30000 of 94842\n",
      "12/02/2022 19:16:16 - INFO - utils -   Writing example 40000 of 94842\n",
      "12/02/2022 19:16:21 - INFO - utils -   Writing example 50000 of 94842\n",
      "12/02/2022 19:16:27 - INFO - utils -   Writing example 60000 of 94842\n",
      "12/02/2022 19:16:33 - INFO - utils -   Writing example 70000 of 94842\n",
      "12/02/2022 19:16:39 - INFO - utils -   Writing example 80000 of 94842\n",
      "12/02/2022 19:16:44 - INFO - utils -   Writing example 90000 of 94842\n",
      "12/02/2022 19:16:47 - INFO - run_classifier -   Saving features into cached file ./data/cached_test_test_data_pytorch_model.bin_200_codesearch\n",
      "Evaluating: 100%|██████████| 1482/1482 [05:18<00:00,  4.65it/s]\n",
      "12/02/2022 19:22:15 - INFO - __main__ -   ***** Writing relevance predictions *****\n"
     ]
    }
   ],
   "source": [
    "# This tells the model that we're switching to evaluation mode (rather than training)\n",
    "# so it should turn off any training-specific functionality that could make it slower\n",
    "# or interfere with our results.\n",
    "model.eval()\n",
    "    \n",
    "# Note here: we're loading the test set ***in sequential order***. This is critical\n",
    "# for the next step because we need to map these predictions to query-document pairs.\n",
    "# In training, we want to see a random order, but typically not during test.\n",
    "eval_dataset, instances = load_and_cache_examples(args, \"codesearch\", tokenizer, ttype='test')\n",
    "eval_sampler = SequentialSampler(eval_dataset)\n",
    "eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "# This data structures will have our predictions and we'll fill them as we process each batch\n",
    "relevance_predictions = np.array([])\n",
    "\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "    # Get the model's cosine similarity for the query-document pairs\n",
    "    # we pass as input. This no_grad() call also tells pytorch that\n",
    "    # we're doing evaluation so pytorch doesn't have to keep track\n",
    "    # of any gradients for updating the model (e.g., remember how\n",
    "    # in the dog t-shirt fitting, we had to keep around how much to change\n",
    "    # the t-shirt sizes).\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Prepare the inputs\n",
    "        inputs = {'query_token_ids': batch[0],\n",
    "                  'code_token_ids': batch[1],\n",
    "                  'labels': batch[3]}\n",
    "       \n",
    "        # Note that this is a list of outputs, which includes the cosine\n",
    "        # similarity, among other stuff\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    # Let's pull out just the cosine similarity\n",
    "    _, cosine_sim = outputs[:2] \n",
    "    \n",
    "    # Pytorch works with \"tensors\" which are just like fancy numpy arrays.\n",
    "    # One main difference is that the tensor might \"live\" on a GPU, which \n",
    "    # means we need to copy it into regular computer memory to use it.\n",
    "    # Here, we'll call .cpu() to get the value back off the GPU and then\n",
    "    # convert the similarities to numpy. Remember, we're getting a list\n",
    "    # of similarities back out!\n",
    "    cosine_sims = cosine_sim.cpu().numpy()\n",
    "    \n",
    "    # Add these similarities to our current similarities\n",
    "    relevance_predictions = np.append(relevance_predictions, cosine_sims, axis=0)\n",
    "\n",
    "if not os.path.exists(args.test_result_dir):\n",
    "    os.makedirs(args.test_result_dir)\n",
    "\n",
    "output_test_file = os.path.join(args.test_result_dir, 'relevance-scores.csv')\n",
    "\n",
    "with open(output_test_file, \"w\") as outf:\n",
    "    logger.info(\"***** Writing relevance predictions *****\")\n",
    "    all_logits = relevance_predictions.tolist()\n",
    "    \n",
    "    # Note that we write these all as one big list. In the next step,\n",
    "    # we'll merge these with the data frame\n",
    "    outf.write(\",\".join([str(item) for item in all_logits]))\n",
    "    \n",
    "# Optional part 3 goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Merge the predictions with the query/doc pairs (10 points)\n",
    "\n",
    "The numpy array `relevance_predictions` now contains a list of all the similarities, which we'll need to merge with the test data. Conveniently, these predictions appear in the exact same order as the query-document pairs in the `data/test_data.csv` file.\n",
    "\n",
    "Your task is to read in `test_data.csv` as a dataframe and merge these relevance predictions as a new column called \"sim\". We'll export this dataframe to a separate file with just a few columns for better efficiency. Write a _new_ file with a subset of this dataframe containing only the columns\n",
    "* \"sim\"\n",
    "* \"qid\" (the query id)\n",
    "* \"docno\" (the document id)\n",
    "These last two columns match the pyterrier naming conventions, which you'll need for Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Query</th>\n",
       "      <th>GitHubUrl</th>\n",
       "      <th>code</th>\n",
       "      <th>qid</th>\n",
       "      <th>docno</th>\n",
       "      <th>relevance</th>\n",
       "      <th>Language</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>write csv</td>\n",
       "      <td>https://github.com/sentinel-hub/sentinelhub-py...</td>\n",
       "      <td>def write_csv(filename, data, delimiter=CSV_DE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Python</td>\n",
       "      <td>0.939095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>write csv</td>\n",
       "      <td>https://github.com/jordanjoz1/flickr-views-cou...</td>\n",
       "      <td>def write_to_csv(fname, header, rows): with op...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Python</td>\n",
       "      <td>0.895038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>write csv</td>\n",
       "      <td>https://github.com/fastai/fastai/blob/9fb84a5c...</td>\n",
       "      <td>def write_csv(self): # Get first element's fil...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Python</td>\n",
       "      <td>0.884143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>write csv</td>\n",
       "      <td>https://github.com/majerteam/sylk_parser/blob/...</td>\n",
       "      <td>def to_csv(self, fbuf, quotechar='\"', delimite...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Python</td>\n",
       "      <td>0.986671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>write csv</td>\n",
       "      <td>https://github.com/gem/oq-engine/blob/8294553a...</td>\n",
       "      <td>def write_to_csv(self, filename): ''' Exports ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Python</td>\n",
       "      <td>0.909797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      Query                                          GitHubUrl  \\\n",
       "0           0  write csv  https://github.com/sentinel-hub/sentinelhub-py...   \n",
       "1           1  write csv  https://github.com/jordanjoz1/flickr-views-cou...   \n",
       "2           2  write csv  https://github.com/fastai/fastai/blob/9fb84a5c...   \n",
       "3           3  write csv  https://github.com/majerteam/sylk_parser/blob/...   \n",
       "4           4  write csv  https://github.com/gem/oq-engine/blob/8294553a...   \n",
       "\n",
       "                                                code  qid  docno  relevance  \\\n",
       "0  def write_csv(filename, data, delimiter=CSV_DE...    0      0          3   \n",
       "1  def write_to_csv(fname, header, rows): with op...    0      1          3   \n",
       "2  def write_csv(self): # Get first element's fil...    0      2          3   \n",
       "3  def to_csv(self, fbuf, quotechar='\"', delimite...    0      3          3   \n",
       "4  def write_to_csv(self, filename): ''' Exports ...    0      4          3   \n",
       "\n",
       "  Language       sim  \n",
       "0   Python  0.939095  \n",
       "1   Python  0.895038  \n",
       "2   Python  0.884143  \n",
       "3   Python  0.986671  \n",
       "4   Python  0.909797  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/test_data.csv').rename(columns = {'Query_id': 'qid', 'Doc_id':'docno'})\n",
    "df['sim'] = relevance_predictions\n",
    "df[[\"sim\",'qid','docno']].to_csv('data/test_relevance.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Optional TODO_: Evaluating the different models (20 points total; this is part 1)\n",
    "\n",
    " In the code above, we save the model's parameter for the most recent epoch and an extra directory for saving the model with the highest accuracy so far on the validation data. How much training does the model actually need to recognize relevance? Would one epoch be enough? What if we did 10? or 100? (100 might be too many for Great Lakes limits...). In this **optional part**, we'll describe a series of steps you can take to explore this part!\n",
    " \n",
    "Most of this optional part consists of changing or extending the code above using regular python/pandas things (no deep learning) so this is accessible to anyone. It will require you to figure out how some of the code does work though, so it's useful in general. \n",
    " \n",
    "Here's what you need to do:\n",
    "* Right now the model trains for 3 epochs total. Increase that number to 5 or more. There's a 3-hour limit per session for GPUs in Great Lakes so if you've completed all of part 1, it's worth getting a fresh session to get all 3 hours again. You can increase the number of epochs if you want too.\n",
    "* When training, we save the last checkpoint (overwriting the previous result) and also see if this is the \"best\" model and save that too. You will need to add more code here to save the model after every epoch. The code to do the saving is already shown in that block, so you'll need to figure out which parts to re-use _and_ be sure to change the directory. Look for \"Optional part 1 goes here\" on where to start\n",
    "* After training completes, let's see how well each of the models does on the test set. We've already provided some code that does the evaluation on the best performing model. Add more code so that it evaluates the models you just saved for each epoch (look for \"Optional part 2 goes here\") and make a plot of the performance on the test set\n",
    "* To measure the impact on NDCG, we'll need to calculate the different bi-encoders' relevance estimates to different files to use in Part 2. You'll need to add more code that loads in each of these models from the checkpoint directories and runs the inference\n",
    "* In the Part 4, just write the different models' predictions to separate files. You might combine this with the part 3 code if it's easier to do there. We haven't marked a spot for it explicitly, but you'll use these files in part 2.\n",
    "\n",
    "The output of Part 1 should be a plot showing the F1 performance per epoch and a list of files for each epoch's trained model's relevance predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Where to save things\n",
    "        self.data_dir = './data'\n",
    "        self.model_type = 'roberta'\n",
    "        self.model_name_or_path = 'microsoft/codebert-base'\n",
    "        self.task_name = 'codesearch'\n",
    "        self.output_dir = './models'\n",
    "        self.output_mode = 'codesearch'\n",
    "\n",
    "        # These are going to be your most common hyperparameters to change.\n",
    "        # If you want to do deep learning stuff, it's worth learning a bit\n",
    "        # about what they are and what they do.\n",
    "        self.train_batch_size = 64\n",
    "        self.eval_batch_size = 64\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 1e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 5 # NOTE: Change this to 1 if debugging so it runs faster, original: 3\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.n_gpu = 1\n",
    "        self.no_cuda = False\n",
    "\n",
    "        # These are mostly configuration options for which pieces to run\n",
    "        self.config_name = \"\"\n",
    "        self.tokenizer_name = \"\"\n",
    "        self.cache_dir = \"\"\n",
    "        self.max_seq_length = 200\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.do_predict = False\n",
    "        self.evaluate_during_training = False\n",
    "        self.do_lower_case = False\n",
    "\n",
    "        # How often we save things\n",
    "        self.logging_steps = 1000\n",
    "        self.save_steps = 1000\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.seed = 42\n",
    "        \n",
    "        # Ignore all of these\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "        self.local_rank = -1\n",
    "        self.server_ip = \"\"\n",
    "        self.server_port = \"\"\n",
    "        \n",
    "        # Input and output files.\n",
    "        #\n",
    "        # TODO: Change the training file to train_300k.txt when ready\n",
    "        #\n",
    "        self.train_file = \"train_300k.txt\" # CHANGE ME WHEN READY TO TRAIN!!!!!\n",
    "        self.dev_file = \"valid.txt\"\n",
    "        self.test_file = \"test_data.txt\"\n",
    "        self.pred_model_dir = './models/checkpoint-best'\n",
    "        self.test_result_dir = './results/'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer, optimizer):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "\n",
    "    # The sampler specifies how we should access the training data, which\n",
    "    # in this case is in a random order\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    \n",
    "    # TODO: Initailize the DataLoader (https://pytorch.org/docs/stable/data.html)\n",
    "    # so that it \n",
    "    # - loads from the provided train_dataset \n",
    "    # - samples using our sampler\n",
    "    # - uses the specified batch size\n",
    "    #\n",
    "    # NOTE: The batch size is pretty important! This says how many examples to train on \n",
    "    # at one time. If you recall, we talked about Stocastic Gradient Descent (SGD) that\n",
    "    # updates based on one instance at a time (e.g., changing the dog t-shirt size after seeing one dog)\n",
    "    # versus Gradient Descent (GD) that updates after all the data. SGD is much faster to\n",
    "    # converge to the \"right\" parameters but can make many missteps. The batch size \n",
    "    # says we can look at more than one instance at a time in determining how to update\n",
    "    # our parameters (e.g., look at a few dogs at a time to determine how to best update \n",
    "    # the t-shirt size, rather than just one dog or all the dogs)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.train_batch_size, sampler=train_sampler)\n",
    "    \n",
    "    # How many total steps we'll take\n",
    "    t_total = len(train_dataloader) //  args.num_train_epochs\n",
    "\n",
    "    # The scheduler helps decide how quickly to update the weights based on how much\n",
    "    # training data we've seen. \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, args.warmup_steps, t_total)\n",
    "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "    scheduler_last = os.path.join(checkpoint_last, 'scheduler.pt')\n",
    "    if os.path.exists(scheduler_last):\n",
    "        scheduler.load_state_dict(torch.load(scheduler_last))\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                args.train_batch_size * args.gradient_accumulation_steps * (\n",
    "                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = args.start_step\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    best_acc = 0.0\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Note that this \"train_iterator\" is just tdqm wrapper that prints out which\n",
    "    # epoch we're currently in.     \n",
    "    train_iterator = trange(args.start_epoch, int(args.num_train_epochs), desc=\"Epoch\")\n",
    "    \n",
    "    set_seed(args) \n",
    "    \n",
    "    # This tells pytorch that we're going to be changing the parameters so it needs\n",
    "    # to start keeping track of stuff\n",
    "    model.train()\n",
    "    for idx, _ in enumerate(train_iterator):\n",
    "        \n",
    "        # Keep train of the training loss (how \"bad\" the performance is) for this epohch\n",
    "        tr_loss = 0.0\n",
    "        \n",
    "        # For one epoch, loop over all the data, one batch at a time\n",
    "        for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            inputs = {'query_token_ids': batch[0],\n",
    "                      'code_token_ids': batch[1],\n",
    "                      'labels': batch[3]}\n",
    "            \n",
    "            ouputs = model(**inputs)\n",
    "            loss = ouputs[0]        \n",
    "            \n",
    "            # Do the back propagration to figure out which parameters to change.\n",
    "            # It's that easy!\n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            \n",
    "            # Update the parameters of our model based on the gradient and whatever\n",
    "            # else the optimizer is keeping track of\n",
    "            optimizer.step() \n",
    "            scheduler.step()  \n",
    "            \n",
    "            # This sets the gradient to zero before doing next update so we don't\n",
    "            # accidentally update the model based on the last batch's performance\n",
    "            model.zero_grad() \n",
    "            global_step += 1\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                break\n",
    "\n",
    "        # Once we finish an epoch, evaluate the model on the development data and see\n",
    "        # how well it does. We'll use this information to decide which version of\n",
    "        # the parameters to use.\n",
    "        results = evaluate(args, model, tokenizer, checkpoint=str(args.start_epoch + idx))\n",
    "\n",
    "        # \n",
    "        # Save the model and if we've already saved it, overwrite that saved model with \n",
    "        # the newly-trained parameters\n",
    "        #\n",
    "        last_output_dir = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "        if not os.path.exists(last_output_dir):\n",
    "            os.makedirs(last_output_dir)\n",
    "        model_to_save = model.module if hasattr(model,\n",
    "                                                'module') else model \n",
    "        model_to_save.save_pretrained(last_output_dir)\n",
    "        logger.info(\"Saving model checkpoint to %s\", last_output_dir)\n",
    "        idx_file = os.path.join(last_output_dir, 'idx_file.txt')\n",
    "        with open(idx_file, 'w', encoding='utf-8') as idxf:\n",
    "            idxf.write(str(args.start_epoch + idx) + '\\n')\n",
    "\n",
    "        torch.save(optimizer.state_dict(), os.path.join(last_output_dir, \"optimizer.pt\"))\n",
    "        torch.save(scheduler.state_dict(), os.path.join(last_output_dir, \"scheduler.pt\"))\n",
    "        logger.info(\"Saving optimizer and scheduler states to %s\", last_output_dir)\n",
    "\n",
    "        step_file = os.path.join(last_output_dir, 'step_file.txt')\n",
    "        with open(step_file, 'w', encoding='utf-8') as stepf:\n",
    "            stepf.write(str(global_step) + '\\n')\n",
    "\n",
    "        # Optional part 1 goes here\n",
    "        # When training, we save the last checkpoint (overwriting the previous result)\n",
    "        # and also see if this is the \"best\" model and save that too. \n",
    "        # You will need to add more code here to save the model after every epoch. \n",
    "        # The code to do the saving is already shown in that block, \n",
    "        # so you'll need to figure out which parts to re-use and be sure to change the directory. \n",
    "        epoch_output_dir = os.path.join(args.output_dir, 'epoch_model_{}'.format(idx))\n",
    "        if not os.path.exists(epoch_output_dir):\n",
    "            os.makedirs(epoch_output_dir)\n",
    "        model_to_save = model.module if hasattr(model,'module') else model \n",
    "        model_to_save.save_pretrained(epoch_output_dir)\n",
    "        torch.save(args, os.path.join(epoch_output_dir, 'training_{}.bin'.format(idx)))\n",
    "        logger.info(\"Saving the model after every epoch to %s\", epoch_output_dir)\n",
    "\n",
    "        torch.save(optimizer.state_dict(), os.path.join(epoch_output_dir, \"optimizer.pt\"))\n",
    "        torch.save(scheduler.state_dict(), os.path.join(epoch_output_dir, \"scheduler.pt\"))\n",
    "        logger.info(\"Saving optimizer and scheduler states to %s\", epoch_output_dir)\n",
    "\n",
    "\n",
    "        #\n",
    "        # If this model is better (on the training data) than the models from any of the \n",
    "        # past checkpoints, then keep a separate record of that too\n",
    "        #\n",
    "        if (results['acc'] > best_acc):\n",
    "            best_acc = results['acc']\n",
    "            output_dir = os.path.join(args.output_dir, 'checkpoint-best')\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model  \n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            torch.save(args, os.path.join(output_dir, 'training_{}.bin'.format(idx)))\n",
    "            logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "            torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "            torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "            logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 12:53:47 - INFO - __main__ -   reload model from ./models/checkpoint-last, resume from 5 epoch\n"
     ]
    }
   ],
   "source": [
    "# Setup CUDA so we can run on the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "args.device = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "\n",
    "# Set seed\n",
    "set_seed(args)\n",
    "\n",
    "# This code will help us if we restart training and want to pick back up where we left off\n",
    "args.start_epoch = 0\n",
    "args.start_step = 0\n",
    "checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\n",
    "    args.model_name_or_path = os.path.join(checkpoint_last, 'pytorch_model.bin')\n",
    "    args.config_name = os.path.join(checkpoint_last, 'config.json')\n",
    "    idx_file = os.path.join(checkpoint_last, 'idx_file.txt')\n",
    "    with open(idx_file, encoding='utf-8') as idxf:\n",
    "        args.start_epoch = int(idxf.readlines()[0].strip()) + 1\n",
    "\n",
    "    step_file = os.path.join(checkpoint_last, 'step_file.txt')\n",
    "    if os.path.exists(step_file):\n",
    "        with open(step_file, encoding='utf-8') as stepf:\n",
    "            args.start_step = int(stepf.readlines()[0].strip())\n",
    "    logger.info(\"reload model from {}, resume from {} epoch\".format(checkpoint_last, args.start_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/sijuntao/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We'll specify some general configurations that tell the models what kind\n",
    "# of parameters to use and how to turn incoming text/code data into identifiers for \n",
    "# processing with the neural network \n",
    "#\n",
    "# We set num_labels = 1 because this is a regression class\n",
    "# (compared to a classification task with many class labels)\n",
    "\n",
    "num_labels = 1\n",
    "config = RobertaConfig.from_pretrained('microsoft/codebert-base',\n",
    "                                      num_labels=num_labels, finetuning_task=args.task_name)\n",
    "# We'll treat relevance as a regression problem\n",
    "config.problem_type = 'regression'\n",
    "\n",
    "# If you remember from our neural language model part of the lecture, we talked\n",
    "# about one language model that gets fed a series of words to predict the next\n",
    "# and each word is mapped to an embedding. The \"tokenizer\" specifies how to \n",
    "# break up words into tokens but it frequently doesn't use just spaces!\n",
    "# In fact, most tokenizers break words into *pieces* to reduce the size of the\n",
    "# vocabularly (fewer embeddings!) so we need to specify which tokenizer to use.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "\n",
    "# Now comes the time to define our model. Let's specify the model class (which we'll need later)\n",
    "model_class = CodeSearchBiencoderModel\n",
    "# And we'll instantiate the model itself.\n",
    "model = CodeSearchBiencoderModel(config)\n",
    "\n",
    "# Now comes the magic where we specify the two encoders. Conveniently for us,\n",
    "# there's actually a very recent langauge model that knows *both* code and human language!!\n",
    "# We'll use this set of parameters to initialize *each* of our encoders. Over time,\n",
    "# each encoder's parameters will start to become different since one side is going\n",
    "# to learn how to encode queries better and the other will learn how to encode \n",
    "# code documents.\n",
    "#\n",
    "# NOTE: There's nothing stopping us from trying other parameters for the\n",
    "# encoders too. If you're feeling curious you could swap in any RoBERTa model\n",
    "# for the query encoder and it will just work.\n",
    "#\n",
    "# TODO: Initialize each of the coders using the \"from_pretrained\" method and\n",
    "# specifying the pretrained model you want. Here, we'll use the CodeBERT model, \n",
    "# which is hosted on Huggingface https://huggingface.co/microsoft/codebert-base\n",
    "# You should pass in the full name of the pretrained model (which includes the \"/\").\n",
    "# Note that this code is going to look the same for both encoders and may \n",
    "# seem kind of easy to do but we want you to see how to do it yourself. :) \n",
    "model.query_encoder = (model.query_encoder).from_pretrained(\"microsoft/codebert-base\")\n",
    "model.code_encoder = (model.code_encoder).from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# This will move the model's parameters onto the GPU so it runs fast\n",
    "model.to(args.device)\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': args.weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "# Remember how we talked about stochastic gradient descent (SGD)? Well, it's not\n",
    "# the only way to update parameters. There are many (many) ways to do this\n",
    "# and the usual standard is actually AdamW which uses a bit of bookkeeping to figure\n",
    "# out how to update the weights more efficiently so the model learns faster.\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "\n",
    "# If we're restarting, load the optimizer's state at the last time step\n",
    "optimizer_last = os.path.join(checkpoint_last, 'optimizer.pt')\n",
    "if os.path.exists(optimizer_last):\n",
    "    optimizer.load_state_dict(torch.load(optimizer_last))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 12:54:09 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x14848f4dbca0>\n",
      "12/06/2022 12:54:09 - INFO - run_classifier -   Loading features from cached file ./data/cached_train_train_300k_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 12:54:28 - INFO - __main__ -   ***** Running training *****\n",
      "12/06/2022 12:54:28 - INFO - __main__ -     Num examples = 439686\n",
      "12/06/2022 12:54:28 - INFO - __main__ -     Num Epochs = 5\n",
      "12/06/2022 12:54:28 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "12/06/2022 12:54:28 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "12/06/2022 12:54:28 - INFO - __main__ -     Total optimization steps = 1374\n",
      "Epoch: 0it [00:00, ?it/s]\n",
      "12/06/2022 12:54:28 - INFO - __main__ -    global_step = 34355, average loss = 0.0\n",
      "12/06/2022 12:54:28 - INFO - __main__ -   Saving model checkpoint to ./models\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "# Load in the training dataset. Here, we've handled most of the data preprocessing for you\n",
    "train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, ttype='train')\n",
    "\n",
    "# Call the training function that we defined above\n",
    "global_step, tr_loss = train(args, train_dataset, model, tokenizer, optimizer)\n",
    "logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "\n",
    "# Save the trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  \n",
    "model_to_save.save_pretrained(args.output_dir)\n",
    "tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "# model = AutoModel.from_pretrained(args.output_dir)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "# model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 12:54:34 - INFO - __main__ -   Evaluate the following checkpoint: ./models/epoch_model_0\n",
      "Some weights of the model checkpoint at ./models/epoch_model_0 were not used when initializing RobertaModel: ['code_encoder.encoder.layer.8.attention.self.value.bias', 'code_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.6.attention.output.dense.weight', 'query_encoder.encoder.layer.5.intermediate.dense.weight', 'query_encoder.encoder.layer.6.attention.self.key.bias', 'code_encoder.embeddings.position_embeddings.weight', 'code_encoder.encoder.layer.11.attention.output.dense.bias', 'query_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.6.attention.self.key.bias', 'query_encoder.encoder.layer.11.output.LayerNorm.bias', 'code_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.self.query.bias', 'query_encoder.embeddings.LayerNorm.bias', 'code_encoder.encoder.layer.0.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.attention.output.dense.bias', 'code_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.9.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.self.key.bias', 'code_encoder.encoder.layer.5.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.self.key.weight', 'code_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.intermediate.dense.weight', 'query_encoder.encoder.layer.11.intermediate.dense.weight', 'query_encoder.encoder.layer.3.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.11.attention.output.dense.weight', 'query_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.1.attention.output.dense.bias', 'code_encoder.encoder.layer.3.attention.output.dense.weight', 'query_encoder.encoder.layer.4.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.output.dense.bias', 'code_encoder.encoder.layer.6.output.dense.weight', 'code_encoder.encoder.layer.8.attention.self.key.bias', 'query_encoder.encoder.layer.9.attention.self.key.weight', 'query_encoder.encoder.layer.9.intermediate.dense.weight', 'code_encoder.encoder.layer.11.attention.self.key.weight', 'query_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.attention.self.value.bias', 'code_encoder.encoder.layer.2.intermediate.dense.weight', 'code_encoder.encoder.layer.9.intermediate.dense.bias', 'code_encoder.encoder.layer.8.output.dense.bias', 'query_encoder.encoder.layer.9.attention.self.key.bias', 'code_encoder.encoder.layer.9.attention.output.dense.weight', 'query_encoder.encoder.layer.7.attention.self.key.bias', 'code_encoder.encoder.layer.1.attention.self.query.bias', 'code_encoder.encoder.layer.7.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.output.dense.weight', 'query_encoder.encoder.layer.6.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.output.LayerNorm.bias', 'query_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.output.dense.weight', 'code_encoder.encoder.layer.5.attention.self.query.weight', 'query_encoder.encoder.layer.6.attention.self.query.weight', 'query_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.intermediate.dense.bias', 'query_encoder.encoder.layer.11.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.output.dense.bias', 'code_encoder.encoder.layer.11.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.intermediate.dense.weight', 'code_encoder.encoder.layer.3.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.dense.bias', 'query_encoder.encoder.layer.2.intermediate.dense.bias', 'code_encoder.encoder.layer.9.attention.self.key.bias', 'query_encoder.encoder.layer.2.intermediate.dense.weight', 'code_encoder.encoder.layer.0.intermediate.dense.weight', 'code_encoder.encoder.layer.8.intermediate.dense.bias', 'code_encoder.encoder.layer.1.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.self.query.weight', 'query_encoder.encoder.layer.4.attention.self.value.bias', 'query_encoder.encoder.layer.2.attention.self.value.bias', 'query_encoder.encoder.layer.3.attention.output.dense.weight', 'query_encoder.encoder.layer.0.intermediate.dense.weight', 'query_encoder.encoder.layer.2.attention.self.value.weight', 'code_encoder.encoder.layer.7.intermediate.dense.bias', 'code_encoder.encoder.layer.9.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.self.key.weight', 'code_encoder.encoder.layer.6.intermediate.dense.weight', 'code_encoder.encoder.layer.3.output.dense.weight', 'query_encoder.encoder.layer.10.attention.self.key.weight', 'code_encoder.encoder.layer.2.attention.self.key.weight', 'code_encoder.encoder.layer.1.attention.self.value.bias', 'code_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.7.output.dense.weight', 'code_encoder.encoder.layer.9.attention.self.value.weight', 'code_encoder.encoder.layer.5.output.dense.bias', 'code_encoder.encoder.layer.9.attention.self.query.bias', 'code_encoder.encoder.layer.1.attention.self.value.weight', 'query_encoder.encoder.layer.11.attention.self.key.weight', 'query_encoder.encoder.layer.6.intermediate.dense.bias', 'query_encoder.encoder.layer.8.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.output.dense.weight', 'query_encoder.encoder.layer.0.output.LayerNorm.weight', 'code_encoder.encoder.layer.3.attention.self.key.weight', 'query_encoder.encoder.layer.5.output.dense.weight', 'query_encoder.encoder.layer.5.intermediate.dense.bias', 'query_encoder.encoder.layer.6.output.dense.bias', 'query_encoder.encoder.layer.7.intermediate.dense.bias', 'query_encoder.encoder.layer.2.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.key.weight', 'code_encoder.encoder.layer.11.attention.self.value.weight', 'query_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.attention.self.key.weight', 'code_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.attention.self.query.weight', 'code_encoder.encoder.layer.0.attention.self.value.bias', 'code_encoder.encoder.layer.1.attention.output.dense.weight', 'code_encoder.encoder.layer.6.attention.output.dense.bias', 'code_encoder.encoder.layer.11.attention.self.value.bias', 'code_encoder.encoder.layer.5.intermediate.dense.weight', 'query_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.self.key.weight', 'code_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.1.intermediate.dense.bias', 'query_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.output.dense.bias', 'query_encoder.encoder.layer.4.attention.self.query.weight', 'query_encoder.encoder.layer.1.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.query.bias', 'code_encoder.encoder.layer.2.attention.output.dense.weight', 'query_encoder.encoder.layer.6.attention.self.value.bias', 'query_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.output.dense.bias', 'code_encoder.encoder.layer.7.attention.self.key.bias', 'code_encoder.encoder.layer.6.attention.self.query.bias', 'code_encoder.encoder.layer.10.attention.self.query.weight', 'code_encoder.encoder.layer.3.attention.self.query.bias', 'query_encoder.encoder.layer.5.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.attention.output.dense.bias', 'code_encoder.encoder.layer.7.attention.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.key.bias', 'query_encoder.encoder.layer.9.output.dense.weight', 'code_encoder.encoder.layer.5.output.LayerNorm.bias', 'query_encoder.encoder.layer.4.intermediate.dense.bias', 'query_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.output.LayerNorm.weight', 'code_encoder.encoder.layer.8.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.self.query.bias', 'query_encoder.encoder.layer.1.output.LayerNorm.bias', 'code_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.2.attention.self.key.bias', 'code_encoder.encoder.layer.6.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.output.dense.weight', 'code_encoder.encoder.layer.5.attention.self.key.bias', 'code_encoder.encoder.layer.7.output.dense.weight', 'query_encoder.encoder.layer.1.output.dense.bias', 'query_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.intermediate.dense.weight', 'code_encoder.encoder.layer.0.attention.self.query.bias', 'query_encoder.encoder.layer.0.attention.self.value.weight', 'code_encoder.encoder.layer.2.output.dense.weight', 'query_encoder.encoder.layer.7.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.output.dense.weight', 'code_encoder.embeddings.position_ids', 'code_encoder.encoder.layer.3.attention.self.query.weight', 'code_encoder.encoder.layer.4.output.dense.weight', 'code_encoder.encoder.layer.8.attention.self.query.weight', 'code_encoder.encoder.layer.7.intermediate.dense.weight', 'code_encoder.encoder.layer.6.attention.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.value.bias', 'code_encoder.encoder.layer.4.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.output.dense.weight', 'code_encoder.encoder.layer.7.attention.output.dense.bias', 'query_encoder.encoder.layer.8.attention.self.query.bias', 'code_encoder.encoder.layer.9.attention.self.key.weight', 'query_encoder.encoder.layer.7.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.output.dense.weight', 'query_encoder.encoder.layer.0.attention.output.dense.weight', 'query_encoder.encoder.layer.1.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.6.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.key.weight', 'code_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.output.dense.weight', 'query_encoder.encoder.layer.9.output.LayerNorm.weight', 'code_encoder.encoder.layer.7.attention.self.query.bias', 'query_encoder.encoder.layer.3.intermediate.dense.weight', 'code_encoder.encoder.layer.5.attention.self.value.bias', 'code_encoder.encoder.layer.1.intermediate.dense.weight', 'query_encoder.encoder.layer.2.attention.self.query.bias', 'query_encoder.encoder.layer.10.output.LayerNorm.weight', 'code_encoder.encoder.layer.7.output.dense.bias', 'query_encoder.encoder.layer.4.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.output.dense.bias', 'query_encoder.encoder.layer.11.intermediate.dense.bias', 'query_encoder.encoder.layer.1.intermediate.dense.bias', 'code_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.attention.output.dense.bias', 'code_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.attention.self.key.weight', 'query_encoder.encoder.layer.1.attention.self.key.bias', 'query_encoder.encoder.layer.3.output.dense.weight', 'query_encoder.encoder.layer.6.output.dense.weight', 'code_encoder.encoder.layer.9.attention.self.value.bias', 'query_encoder.encoder.layer.11.attention.self.query.weight', 'query_encoder.encoder.layer.1.attention.output.dense.weight', 'code_encoder.encoder.layer.8.output.dense.weight', 'query_encoder.encoder.layer.8.output.dense.weight', 'query_encoder.encoder.layer.11.output.dense.weight', 'code_encoder.encoder.layer.2.attention.self.query.bias', 'code_encoder.encoder.layer.0.attention.self.value.weight', 'code_encoder.encoder.layer.3.attention.self.key.bias', 'code_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.attention.output.dense.bias', 'code_encoder.encoder.layer.3.output.LayerNorm.bias', 'code_encoder.encoder.layer.8.attention.self.query.bias', 'code_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.0.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.attention.self.value.weight', 'code_encoder.embeddings.LayerNorm.bias', 'query_encoder.encoder.layer.10.output.dense.weight', 'query_encoder.encoder.layer.4.output.dense.weight', 'query_encoder.encoder.layer.7.attention.self.query.bias', 'query_encoder.embeddings.word_embeddings.weight', 'code_encoder.encoder.layer.6.attention.self.value.bias', 'query_encoder.encoder.layer.9.intermediate.dense.bias', 'query_encoder.encoder.layer.11.attention.self.value.weight', 'query_encoder.encoder.layer.7.attention.self.value.bias', 'code_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.attention.output.dense.weight', 'query_encoder.encoder.layer.0.attention.self.value.bias', 'code_encoder.encoder.layer.1.attention.self.key.bias', 'code_encoder.encoder.layer.11.attention.self.key.bias', 'query_encoder.encoder.layer.3.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.intermediate.dense.weight', 'code_encoder.encoder.layer.8.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.self.query.bias', 'query_encoder.encoder.layer.5.attention.self.key.bias', 'code_encoder.encoder.layer.0.intermediate.dense.bias', 'query_encoder.encoder.layer.0.output.dense.weight', 'code_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.11.output.dense.bias', 'code_encoder.embeddings.LayerNorm.weight', 'code_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.9.attention.self.query.weight', 'code_encoder.encoder.layer.1.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.attention.self.key.weight', 'code_encoder.encoder.layer.3.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.output.dense.weight', 'query_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.attention.self.query.weight', 'query_encoder.encoder.layer.7.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.attention.self.query.weight', 'query_encoder.encoder.layer.10.intermediate.dense.bias', 'query_encoder.encoder.layer.1.attention.self.query.bias', 'code_encoder.encoder.layer.10.attention.output.dense.bias', 'query_encoder.encoder.layer.7.attention.output.dense.weight', 'query_encoder.encoder.layer.10.attention.self.query.bias', 'code_encoder.encoder.layer.11.output.dense.weight', 'code_encoder.encoder.layer.11.attention.self.query.bias', 'code_encoder.encoder.layer.10.output.dense.bias', 'query_encoder.embeddings.position_ids', 'query_encoder.encoder.layer.7.attention.self.key.weight', 'code_encoder.encoder.layer.2.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.value.weight', 'code_encoder.encoder.layer.4.attention.self.value.bias', 'query_encoder.encoder.layer.3.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.intermediate.dense.weight', 'code_encoder.encoder.layer.4.output.dense.bias', 'code_encoder.encoder.layer.1.output.dense.bias', 'query_encoder.encoder.layer.4.attention.self.key.bias', 'query_encoder.encoder.layer.8.attention.output.dense.bias', 'query_encoder.encoder.layer.9.output.dense.bias', 'query_encoder.encoder.layer.4.attention.output.dense.weight', 'code_encoder.encoder.layer.8.attention.output.dense.bias', 'code_encoder.encoder.layer.3.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.self.query.bias', 'code_encoder.encoder.layer.0.attention.self.query.weight', 'query_encoder.encoder.layer.5.output.dense.bias', 'code_encoder.encoder.layer.9.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.9.attention.self.value.bias', 'code_encoder.encoder.layer.4.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.self.key.weight', 'query_encoder.encoder.layer.2.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.key.bias', 'code_encoder.encoder.layer.5.output.dense.weight', 'code_encoder.encoder.layer.10.intermediate.dense.weight', 'query_encoder.encoder.layer.8.attention.self.value.bias', 'query_encoder.encoder.layer.7.intermediate.dense.weight', 'query_encoder.encoder.layer.0.attention.output.dense.bias', 'code_encoder.encoder.layer.11.intermediate.dense.bias', 'code_encoder.encoder.layer.7.attention.self.value.weight', 'code_encoder.encoder.layer.9.output.dense.bias', 'query_encoder.encoder.layer.0.output.dense.bias', 'code_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.2.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.11.attention.output.dense.bias', 'code_encoder.encoder.layer.5.attention.self.value.weight', 'code_encoder.encoder.layer.10.output.LayerNorm.bias', 'query_encoder.encoder.layer.2.attention.self.key.weight', 'query_encoder.encoder.layer.2.attention.output.dense.bias', 'query_encoder.encoder.layer.8.intermediate.dense.weight', 'query_encoder.encoder.layer.6.attention.output.dense.bias', 'code_encoder.encoder.layer.3.attention.self.value.bias', 'query_encoder.encoder.layer.1.intermediate.dense.weight', 'code_encoder.encoder.layer.1.attention.self.key.weight', 'code_encoder.encoder.layer.8.intermediate.dense.weight', 'code_encoder.encoder.layer.3.attention.output.dense.bias', 'query_encoder.encoder.layer.6.intermediate.dense.weight', 'code_encoder.encoder.layer.0.output.dense.bias', 'code_encoder.encoder.layer.9.attention.self.query.weight', 'code_encoder.encoder.layer.0.attention.output.dense.weight', 'code_encoder.encoder.layer.2.attention.output.dense.bias', 'code_encoder.encoder.layer.10.intermediate.dense.bias', 'query_encoder.encoder.layer.6.attention.self.query.bias', 'query_encoder.encoder.layer.6.attention.self.key.weight', 'query_encoder.encoder.layer.8.intermediate.dense.bias', 'query_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.self.value.weight', 'query_encoder.encoder.layer.3.output.dense.bias', 'query_encoder.encoder.layer.1.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.value.weight', 'query_encoder.encoder.layer.7.attention.self.query.weight', 'query_encoder.encoder.layer.7.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.output.dense.bias', 'code_encoder.encoder.layer.0.attention.self.key.bias', 'query_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.output.dense.bias', 'code_encoder.encoder.layer.3.intermediate.dense.weight', 'code_encoder.encoder.layer.1.output.dense.weight', 'code_encoder.encoder.layer.6.output.dense.bias', 'code_encoder.encoder.layer.10.attention.self.value.bias', 'code_encoder.encoder.layer.2.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.intermediate.dense.bias', 'code_encoder.encoder.layer.6.intermediate.dense.bias', 'query_encoder.encoder.layer.1.attention.self.value.bias', 'code_encoder.encoder.layer.7.attention.self.key.weight', 'query_encoder.encoder.layer.0.attention.self.key.bias', 'query_encoder.encoder.layer.6.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.output.dense.bias', 'query_encoder.encoder.layer.9.attention.self.query.bias', 'code_encoder.encoder.layer.6.output.LayerNorm.weight', 'query_encoder.embeddings.token_type_embeddings.weight', 'query_encoder.encoder.layer.5.attention.self.value.bias', 'query_encoder.encoder.layer.2.output.dense.bias', 'code_encoder.encoder.layer.2.attention.self.value.bias', 'query_encoder.encoder.layer.10.attention.self.value.weight', 'query_encoder.encoder.layer.4.output.dense.bias', 'code_encoder.encoder.layer.2.attention.self.value.weight', 'query_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.query.bias', 'code_encoder.encoder.layer.4.attention.self.query.bias', 'query_encoder.encoder.layer.7.attention.output.dense.bias', 'code_encoder.encoder.layer.7.attention.self.value.bias', 'query_encoder.encoder.layer.0.attention.self.query.bias', 'code_encoder.encoder.layer.8.attention.self.key.weight', 'query_encoder.encoder.layer.10.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.self.query.weight', 'query_encoder.embeddings.LayerNorm.weight', 'query_encoder.encoder.layer.0.attention.self.query.weight', 'code_encoder.embeddings.token_type_embeddings.weight', 'code_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.intermediate.dense.bias', 'code_encoder.encoder.layer.3.intermediate.dense.bias', 'query_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.8.attention.self.value.weight', 'code_encoder.encoder.layer.10.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.self.value.bias', 'query_encoder.embeddings.position_embeddings.weight', 'code_encoder.encoder.layer.1.attention.self.query.weight', 'query_encoder.encoder.layer.5.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.query.weight', 'query_encoder.encoder.layer.2.output.dense.weight', 'code_encoder.encoder.layer.11.attention.self.query.weight', 'code_encoder.encoder.layer.5.intermediate.dense.bias', 'query_encoder.encoder.layer.0.intermediate.dense.bias', 'query_encoder.encoder.layer.4.attention.self.value.weight', 'query_encoder.encoder.layer.1.attention.self.value.weight', 'code_encoder.embeddings.word_embeddings.weight', 'query_encoder.encoder.layer.10.attention.output.dense.weight', 'query_encoder.encoder.layer.3.attention.self.key.weight', 'query_encoder.encoder.layer.0.attention.self.key.weight', 'code_encoder.encoder.layer.8.attention.output.dense.weight', 'query_encoder.encoder.layer.2.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./models/epoch_model_0 and are newly initialized: ['encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/epoch_model_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 12:54:49 - INFO - run_classifier -   Loading features from cached file ./data/cached_dev_valid_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 12:54:53 - INFO - run_classifier -   ***** Running evaluation  *****\n",
      "12/06/2022 12:54:53 - INFO - run_classifier -     Num examples = 83397\n",
      "12/06/2022 12:54:53 - INFO - run_classifier -     Batch size = 64\n",
      "Evaluating: 100%|██████████| 1304/1304 [04:48<00:00,  4.51it/s]\n",
      "12/06/2022 12:59:42 - INFO - __main__ -   Evaluate the following checkpoint: ./models/epoch_model_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_': 0.49941844430854826, 'f1_': 0.0, 'acc_and_f1_': 0.24970922215427413}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./models/epoch_model_1 were not used when initializing RobertaModel: ['code_encoder.encoder.layer.8.attention.self.value.bias', 'code_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.6.attention.output.dense.weight', 'query_encoder.encoder.layer.5.intermediate.dense.weight', 'query_encoder.encoder.layer.6.attention.self.key.bias', 'code_encoder.embeddings.position_embeddings.weight', 'code_encoder.encoder.layer.11.attention.output.dense.bias', 'query_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.6.attention.self.key.bias', 'query_encoder.encoder.layer.11.output.LayerNorm.bias', 'code_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.self.query.bias', 'query_encoder.embeddings.LayerNorm.bias', 'code_encoder.encoder.layer.0.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.attention.output.dense.bias', 'code_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.9.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.self.key.bias', 'code_encoder.encoder.layer.5.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.self.key.weight', 'code_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.intermediate.dense.weight', 'query_encoder.encoder.layer.11.intermediate.dense.weight', 'query_encoder.encoder.layer.3.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.11.attention.output.dense.weight', 'query_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.1.attention.output.dense.bias', 'code_encoder.encoder.layer.3.attention.output.dense.weight', 'query_encoder.encoder.layer.4.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.output.dense.bias', 'code_encoder.encoder.layer.6.output.dense.weight', 'code_encoder.encoder.layer.8.attention.self.key.bias', 'query_encoder.encoder.layer.9.attention.self.key.weight', 'query_encoder.encoder.layer.9.intermediate.dense.weight', 'code_encoder.encoder.layer.11.attention.self.key.weight', 'query_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.attention.self.value.bias', 'code_encoder.encoder.layer.2.intermediate.dense.weight', 'code_encoder.encoder.layer.9.intermediate.dense.bias', 'code_encoder.encoder.layer.8.output.dense.bias', 'query_encoder.encoder.layer.9.attention.self.key.bias', 'code_encoder.encoder.layer.9.attention.output.dense.weight', 'query_encoder.encoder.layer.7.attention.self.key.bias', 'code_encoder.encoder.layer.1.attention.self.query.bias', 'code_encoder.encoder.layer.7.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.output.dense.weight', 'query_encoder.encoder.layer.6.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.output.LayerNorm.bias', 'query_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.output.dense.weight', 'code_encoder.encoder.layer.5.attention.self.query.weight', 'query_encoder.encoder.layer.6.attention.self.query.weight', 'query_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.intermediate.dense.bias', 'query_encoder.encoder.layer.11.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.output.dense.bias', 'code_encoder.encoder.layer.11.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.intermediate.dense.weight', 'code_encoder.encoder.layer.3.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.dense.bias', 'query_encoder.encoder.layer.2.intermediate.dense.bias', 'code_encoder.encoder.layer.9.attention.self.key.bias', 'query_encoder.encoder.layer.2.intermediate.dense.weight', 'code_encoder.encoder.layer.0.intermediate.dense.weight', 'code_encoder.encoder.layer.8.intermediate.dense.bias', 'code_encoder.encoder.layer.1.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.self.query.weight', 'query_encoder.encoder.layer.4.attention.self.value.bias', 'query_encoder.encoder.layer.2.attention.self.value.bias', 'query_encoder.encoder.layer.3.attention.output.dense.weight', 'query_encoder.encoder.layer.0.intermediate.dense.weight', 'query_encoder.encoder.layer.2.attention.self.value.weight', 'code_encoder.encoder.layer.7.intermediate.dense.bias', 'code_encoder.encoder.layer.9.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.self.key.weight', 'code_encoder.encoder.layer.6.intermediate.dense.weight', 'code_encoder.encoder.layer.3.output.dense.weight', 'query_encoder.encoder.layer.10.attention.self.key.weight', 'code_encoder.encoder.layer.2.attention.self.key.weight', 'code_encoder.encoder.layer.1.attention.self.value.bias', 'code_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.7.output.dense.weight', 'code_encoder.encoder.layer.9.attention.self.value.weight', 'code_encoder.encoder.layer.5.output.dense.bias', 'code_encoder.encoder.layer.9.attention.self.query.bias', 'code_encoder.encoder.layer.1.attention.self.value.weight', 'query_encoder.encoder.layer.11.attention.self.key.weight', 'query_encoder.encoder.layer.6.intermediate.dense.bias', 'query_encoder.encoder.layer.8.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.output.dense.weight', 'query_encoder.encoder.layer.0.output.LayerNorm.weight', 'code_encoder.encoder.layer.3.attention.self.key.weight', 'query_encoder.encoder.layer.5.output.dense.weight', 'query_encoder.encoder.layer.5.intermediate.dense.bias', 'query_encoder.encoder.layer.6.output.dense.bias', 'query_encoder.encoder.layer.7.intermediate.dense.bias', 'query_encoder.encoder.layer.2.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.key.weight', 'code_encoder.encoder.layer.11.attention.self.value.weight', 'query_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.attention.self.key.weight', 'code_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.attention.self.query.weight', 'code_encoder.encoder.layer.0.attention.self.value.bias', 'code_encoder.encoder.layer.1.attention.output.dense.weight', 'code_encoder.encoder.layer.6.attention.output.dense.bias', 'code_encoder.encoder.layer.11.attention.self.value.bias', 'code_encoder.encoder.layer.5.intermediate.dense.weight', 'query_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.self.key.weight', 'code_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.1.intermediate.dense.bias', 'query_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.output.dense.bias', 'query_encoder.encoder.layer.4.attention.self.query.weight', 'query_encoder.encoder.layer.1.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.query.bias', 'code_encoder.encoder.layer.2.attention.output.dense.weight', 'query_encoder.encoder.layer.6.attention.self.value.bias', 'query_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.output.dense.bias', 'code_encoder.encoder.layer.7.attention.self.key.bias', 'code_encoder.encoder.layer.6.attention.self.query.bias', 'code_encoder.encoder.layer.10.attention.self.query.weight', 'code_encoder.encoder.layer.3.attention.self.query.bias', 'query_encoder.encoder.layer.5.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.attention.output.dense.bias', 'code_encoder.encoder.layer.7.attention.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.key.bias', 'query_encoder.encoder.layer.9.output.dense.weight', 'code_encoder.encoder.layer.5.output.LayerNorm.bias', 'query_encoder.encoder.layer.4.intermediate.dense.bias', 'query_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.output.LayerNorm.weight', 'code_encoder.encoder.layer.8.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.self.query.bias', 'query_encoder.encoder.layer.1.output.LayerNorm.bias', 'code_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.2.attention.self.key.bias', 'code_encoder.encoder.layer.6.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.output.dense.weight', 'code_encoder.encoder.layer.5.attention.self.key.bias', 'code_encoder.encoder.layer.7.output.dense.weight', 'query_encoder.encoder.layer.1.output.dense.bias', 'query_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.intermediate.dense.weight', 'code_encoder.encoder.layer.0.attention.self.query.bias', 'query_encoder.encoder.layer.0.attention.self.value.weight', 'code_encoder.encoder.layer.2.output.dense.weight', 'query_encoder.encoder.layer.7.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.output.dense.weight', 'code_encoder.embeddings.position_ids', 'code_encoder.encoder.layer.3.attention.self.query.weight', 'code_encoder.encoder.layer.4.output.dense.weight', 'code_encoder.encoder.layer.8.attention.self.query.weight', 'code_encoder.encoder.layer.7.intermediate.dense.weight', 'code_encoder.encoder.layer.6.attention.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.value.bias', 'code_encoder.encoder.layer.4.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.output.dense.weight', 'code_encoder.encoder.layer.7.attention.output.dense.bias', 'query_encoder.encoder.layer.8.attention.self.query.bias', 'code_encoder.encoder.layer.9.attention.self.key.weight', 'query_encoder.encoder.layer.7.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.output.dense.weight', 'query_encoder.encoder.layer.0.attention.output.dense.weight', 'query_encoder.encoder.layer.1.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.6.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.key.weight', 'code_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.output.dense.weight', 'query_encoder.encoder.layer.9.output.LayerNorm.weight', 'code_encoder.encoder.layer.7.attention.self.query.bias', 'query_encoder.encoder.layer.3.intermediate.dense.weight', 'code_encoder.encoder.layer.5.attention.self.value.bias', 'code_encoder.encoder.layer.1.intermediate.dense.weight', 'query_encoder.encoder.layer.2.attention.self.query.bias', 'query_encoder.encoder.layer.10.output.LayerNorm.weight', 'code_encoder.encoder.layer.7.output.dense.bias', 'query_encoder.encoder.layer.4.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.output.dense.bias', 'query_encoder.encoder.layer.11.intermediate.dense.bias', 'query_encoder.encoder.layer.1.intermediate.dense.bias', 'code_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.attention.output.dense.bias', 'code_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.attention.self.key.weight', 'query_encoder.encoder.layer.1.attention.self.key.bias', 'query_encoder.encoder.layer.3.output.dense.weight', 'query_encoder.encoder.layer.6.output.dense.weight', 'code_encoder.encoder.layer.9.attention.self.value.bias', 'query_encoder.encoder.layer.11.attention.self.query.weight', 'query_encoder.encoder.layer.1.attention.output.dense.weight', 'code_encoder.encoder.layer.8.output.dense.weight', 'query_encoder.encoder.layer.8.output.dense.weight', 'query_encoder.encoder.layer.11.output.dense.weight', 'code_encoder.encoder.layer.2.attention.self.query.bias', 'code_encoder.encoder.layer.0.attention.self.value.weight', 'code_encoder.encoder.layer.3.attention.self.key.bias', 'code_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.attention.output.dense.bias', 'code_encoder.encoder.layer.3.output.LayerNorm.bias', 'code_encoder.encoder.layer.8.attention.self.query.bias', 'code_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.0.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.attention.self.value.weight', 'code_encoder.embeddings.LayerNorm.bias', 'query_encoder.encoder.layer.10.output.dense.weight', 'query_encoder.encoder.layer.4.output.dense.weight', 'query_encoder.encoder.layer.7.attention.self.query.bias', 'query_encoder.embeddings.word_embeddings.weight', 'code_encoder.encoder.layer.6.attention.self.value.bias', 'query_encoder.encoder.layer.9.intermediate.dense.bias', 'query_encoder.encoder.layer.11.attention.self.value.weight', 'query_encoder.encoder.layer.7.attention.self.value.bias', 'code_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.attention.output.dense.weight', 'query_encoder.encoder.layer.0.attention.self.value.bias', 'code_encoder.encoder.layer.1.attention.self.key.bias', 'code_encoder.encoder.layer.11.attention.self.key.bias', 'query_encoder.encoder.layer.3.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.intermediate.dense.weight', 'code_encoder.encoder.layer.8.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.self.query.bias', 'query_encoder.encoder.layer.5.attention.self.key.bias', 'code_encoder.encoder.layer.0.intermediate.dense.bias', 'query_encoder.encoder.layer.0.output.dense.weight', 'code_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.11.output.dense.bias', 'code_encoder.embeddings.LayerNorm.weight', 'code_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.9.attention.self.query.weight', 'code_encoder.encoder.layer.1.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.attention.self.key.weight', 'code_encoder.encoder.layer.3.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.output.dense.weight', 'query_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.attention.self.query.weight', 'query_encoder.encoder.layer.7.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.attention.self.query.weight', 'query_encoder.encoder.layer.10.intermediate.dense.bias', 'query_encoder.encoder.layer.1.attention.self.query.bias', 'code_encoder.encoder.layer.10.attention.output.dense.bias', 'query_encoder.encoder.layer.7.attention.output.dense.weight', 'query_encoder.encoder.layer.10.attention.self.query.bias', 'code_encoder.encoder.layer.11.output.dense.weight', 'code_encoder.encoder.layer.11.attention.self.query.bias', 'code_encoder.encoder.layer.10.output.dense.bias', 'query_encoder.embeddings.position_ids', 'query_encoder.encoder.layer.7.attention.self.key.weight', 'code_encoder.encoder.layer.2.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.value.weight', 'code_encoder.encoder.layer.4.attention.self.value.bias', 'query_encoder.encoder.layer.3.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.intermediate.dense.weight', 'code_encoder.encoder.layer.4.output.dense.bias', 'code_encoder.encoder.layer.1.output.dense.bias', 'query_encoder.encoder.layer.4.attention.self.key.bias', 'query_encoder.encoder.layer.8.attention.output.dense.bias', 'query_encoder.encoder.layer.9.output.dense.bias', 'query_encoder.encoder.layer.4.attention.output.dense.weight', 'code_encoder.encoder.layer.8.attention.output.dense.bias', 'code_encoder.encoder.layer.3.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.self.query.bias', 'code_encoder.encoder.layer.0.attention.self.query.weight', 'query_encoder.encoder.layer.5.output.dense.bias', 'code_encoder.encoder.layer.9.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.9.attention.self.value.bias', 'code_encoder.encoder.layer.4.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.self.key.weight', 'query_encoder.encoder.layer.2.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.key.bias', 'code_encoder.encoder.layer.5.output.dense.weight', 'code_encoder.encoder.layer.10.intermediate.dense.weight', 'query_encoder.encoder.layer.8.attention.self.value.bias', 'query_encoder.encoder.layer.7.intermediate.dense.weight', 'query_encoder.encoder.layer.0.attention.output.dense.bias', 'code_encoder.encoder.layer.11.intermediate.dense.bias', 'code_encoder.encoder.layer.7.attention.self.value.weight', 'code_encoder.encoder.layer.9.output.dense.bias', 'query_encoder.encoder.layer.0.output.dense.bias', 'code_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.2.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.11.attention.output.dense.bias', 'code_encoder.encoder.layer.5.attention.self.value.weight', 'code_encoder.encoder.layer.10.output.LayerNorm.bias', 'query_encoder.encoder.layer.2.attention.self.key.weight', 'query_encoder.encoder.layer.2.attention.output.dense.bias', 'query_encoder.encoder.layer.8.intermediate.dense.weight', 'query_encoder.encoder.layer.6.attention.output.dense.bias', 'code_encoder.encoder.layer.3.attention.self.value.bias', 'query_encoder.encoder.layer.1.intermediate.dense.weight', 'code_encoder.encoder.layer.1.attention.self.key.weight', 'code_encoder.encoder.layer.8.intermediate.dense.weight', 'code_encoder.encoder.layer.3.attention.output.dense.bias', 'query_encoder.encoder.layer.6.intermediate.dense.weight', 'code_encoder.encoder.layer.0.output.dense.bias', 'code_encoder.encoder.layer.9.attention.self.query.weight', 'code_encoder.encoder.layer.0.attention.output.dense.weight', 'code_encoder.encoder.layer.2.attention.output.dense.bias', 'code_encoder.encoder.layer.10.intermediate.dense.bias', 'query_encoder.encoder.layer.6.attention.self.query.bias', 'query_encoder.encoder.layer.6.attention.self.key.weight', 'query_encoder.encoder.layer.8.intermediate.dense.bias', 'query_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.self.value.weight', 'query_encoder.encoder.layer.3.output.dense.bias', 'query_encoder.encoder.layer.1.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.value.weight', 'query_encoder.encoder.layer.7.attention.self.query.weight', 'query_encoder.encoder.layer.7.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.output.dense.bias', 'code_encoder.encoder.layer.0.attention.self.key.bias', 'query_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.output.dense.bias', 'code_encoder.encoder.layer.3.intermediate.dense.weight', 'code_encoder.encoder.layer.1.output.dense.weight', 'code_encoder.encoder.layer.6.output.dense.bias', 'code_encoder.encoder.layer.10.attention.self.value.bias', 'code_encoder.encoder.layer.2.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.intermediate.dense.bias', 'code_encoder.encoder.layer.6.intermediate.dense.bias', 'query_encoder.encoder.layer.1.attention.self.value.bias', 'code_encoder.encoder.layer.7.attention.self.key.weight', 'query_encoder.encoder.layer.0.attention.self.key.bias', 'query_encoder.encoder.layer.6.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.output.dense.bias', 'query_encoder.encoder.layer.9.attention.self.query.bias', 'code_encoder.encoder.layer.6.output.LayerNorm.weight', 'query_encoder.embeddings.token_type_embeddings.weight', 'query_encoder.encoder.layer.5.attention.self.value.bias', 'query_encoder.encoder.layer.2.output.dense.bias', 'code_encoder.encoder.layer.2.attention.self.value.bias', 'query_encoder.encoder.layer.10.attention.self.value.weight', 'query_encoder.encoder.layer.4.output.dense.bias', 'code_encoder.encoder.layer.2.attention.self.value.weight', 'query_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.query.bias', 'code_encoder.encoder.layer.4.attention.self.query.bias', 'query_encoder.encoder.layer.7.attention.output.dense.bias', 'code_encoder.encoder.layer.7.attention.self.value.bias', 'query_encoder.encoder.layer.0.attention.self.query.bias', 'code_encoder.encoder.layer.8.attention.self.key.weight', 'query_encoder.encoder.layer.10.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.self.query.weight', 'query_encoder.embeddings.LayerNorm.weight', 'query_encoder.encoder.layer.0.attention.self.query.weight', 'code_encoder.embeddings.token_type_embeddings.weight', 'code_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.intermediate.dense.bias', 'code_encoder.encoder.layer.3.intermediate.dense.bias', 'query_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.8.attention.self.value.weight', 'code_encoder.encoder.layer.10.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.self.value.bias', 'query_encoder.embeddings.position_embeddings.weight', 'code_encoder.encoder.layer.1.attention.self.query.weight', 'query_encoder.encoder.layer.5.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.query.weight', 'query_encoder.encoder.layer.2.output.dense.weight', 'code_encoder.encoder.layer.11.attention.self.query.weight', 'code_encoder.encoder.layer.5.intermediate.dense.bias', 'query_encoder.encoder.layer.0.intermediate.dense.bias', 'query_encoder.encoder.layer.4.attention.self.value.weight', 'query_encoder.encoder.layer.1.attention.self.value.weight', 'code_encoder.embeddings.word_embeddings.weight', 'query_encoder.encoder.layer.10.attention.output.dense.weight', 'query_encoder.encoder.layer.3.attention.self.key.weight', 'query_encoder.encoder.layer.0.attention.self.key.weight', 'code_encoder.encoder.layer.8.attention.output.dense.weight', 'query_encoder.encoder.layer.2.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./models/epoch_model_1 and are newly initialized: ['encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/epoch_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 13:00:01 - INFO - run_classifier -   Loading features from cached file ./data/cached_dev_valid_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 13:00:04 - INFO - run_classifier -   ***** Running evaluation  *****\n",
      "12/06/2022 13:00:04 - INFO - run_classifier -     Num examples = 83397\n",
      "12/06/2022 13:00:04 - INFO - run_classifier -     Batch size = 64\n",
      "Evaluating: 100%|██████████| 1304/1304 [04:48<00:00,  4.52it/s]\n",
      "12/06/2022 13:04:53 - INFO - __main__ -   Evaluate the following checkpoint: ./models/epoch_model_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_': 0.5005815556914517, 'f1_': 0.667183404717765, 'acc_and_f1_': 0.5838824802046083}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./models/epoch_model_2 were not used when initializing RobertaModel: ['code_encoder.encoder.layer.8.attention.self.value.bias', 'code_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.6.attention.output.dense.weight', 'query_encoder.encoder.layer.5.intermediate.dense.weight', 'query_encoder.encoder.layer.6.attention.self.key.bias', 'code_encoder.embeddings.position_embeddings.weight', 'code_encoder.encoder.layer.11.attention.output.dense.bias', 'query_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.6.attention.self.key.bias', 'query_encoder.encoder.layer.11.output.LayerNorm.bias', 'code_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.self.query.bias', 'query_encoder.embeddings.LayerNorm.bias', 'code_encoder.encoder.layer.0.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.attention.output.dense.bias', 'code_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.9.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.self.key.bias', 'code_encoder.encoder.layer.5.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.self.key.weight', 'code_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.intermediate.dense.weight', 'query_encoder.encoder.layer.11.intermediate.dense.weight', 'query_encoder.encoder.layer.3.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.11.attention.output.dense.weight', 'query_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.1.attention.output.dense.bias', 'code_encoder.encoder.layer.3.attention.output.dense.weight', 'query_encoder.encoder.layer.4.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.output.dense.bias', 'code_encoder.encoder.layer.6.output.dense.weight', 'code_encoder.encoder.layer.8.attention.self.key.bias', 'query_encoder.encoder.layer.9.attention.self.key.weight', 'query_encoder.encoder.layer.9.intermediate.dense.weight', 'code_encoder.encoder.layer.11.attention.self.key.weight', 'query_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.attention.self.value.bias', 'code_encoder.encoder.layer.2.intermediate.dense.weight', 'code_encoder.encoder.layer.9.intermediate.dense.bias', 'code_encoder.encoder.layer.8.output.dense.bias', 'query_encoder.encoder.layer.9.attention.self.key.bias', 'code_encoder.encoder.layer.9.attention.output.dense.weight', 'query_encoder.encoder.layer.7.attention.self.key.bias', 'code_encoder.encoder.layer.1.attention.self.query.bias', 'code_encoder.encoder.layer.7.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.output.dense.weight', 'query_encoder.encoder.layer.6.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.output.LayerNorm.bias', 'query_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.output.dense.weight', 'code_encoder.encoder.layer.5.attention.self.query.weight', 'query_encoder.encoder.layer.6.attention.self.query.weight', 'query_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.intermediate.dense.bias', 'query_encoder.encoder.layer.11.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.output.dense.bias', 'code_encoder.encoder.layer.11.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.intermediate.dense.weight', 'code_encoder.encoder.layer.3.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.dense.bias', 'query_encoder.encoder.layer.2.intermediate.dense.bias', 'code_encoder.encoder.layer.9.attention.self.key.bias', 'query_encoder.encoder.layer.2.intermediate.dense.weight', 'code_encoder.encoder.layer.0.intermediate.dense.weight', 'code_encoder.encoder.layer.8.intermediate.dense.bias', 'code_encoder.encoder.layer.1.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.self.query.weight', 'query_encoder.encoder.layer.4.attention.self.value.bias', 'query_encoder.encoder.layer.2.attention.self.value.bias', 'query_encoder.encoder.layer.3.attention.output.dense.weight', 'query_encoder.encoder.layer.0.intermediate.dense.weight', 'query_encoder.encoder.layer.2.attention.self.value.weight', 'code_encoder.encoder.layer.7.intermediate.dense.bias', 'code_encoder.encoder.layer.9.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.self.key.weight', 'code_encoder.encoder.layer.6.intermediate.dense.weight', 'code_encoder.encoder.layer.3.output.dense.weight', 'query_encoder.encoder.layer.10.attention.self.key.weight', 'code_encoder.encoder.layer.2.attention.self.key.weight', 'code_encoder.encoder.layer.1.attention.self.value.bias', 'code_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.7.output.dense.weight', 'code_encoder.encoder.layer.9.attention.self.value.weight', 'code_encoder.encoder.layer.5.output.dense.bias', 'code_encoder.encoder.layer.9.attention.self.query.bias', 'code_encoder.encoder.layer.1.attention.self.value.weight', 'query_encoder.encoder.layer.11.attention.self.key.weight', 'query_encoder.encoder.layer.6.intermediate.dense.bias', 'query_encoder.encoder.layer.8.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.output.dense.weight', 'query_encoder.encoder.layer.0.output.LayerNorm.weight', 'code_encoder.encoder.layer.3.attention.self.key.weight', 'query_encoder.encoder.layer.5.output.dense.weight', 'query_encoder.encoder.layer.5.intermediate.dense.bias', 'query_encoder.encoder.layer.6.output.dense.bias', 'query_encoder.encoder.layer.7.intermediate.dense.bias', 'query_encoder.encoder.layer.2.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.key.weight', 'code_encoder.encoder.layer.11.attention.self.value.weight', 'query_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.attention.self.key.weight', 'code_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.attention.self.query.weight', 'code_encoder.encoder.layer.0.attention.self.value.bias', 'code_encoder.encoder.layer.1.attention.output.dense.weight', 'code_encoder.encoder.layer.6.attention.output.dense.bias', 'code_encoder.encoder.layer.11.attention.self.value.bias', 'code_encoder.encoder.layer.5.intermediate.dense.weight', 'query_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.self.key.weight', 'code_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.1.intermediate.dense.bias', 'query_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.output.dense.bias', 'query_encoder.encoder.layer.4.attention.self.query.weight', 'query_encoder.encoder.layer.1.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.query.bias', 'code_encoder.encoder.layer.2.attention.output.dense.weight', 'query_encoder.encoder.layer.6.attention.self.value.bias', 'query_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.output.dense.bias', 'code_encoder.encoder.layer.7.attention.self.key.bias', 'code_encoder.encoder.layer.6.attention.self.query.bias', 'code_encoder.encoder.layer.10.attention.self.query.weight', 'code_encoder.encoder.layer.3.attention.self.query.bias', 'query_encoder.encoder.layer.5.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.attention.output.dense.bias', 'code_encoder.encoder.layer.7.attention.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.key.bias', 'query_encoder.encoder.layer.9.output.dense.weight', 'code_encoder.encoder.layer.5.output.LayerNorm.bias', 'query_encoder.encoder.layer.4.intermediate.dense.bias', 'query_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.output.LayerNorm.weight', 'code_encoder.encoder.layer.8.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.self.query.bias', 'query_encoder.encoder.layer.1.output.LayerNorm.bias', 'code_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.2.attention.self.key.bias', 'code_encoder.encoder.layer.6.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.output.dense.weight', 'code_encoder.encoder.layer.5.attention.self.key.bias', 'code_encoder.encoder.layer.7.output.dense.weight', 'query_encoder.encoder.layer.1.output.dense.bias', 'query_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.intermediate.dense.weight', 'code_encoder.encoder.layer.0.attention.self.query.bias', 'query_encoder.encoder.layer.0.attention.self.value.weight', 'code_encoder.encoder.layer.2.output.dense.weight', 'query_encoder.encoder.layer.7.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.output.dense.weight', 'code_encoder.embeddings.position_ids', 'code_encoder.encoder.layer.3.attention.self.query.weight', 'code_encoder.encoder.layer.4.output.dense.weight', 'code_encoder.encoder.layer.8.attention.self.query.weight', 'code_encoder.encoder.layer.7.intermediate.dense.weight', 'code_encoder.encoder.layer.6.attention.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.value.bias', 'code_encoder.encoder.layer.4.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.output.dense.weight', 'code_encoder.encoder.layer.7.attention.output.dense.bias', 'query_encoder.encoder.layer.8.attention.self.query.bias', 'code_encoder.encoder.layer.9.attention.self.key.weight', 'query_encoder.encoder.layer.7.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.output.dense.weight', 'query_encoder.encoder.layer.0.attention.output.dense.weight', 'query_encoder.encoder.layer.1.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.6.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.key.weight', 'code_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.output.dense.weight', 'query_encoder.encoder.layer.9.output.LayerNorm.weight', 'code_encoder.encoder.layer.7.attention.self.query.bias', 'query_encoder.encoder.layer.3.intermediate.dense.weight', 'code_encoder.encoder.layer.5.attention.self.value.bias', 'code_encoder.encoder.layer.1.intermediate.dense.weight', 'query_encoder.encoder.layer.2.attention.self.query.bias', 'query_encoder.encoder.layer.10.output.LayerNorm.weight', 'code_encoder.encoder.layer.7.output.dense.bias', 'query_encoder.encoder.layer.4.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.output.dense.bias', 'query_encoder.encoder.layer.11.intermediate.dense.bias', 'query_encoder.encoder.layer.1.intermediate.dense.bias', 'code_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.attention.output.dense.bias', 'code_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.attention.self.key.weight', 'query_encoder.encoder.layer.1.attention.self.key.bias', 'query_encoder.encoder.layer.3.output.dense.weight', 'query_encoder.encoder.layer.6.output.dense.weight', 'code_encoder.encoder.layer.9.attention.self.value.bias', 'query_encoder.encoder.layer.11.attention.self.query.weight', 'query_encoder.encoder.layer.1.attention.output.dense.weight', 'code_encoder.encoder.layer.8.output.dense.weight', 'query_encoder.encoder.layer.8.output.dense.weight', 'query_encoder.encoder.layer.11.output.dense.weight', 'code_encoder.encoder.layer.2.attention.self.query.bias', 'code_encoder.encoder.layer.0.attention.self.value.weight', 'code_encoder.encoder.layer.3.attention.self.key.bias', 'code_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.attention.output.dense.bias', 'code_encoder.encoder.layer.3.output.LayerNorm.bias', 'code_encoder.encoder.layer.8.attention.self.query.bias', 'code_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.0.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.attention.self.value.weight', 'code_encoder.embeddings.LayerNorm.bias', 'query_encoder.encoder.layer.10.output.dense.weight', 'query_encoder.encoder.layer.4.output.dense.weight', 'query_encoder.encoder.layer.7.attention.self.query.bias', 'query_encoder.embeddings.word_embeddings.weight', 'code_encoder.encoder.layer.6.attention.self.value.bias', 'query_encoder.encoder.layer.9.intermediate.dense.bias', 'query_encoder.encoder.layer.11.attention.self.value.weight', 'query_encoder.encoder.layer.7.attention.self.value.bias', 'code_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.attention.output.dense.weight', 'query_encoder.encoder.layer.0.attention.self.value.bias', 'code_encoder.encoder.layer.1.attention.self.key.bias', 'code_encoder.encoder.layer.11.attention.self.key.bias', 'query_encoder.encoder.layer.3.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.intermediate.dense.weight', 'code_encoder.encoder.layer.8.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.self.query.bias', 'query_encoder.encoder.layer.5.attention.self.key.bias', 'code_encoder.encoder.layer.0.intermediate.dense.bias', 'query_encoder.encoder.layer.0.output.dense.weight', 'code_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.11.output.dense.bias', 'code_encoder.embeddings.LayerNorm.weight', 'code_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.9.attention.self.query.weight', 'code_encoder.encoder.layer.1.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.attention.self.key.weight', 'code_encoder.encoder.layer.3.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.output.dense.weight', 'query_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.attention.self.query.weight', 'query_encoder.encoder.layer.7.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.attention.self.query.weight', 'query_encoder.encoder.layer.10.intermediate.dense.bias', 'query_encoder.encoder.layer.1.attention.self.query.bias', 'code_encoder.encoder.layer.10.attention.output.dense.bias', 'query_encoder.encoder.layer.7.attention.output.dense.weight', 'query_encoder.encoder.layer.10.attention.self.query.bias', 'code_encoder.encoder.layer.11.output.dense.weight', 'code_encoder.encoder.layer.11.attention.self.query.bias', 'code_encoder.encoder.layer.10.output.dense.bias', 'query_encoder.embeddings.position_ids', 'query_encoder.encoder.layer.7.attention.self.key.weight', 'code_encoder.encoder.layer.2.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.value.weight', 'code_encoder.encoder.layer.4.attention.self.value.bias', 'query_encoder.encoder.layer.3.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.intermediate.dense.weight', 'code_encoder.encoder.layer.4.output.dense.bias', 'code_encoder.encoder.layer.1.output.dense.bias', 'query_encoder.encoder.layer.4.attention.self.key.bias', 'query_encoder.encoder.layer.8.attention.output.dense.bias', 'query_encoder.encoder.layer.9.output.dense.bias', 'query_encoder.encoder.layer.4.attention.output.dense.weight', 'code_encoder.encoder.layer.8.attention.output.dense.bias', 'code_encoder.encoder.layer.3.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.self.query.bias', 'code_encoder.encoder.layer.0.attention.self.query.weight', 'query_encoder.encoder.layer.5.output.dense.bias', 'code_encoder.encoder.layer.9.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.9.attention.self.value.bias', 'code_encoder.encoder.layer.4.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.self.key.weight', 'query_encoder.encoder.layer.2.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.key.bias', 'code_encoder.encoder.layer.5.output.dense.weight', 'code_encoder.encoder.layer.10.intermediate.dense.weight', 'query_encoder.encoder.layer.8.attention.self.value.bias', 'query_encoder.encoder.layer.7.intermediate.dense.weight', 'query_encoder.encoder.layer.0.attention.output.dense.bias', 'code_encoder.encoder.layer.11.intermediate.dense.bias', 'code_encoder.encoder.layer.7.attention.self.value.weight', 'code_encoder.encoder.layer.9.output.dense.bias', 'query_encoder.encoder.layer.0.output.dense.bias', 'code_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.2.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.11.attention.output.dense.bias', 'code_encoder.encoder.layer.5.attention.self.value.weight', 'code_encoder.encoder.layer.10.output.LayerNorm.bias', 'query_encoder.encoder.layer.2.attention.self.key.weight', 'query_encoder.encoder.layer.2.attention.output.dense.bias', 'query_encoder.encoder.layer.8.intermediate.dense.weight', 'query_encoder.encoder.layer.6.attention.output.dense.bias', 'code_encoder.encoder.layer.3.attention.self.value.bias', 'query_encoder.encoder.layer.1.intermediate.dense.weight', 'code_encoder.encoder.layer.1.attention.self.key.weight', 'code_encoder.encoder.layer.8.intermediate.dense.weight', 'code_encoder.encoder.layer.3.attention.output.dense.bias', 'query_encoder.encoder.layer.6.intermediate.dense.weight', 'code_encoder.encoder.layer.0.output.dense.bias', 'code_encoder.encoder.layer.9.attention.self.query.weight', 'code_encoder.encoder.layer.0.attention.output.dense.weight', 'code_encoder.encoder.layer.2.attention.output.dense.bias', 'code_encoder.encoder.layer.10.intermediate.dense.bias', 'query_encoder.encoder.layer.6.attention.self.query.bias', 'query_encoder.encoder.layer.6.attention.self.key.weight', 'query_encoder.encoder.layer.8.intermediate.dense.bias', 'query_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.self.value.weight', 'query_encoder.encoder.layer.3.output.dense.bias', 'query_encoder.encoder.layer.1.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.value.weight', 'query_encoder.encoder.layer.7.attention.self.query.weight', 'query_encoder.encoder.layer.7.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.output.dense.bias', 'code_encoder.encoder.layer.0.attention.self.key.bias', 'query_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.output.dense.bias', 'code_encoder.encoder.layer.3.intermediate.dense.weight', 'code_encoder.encoder.layer.1.output.dense.weight', 'code_encoder.encoder.layer.6.output.dense.bias', 'code_encoder.encoder.layer.10.attention.self.value.bias', 'code_encoder.encoder.layer.2.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.intermediate.dense.bias', 'code_encoder.encoder.layer.6.intermediate.dense.bias', 'query_encoder.encoder.layer.1.attention.self.value.bias', 'code_encoder.encoder.layer.7.attention.self.key.weight', 'query_encoder.encoder.layer.0.attention.self.key.bias', 'query_encoder.encoder.layer.6.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.output.dense.bias', 'query_encoder.encoder.layer.9.attention.self.query.bias', 'code_encoder.encoder.layer.6.output.LayerNorm.weight', 'query_encoder.embeddings.token_type_embeddings.weight', 'query_encoder.encoder.layer.5.attention.self.value.bias', 'query_encoder.encoder.layer.2.output.dense.bias', 'code_encoder.encoder.layer.2.attention.self.value.bias', 'query_encoder.encoder.layer.10.attention.self.value.weight', 'query_encoder.encoder.layer.4.output.dense.bias', 'code_encoder.encoder.layer.2.attention.self.value.weight', 'query_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.query.bias', 'code_encoder.encoder.layer.4.attention.self.query.bias', 'query_encoder.encoder.layer.7.attention.output.dense.bias', 'code_encoder.encoder.layer.7.attention.self.value.bias', 'query_encoder.encoder.layer.0.attention.self.query.bias', 'code_encoder.encoder.layer.8.attention.self.key.weight', 'query_encoder.encoder.layer.10.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.self.query.weight', 'query_encoder.embeddings.LayerNorm.weight', 'query_encoder.encoder.layer.0.attention.self.query.weight', 'code_encoder.embeddings.token_type_embeddings.weight', 'code_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.intermediate.dense.bias', 'code_encoder.encoder.layer.3.intermediate.dense.bias', 'query_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.8.attention.self.value.weight', 'code_encoder.encoder.layer.10.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.self.value.bias', 'query_encoder.embeddings.position_embeddings.weight', 'code_encoder.encoder.layer.1.attention.self.query.weight', 'query_encoder.encoder.layer.5.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.query.weight', 'query_encoder.encoder.layer.2.output.dense.weight', 'code_encoder.encoder.layer.11.attention.self.query.weight', 'code_encoder.encoder.layer.5.intermediate.dense.bias', 'query_encoder.encoder.layer.0.intermediate.dense.bias', 'query_encoder.encoder.layer.4.attention.self.value.weight', 'query_encoder.encoder.layer.1.attention.self.value.weight', 'code_encoder.embeddings.word_embeddings.weight', 'query_encoder.encoder.layer.10.attention.output.dense.weight', 'query_encoder.encoder.layer.3.attention.self.key.weight', 'query_encoder.encoder.layer.0.attention.self.key.weight', 'code_encoder.encoder.layer.8.attention.output.dense.weight', 'query_encoder.encoder.layer.2.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./models/epoch_model_2 and are newly initialized: ['encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/epoch_model_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 13:05:20 - INFO - run_classifier -   Loading features from cached file ./data/cached_dev_valid_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 13:05:24 - INFO - run_classifier -   ***** Running evaluation  *****\n",
      "12/06/2022 13:05:24 - INFO - run_classifier -     Num examples = 83397\n",
      "12/06/2022 13:05:24 - INFO - run_classifier -     Batch size = 64\n",
      "Evaluating: 100%|██████████| 1304/1304 [04:48<00:00,  4.52it/s]\n",
      "12/06/2022 13:10:13 - INFO - __main__ -   Evaluate the following checkpoint: ./models/epoch_model_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_': 0.5005815556914517, 'f1_': 0.667183404717765, 'acc_and_f1_': 0.5838824802046083}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./models/epoch_model_3 were not used when initializing RobertaModel: ['code_encoder.encoder.layer.8.attention.self.value.bias', 'code_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.6.attention.output.dense.weight', 'query_encoder.encoder.layer.5.intermediate.dense.weight', 'query_encoder.encoder.layer.6.attention.self.key.bias', 'code_encoder.embeddings.position_embeddings.weight', 'code_encoder.encoder.layer.11.attention.output.dense.bias', 'query_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.6.attention.self.key.bias', 'query_encoder.encoder.layer.11.output.LayerNorm.bias', 'code_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.self.query.bias', 'query_encoder.embeddings.LayerNorm.bias', 'code_encoder.encoder.layer.0.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.attention.output.dense.bias', 'code_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.9.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.self.key.bias', 'code_encoder.encoder.layer.5.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.self.key.weight', 'code_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.intermediate.dense.weight', 'query_encoder.encoder.layer.11.intermediate.dense.weight', 'query_encoder.encoder.layer.3.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.11.attention.output.dense.weight', 'query_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.1.attention.output.dense.bias', 'code_encoder.encoder.layer.3.attention.output.dense.weight', 'query_encoder.encoder.layer.4.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.output.dense.bias', 'code_encoder.encoder.layer.6.output.dense.weight', 'code_encoder.encoder.layer.8.attention.self.key.bias', 'query_encoder.encoder.layer.9.attention.self.key.weight', 'query_encoder.encoder.layer.9.intermediate.dense.weight', 'code_encoder.encoder.layer.11.attention.self.key.weight', 'query_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.attention.self.value.bias', 'code_encoder.encoder.layer.2.intermediate.dense.weight', 'code_encoder.encoder.layer.9.intermediate.dense.bias', 'code_encoder.encoder.layer.8.output.dense.bias', 'query_encoder.encoder.layer.9.attention.self.key.bias', 'code_encoder.encoder.layer.9.attention.output.dense.weight', 'query_encoder.encoder.layer.7.attention.self.key.bias', 'code_encoder.encoder.layer.1.attention.self.query.bias', 'code_encoder.encoder.layer.7.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.output.dense.weight', 'query_encoder.encoder.layer.6.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.output.LayerNorm.bias', 'query_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.output.dense.weight', 'code_encoder.encoder.layer.5.attention.self.query.weight', 'query_encoder.encoder.layer.6.attention.self.query.weight', 'query_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.intermediate.dense.bias', 'query_encoder.encoder.layer.11.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.output.dense.bias', 'code_encoder.encoder.layer.11.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.intermediate.dense.weight', 'code_encoder.encoder.layer.3.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.dense.bias', 'query_encoder.encoder.layer.2.intermediate.dense.bias', 'code_encoder.encoder.layer.9.attention.self.key.bias', 'query_encoder.encoder.layer.2.intermediate.dense.weight', 'code_encoder.encoder.layer.0.intermediate.dense.weight', 'code_encoder.encoder.layer.8.intermediate.dense.bias', 'code_encoder.encoder.layer.1.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.self.query.weight', 'query_encoder.encoder.layer.4.attention.self.value.bias', 'query_encoder.encoder.layer.2.attention.self.value.bias', 'query_encoder.encoder.layer.3.attention.output.dense.weight', 'query_encoder.encoder.layer.0.intermediate.dense.weight', 'query_encoder.encoder.layer.2.attention.self.value.weight', 'code_encoder.encoder.layer.7.intermediate.dense.bias', 'code_encoder.encoder.layer.9.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.self.key.weight', 'code_encoder.encoder.layer.6.intermediate.dense.weight', 'code_encoder.encoder.layer.3.output.dense.weight', 'query_encoder.encoder.layer.10.attention.self.key.weight', 'code_encoder.encoder.layer.2.attention.self.key.weight', 'code_encoder.encoder.layer.1.attention.self.value.bias', 'code_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.7.output.dense.weight', 'code_encoder.encoder.layer.9.attention.self.value.weight', 'code_encoder.encoder.layer.5.output.dense.bias', 'code_encoder.encoder.layer.9.attention.self.query.bias', 'code_encoder.encoder.layer.1.attention.self.value.weight', 'query_encoder.encoder.layer.11.attention.self.key.weight', 'query_encoder.encoder.layer.6.intermediate.dense.bias', 'query_encoder.encoder.layer.8.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.output.dense.weight', 'query_encoder.encoder.layer.0.output.LayerNorm.weight', 'code_encoder.encoder.layer.3.attention.self.key.weight', 'query_encoder.encoder.layer.5.output.dense.weight', 'query_encoder.encoder.layer.5.intermediate.dense.bias', 'query_encoder.encoder.layer.6.output.dense.bias', 'query_encoder.encoder.layer.7.intermediate.dense.bias', 'query_encoder.encoder.layer.2.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.key.weight', 'code_encoder.encoder.layer.11.attention.self.value.weight', 'query_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.attention.self.key.weight', 'code_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.attention.self.query.weight', 'code_encoder.encoder.layer.0.attention.self.value.bias', 'code_encoder.encoder.layer.1.attention.output.dense.weight', 'code_encoder.encoder.layer.6.attention.output.dense.bias', 'code_encoder.encoder.layer.11.attention.self.value.bias', 'code_encoder.encoder.layer.5.intermediate.dense.weight', 'query_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.self.key.weight', 'code_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.1.intermediate.dense.bias', 'query_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.output.dense.bias', 'query_encoder.encoder.layer.4.attention.self.query.weight', 'query_encoder.encoder.layer.1.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.query.bias', 'code_encoder.encoder.layer.2.attention.output.dense.weight', 'query_encoder.encoder.layer.6.attention.self.value.bias', 'query_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.output.dense.bias', 'code_encoder.encoder.layer.7.attention.self.key.bias', 'code_encoder.encoder.layer.6.attention.self.query.bias', 'code_encoder.encoder.layer.10.attention.self.query.weight', 'code_encoder.encoder.layer.3.attention.self.query.bias', 'query_encoder.encoder.layer.5.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.attention.output.dense.bias', 'code_encoder.encoder.layer.7.attention.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.key.bias', 'query_encoder.encoder.layer.9.output.dense.weight', 'code_encoder.encoder.layer.5.output.LayerNorm.bias', 'query_encoder.encoder.layer.4.intermediate.dense.bias', 'query_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.output.LayerNorm.weight', 'code_encoder.encoder.layer.8.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.self.query.bias', 'query_encoder.encoder.layer.1.output.LayerNorm.bias', 'code_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.2.attention.self.key.bias', 'code_encoder.encoder.layer.6.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.output.dense.weight', 'code_encoder.encoder.layer.5.attention.self.key.bias', 'code_encoder.encoder.layer.7.output.dense.weight', 'query_encoder.encoder.layer.1.output.dense.bias', 'query_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.intermediate.dense.weight', 'code_encoder.encoder.layer.0.attention.self.query.bias', 'query_encoder.encoder.layer.0.attention.self.value.weight', 'code_encoder.encoder.layer.2.output.dense.weight', 'query_encoder.encoder.layer.7.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.output.dense.weight', 'code_encoder.embeddings.position_ids', 'code_encoder.encoder.layer.3.attention.self.query.weight', 'code_encoder.encoder.layer.4.output.dense.weight', 'code_encoder.encoder.layer.8.attention.self.query.weight', 'code_encoder.encoder.layer.7.intermediate.dense.weight', 'code_encoder.encoder.layer.6.attention.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.value.bias', 'code_encoder.encoder.layer.4.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.output.dense.weight', 'code_encoder.encoder.layer.7.attention.output.dense.bias', 'query_encoder.encoder.layer.8.attention.self.query.bias', 'code_encoder.encoder.layer.9.attention.self.key.weight', 'query_encoder.encoder.layer.7.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.output.dense.weight', 'query_encoder.encoder.layer.0.attention.output.dense.weight', 'query_encoder.encoder.layer.1.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.6.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.key.weight', 'code_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.output.dense.weight', 'query_encoder.encoder.layer.9.output.LayerNorm.weight', 'code_encoder.encoder.layer.7.attention.self.query.bias', 'query_encoder.encoder.layer.3.intermediate.dense.weight', 'code_encoder.encoder.layer.5.attention.self.value.bias', 'code_encoder.encoder.layer.1.intermediate.dense.weight', 'query_encoder.encoder.layer.2.attention.self.query.bias', 'query_encoder.encoder.layer.10.output.LayerNorm.weight', 'code_encoder.encoder.layer.7.output.dense.bias', 'query_encoder.encoder.layer.4.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.output.dense.bias', 'query_encoder.encoder.layer.11.intermediate.dense.bias', 'query_encoder.encoder.layer.1.intermediate.dense.bias', 'code_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.attention.output.dense.bias', 'code_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.attention.self.key.weight', 'query_encoder.encoder.layer.1.attention.self.key.bias', 'query_encoder.encoder.layer.3.output.dense.weight', 'query_encoder.encoder.layer.6.output.dense.weight', 'code_encoder.encoder.layer.9.attention.self.value.bias', 'query_encoder.encoder.layer.11.attention.self.query.weight', 'query_encoder.encoder.layer.1.attention.output.dense.weight', 'code_encoder.encoder.layer.8.output.dense.weight', 'query_encoder.encoder.layer.8.output.dense.weight', 'query_encoder.encoder.layer.11.output.dense.weight', 'code_encoder.encoder.layer.2.attention.self.query.bias', 'code_encoder.encoder.layer.0.attention.self.value.weight', 'code_encoder.encoder.layer.3.attention.self.key.bias', 'code_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.attention.output.dense.bias', 'code_encoder.encoder.layer.3.output.LayerNorm.bias', 'code_encoder.encoder.layer.8.attention.self.query.bias', 'code_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.0.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.attention.self.value.weight', 'code_encoder.embeddings.LayerNorm.bias', 'query_encoder.encoder.layer.10.output.dense.weight', 'query_encoder.encoder.layer.4.output.dense.weight', 'query_encoder.encoder.layer.7.attention.self.query.bias', 'query_encoder.embeddings.word_embeddings.weight', 'code_encoder.encoder.layer.6.attention.self.value.bias', 'query_encoder.encoder.layer.9.intermediate.dense.bias', 'query_encoder.encoder.layer.11.attention.self.value.weight', 'query_encoder.encoder.layer.7.attention.self.value.bias', 'code_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.attention.output.dense.weight', 'query_encoder.encoder.layer.0.attention.self.value.bias', 'code_encoder.encoder.layer.1.attention.self.key.bias', 'code_encoder.encoder.layer.11.attention.self.key.bias', 'query_encoder.encoder.layer.3.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.intermediate.dense.weight', 'code_encoder.encoder.layer.8.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.self.query.bias', 'query_encoder.encoder.layer.5.attention.self.key.bias', 'code_encoder.encoder.layer.0.intermediate.dense.bias', 'query_encoder.encoder.layer.0.output.dense.weight', 'code_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.11.output.dense.bias', 'code_encoder.embeddings.LayerNorm.weight', 'code_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.9.attention.self.query.weight', 'code_encoder.encoder.layer.1.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.attention.self.key.weight', 'code_encoder.encoder.layer.3.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.output.dense.weight', 'query_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.attention.self.query.weight', 'query_encoder.encoder.layer.7.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.attention.self.query.weight', 'query_encoder.encoder.layer.10.intermediate.dense.bias', 'query_encoder.encoder.layer.1.attention.self.query.bias', 'code_encoder.encoder.layer.10.attention.output.dense.bias', 'query_encoder.encoder.layer.7.attention.output.dense.weight', 'query_encoder.encoder.layer.10.attention.self.query.bias', 'code_encoder.encoder.layer.11.output.dense.weight', 'code_encoder.encoder.layer.11.attention.self.query.bias', 'code_encoder.encoder.layer.10.output.dense.bias', 'query_encoder.embeddings.position_ids', 'query_encoder.encoder.layer.7.attention.self.key.weight', 'code_encoder.encoder.layer.2.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.value.weight', 'code_encoder.encoder.layer.4.attention.self.value.bias', 'query_encoder.encoder.layer.3.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.intermediate.dense.weight', 'code_encoder.encoder.layer.4.output.dense.bias', 'code_encoder.encoder.layer.1.output.dense.bias', 'query_encoder.encoder.layer.4.attention.self.key.bias', 'query_encoder.encoder.layer.8.attention.output.dense.bias', 'query_encoder.encoder.layer.9.output.dense.bias', 'query_encoder.encoder.layer.4.attention.output.dense.weight', 'code_encoder.encoder.layer.8.attention.output.dense.bias', 'code_encoder.encoder.layer.3.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.self.query.bias', 'code_encoder.encoder.layer.0.attention.self.query.weight', 'query_encoder.encoder.layer.5.output.dense.bias', 'code_encoder.encoder.layer.9.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.9.attention.self.value.bias', 'code_encoder.encoder.layer.4.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.self.key.weight', 'query_encoder.encoder.layer.2.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.key.bias', 'code_encoder.encoder.layer.5.output.dense.weight', 'code_encoder.encoder.layer.10.intermediate.dense.weight', 'query_encoder.encoder.layer.8.attention.self.value.bias', 'query_encoder.encoder.layer.7.intermediate.dense.weight', 'query_encoder.encoder.layer.0.attention.output.dense.bias', 'code_encoder.encoder.layer.11.intermediate.dense.bias', 'code_encoder.encoder.layer.7.attention.self.value.weight', 'code_encoder.encoder.layer.9.output.dense.bias', 'query_encoder.encoder.layer.0.output.dense.bias', 'code_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.2.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.11.attention.output.dense.bias', 'code_encoder.encoder.layer.5.attention.self.value.weight', 'code_encoder.encoder.layer.10.output.LayerNorm.bias', 'query_encoder.encoder.layer.2.attention.self.key.weight', 'query_encoder.encoder.layer.2.attention.output.dense.bias', 'query_encoder.encoder.layer.8.intermediate.dense.weight', 'query_encoder.encoder.layer.6.attention.output.dense.bias', 'code_encoder.encoder.layer.3.attention.self.value.bias', 'query_encoder.encoder.layer.1.intermediate.dense.weight', 'code_encoder.encoder.layer.1.attention.self.key.weight', 'code_encoder.encoder.layer.8.intermediate.dense.weight', 'code_encoder.encoder.layer.3.attention.output.dense.bias', 'query_encoder.encoder.layer.6.intermediate.dense.weight', 'code_encoder.encoder.layer.0.output.dense.bias', 'code_encoder.encoder.layer.9.attention.self.query.weight', 'code_encoder.encoder.layer.0.attention.output.dense.weight', 'code_encoder.encoder.layer.2.attention.output.dense.bias', 'code_encoder.encoder.layer.10.intermediate.dense.bias', 'query_encoder.encoder.layer.6.attention.self.query.bias', 'query_encoder.encoder.layer.6.attention.self.key.weight', 'query_encoder.encoder.layer.8.intermediate.dense.bias', 'query_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.self.value.weight', 'query_encoder.encoder.layer.3.output.dense.bias', 'query_encoder.encoder.layer.1.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.value.weight', 'query_encoder.encoder.layer.7.attention.self.query.weight', 'query_encoder.encoder.layer.7.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.output.dense.bias', 'code_encoder.encoder.layer.0.attention.self.key.bias', 'query_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.output.dense.bias', 'code_encoder.encoder.layer.3.intermediate.dense.weight', 'code_encoder.encoder.layer.1.output.dense.weight', 'code_encoder.encoder.layer.6.output.dense.bias', 'code_encoder.encoder.layer.10.attention.self.value.bias', 'code_encoder.encoder.layer.2.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.intermediate.dense.bias', 'code_encoder.encoder.layer.6.intermediate.dense.bias', 'query_encoder.encoder.layer.1.attention.self.value.bias', 'code_encoder.encoder.layer.7.attention.self.key.weight', 'query_encoder.encoder.layer.0.attention.self.key.bias', 'query_encoder.encoder.layer.6.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.output.dense.bias', 'query_encoder.encoder.layer.9.attention.self.query.bias', 'code_encoder.encoder.layer.6.output.LayerNorm.weight', 'query_encoder.embeddings.token_type_embeddings.weight', 'query_encoder.encoder.layer.5.attention.self.value.bias', 'query_encoder.encoder.layer.2.output.dense.bias', 'code_encoder.encoder.layer.2.attention.self.value.bias', 'query_encoder.encoder.layer.10.attention.self.value.weight', 'query_encoder.encoder.layer.4.output.dense.bias', 'code_encoder.encoder.layer.2.attention.self.value.weight', 'query_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.query.bias', 'code_encoder.encoder.layer.4.attention.self.query.bias', 'query_encoder.encoder.layer.7.attention.output.dense.bias', 'code_encoder.encoder.layer.7.attention.self.value.bias', 'query_encoder.encoder.layer.0.attention.self.query.bias', 'code_encoder.encoder.layer.8.attention.self.key.weight', 'query_encoder.encoder.layer.10.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.self.query.weight', 'query_encoder.embeddings.LayerNorm.weight', 'query_encoder.encoder.layer.0.attention.self.query.weight', 'code_encoder.embeddings.token_type_embeddings.weight', 'code_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.intermediate.dense.bias', 'code_encoder.encoder.layer.3.intermediate.dense.bias', 'query_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.8.attention.self.value.weight', 'code_encoder.encoder.layer.10.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.self.value.bias', 'query_encoder.embeddings.position_embeddings.weight', 'code_encoder.encoder.layer.1.attention.self.query.weight', 'query_encoder.encoder.layer.5.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.query.weight', 'query_encoder.encoder.layer.2.output.dense.weight', 'code_encoder.encoder.layer.11.attention.self.query.weight', 'code_encoder.encoder.layer.5.intermediate.dense.bias', 'query_encoder.encoder.layer.0.intermediate.dense.bias', 'query_encoder.encoder.layer.4.attention.self.value.weight', 'query_encoder.encoder.layer.1.attention.self.value.weight', 'code_encoder.embeddings.word_embeddings.weight', 'query_encoder.encoder.layer.10.attention.output.dense.weight', 'query_encoder.encoder.layer.3.attention.self.key.weight', 'query_encoder.encoder.layer.0.attention.self.key.weight', 'code_encoder.encoder.layer.8.attention.output.dense.weight', 'query_encoder.encoder.layer.2.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./models/epoch_model_3 and are newly initialized: ['encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/epoch_model_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 13:10:27 - INFO - run_classifier -   Loading features from cached file ./data/cached_dev_valid_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 13:10:30 - INFO - run_classifier -   ***** Running evaluation  *****\n",
      "12/06/2022 13:10:30 - INFO - run_classifier -     Num examples = 83397\n",
      "12/06/2022 13:10:30 - INFO - run_classifier -     Batch size = 64\n",
      "Evaluating: 100%|██████████| 1304/1304 [04:48<00:00,  4.52it/s]\n",
      "12/06/2022 13:15:19 - INFO - __main__ -   Evaluate the following checkpoint: ./models/epoch_model_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_': 0.5005815556914517, 'f1_': 0.667183404717765, 'acc_and_f1_': 0.5838824802046083}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./models/epoch_model_4 were not used when initializing RobertaModel: ['code_encoder.encoder.layer.8.attention.self.value.bias', 'code_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.6.attention.output.dense.weight', 'query_encoder.encoder.layer.5.intermediate.dense.weight', 'query_encoder.encoder.layer.6.attention.self.key.bias', 'code_encoder.embeddings.position_embeddings.weight', 'code_encoder.encoder.layer.11.attention.output.dense.bias', 'query_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.6.attention.self.key.bias', 'query_encoder.encoder.layer.11.output.LayerNorm.bias', 'code_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.self.query.bias', 'query_encoder.embeddings.LayerNorm.bias', 'code_encoder.encoder.layer.0.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.attention.output.dense.bias', 'code_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.9.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.self.key.bias', 'code_encoder.encoder.layer.5.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.self.key.weight', 'code_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.intermediate.dense.weight', 'query_encoder.encoder.layer.11.intermediate.dense.weight', 'query_encoder.encoder.layer.3.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.11.attention.output.dense.weight', 'query_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.1.attention.output.dense.bias', 'code_encoder.encoder.layer.3.attention.output.dense.weight', 'query_encoder.encoder.layer.4.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.output.dense.bias', 'code_encoder.encoder.layer.6.output.dense.weight', 'code_encoder.encoder.layer.8.attention.self.key.bias', 'query_encoder.encoder.layer.9.attention.self.key.weight', 'query_encoder.encoder.layer.9.intermediate.dense.weight', 'code_encoder.encoder.layer.11.attention.self.key.weight', 'query_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.attention.self.value.bias', 'code_encoder.encoder.layer.2.intermediate.dense.weight', 'code_encoder.encoder.layer.9.intermediate.dense.bias', 'code_encoder.encoder.layer.8.output.dense.bias', 'query_encoder.encoder.layer.9.attention.self.key.bias', 'code_encoder.encoder.layer.9.attention.output.dense.weight', 'query_encoder.encoder.layer.7.attention.self.key.bias', 'code_encoder.encoder.layer.1.attention.self.query.bias', 'code_encoder.encoder.layer.7.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.output.dense.weight', 'query_encoder.encoder.layer.6.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.output.LayerNorm.bias', 'query_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.output.dense.weight', 'code_encoder.encoder.layer.5.attention.self.query.weight', 'query_encoder.encoder.layer.6.attention.self.query.weight', 'query_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.intermediate.dense.bias', 'query_encoder.encoder.layer.11.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.output.dense.bias', 'code_encoder.encoder.layer.11.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.intermediate.dense.weight', 'code_encoder.encoder.layer.3.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.dense.bias', 'query_encoder.encoder.layer.2.intermediate.dense.bias', 'code_encoder.encoder.layer.9.attention.self.key.bias', 'query_encoder.encoder.layer.2.intermediate.dense.weight', 'code_encoder.encoder.layer.0.intermediate.dense.weight', 'code_encoder.encoder.layer.8.intermediate.dense.bias', 'code_encoder.encoder.layer.1.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.self.query.weight', 'query_encoder.encoder.layer.4.attention.self.value.bias', 'query_encoder.encoder.layer.2.attention.self.value.bias', 'query_encoder.encoder.layer.3.attention.output.dense.weight', 'query_encoder.encoder.layer.0.intermediate.dense.weight', 'query_encoder.encoder.layer.2.attention.self.value.weight', 'code_encoder.encoder.layer.7.intermediate.dense.bias', 'code_encoder.encoder.layer.9.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.self.key.weight', 'code_encoder.encoder.layer.6.intermediate.dense.weight', 'code_encoder.encoder.layer.3.output.dense.weight', 'query_encoder.encoder.layer.10.attention.self.key.weight', 'code_encoder.encoder.layer.2.attention.self.key.weight', 'code_encoder.encoder.layer.1.attention.self.value.bias', 'code_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.7.output.dense.weight', 'code_encoder.encoder.layer.9.attention.self.value.weight', 'code_encoder.encoder.layer.5.output.dense.bias', 'code_encoder.encoder.layer.9.attention.self.query.bias', 'code_encoder.encoder.layer.1.attention.self.value.weight', 'query_encoder.encoder.layer.11.attention.self.key.weight', 'query_encoder.encoder.layer.6.intermediate.dense.bias', 'query_encoder.encoder.layer.8.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.output.dense.weight', 'query_encoder.encoder.layer.0.output.LayerNorm.weight', 'code_encoder.encoder.layer.3.attention.self.key.weight', 'query_encoder.encoder.layer.5.output.dense.weight', 'query_encoder.encoder.layer.5.intermediate.dense.bias', 'query_encoder.encoder.layer.6.output.dense.bias', 'query_encoder.encoder.layer.7.intermediate.dense.bias', 'query_encoder.encoder.layer.2.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.key.weight', 'code_encoder.encoder.layer.11.attention.self.value.weight', 'query_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.attention.self.key.weight', 'code_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.attention.self.query.weight', 'code_encoder.encoder.layer.0.attention.self.value.bias', 'code_encoder.encoder.layer.1.attention.output.dense.weight', 'code_encoder.encoder.layer.6.attention.output.dense.bias', 'code_encoder.encoder.layer.11.attention.self.value.bias', 'code_encoder.encoder.layer.5.intermediate.dense.weight', 'query_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.self.key.weight', 'code_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.1.intermediate.dense.bias', 'query_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.output.dense.bias', 'query_encoder.encoder.layer.4.attention.self.query.weight', 'query_encoder.encoder.layer.1.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.query.bias', 'code_encoder.encoder.layer.2.attention.output.dense.weight', 'query_encoder.encoder.layer.6.attention.self.value.bias', 'query_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.output.dense.bias', 'code_encoder.encoder.layer.7.attention.self.key.bias', 'code_encoder.encoder.layer.6.attention.self.query.bias', 'code_encoder.encoder.layer.10.attention.self.query.weight', 'code_encoder.encoder.layer.3.attention.self.query.bias', 'query_encoder.encoder.layer.5.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.attention.output.dense.bias', 'code_encoder.encoder.layer.7.attention.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.key.bias', 'query_encoder.encoder.layer.9.output.dense.weight', 'code_encoder.encoder.layer.5.output.LayerNorm.bias', 'query_encoder.encoder.layer.4.intermediate.dense.bias', 'query_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.output.LayerNorm.weight', 'code_encoder.encoder.layer.8.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.self.query.bias', 'query_encoder.encoder.layer.1.output.LayerNorm.bias', 'code_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.2.attention.self.key.bias', 'code_encoder.encoder.layer.6.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.output.dense.weight', 'code_encoder.encoder.layer.5.attention.self.key.bias', 'code_encoder.encoder.layer.7.output.dense.weight', 'query_encoder.encoder.layer.1.output.dense.bias', 'query_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.intermediate.dense.weight', 'code_encoder.encoder.layer.0.attention.self.query.bias', 'query_encoder.encoder.layer.0.attention.self.value.weight', 'code_encoder.encoder.layer.2.output.dense.weight', 'query_encoder.encoder.layer.7.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.output.dense.weight', 'code_encoder.embeddings.position_ids', 'code_encoder.encoder.layer.3.attention.self.query.weight', 'code_encoder.encoder.layer.4.output.dense.weight', 'code_encoder.encoder.layer.8.attention.self.query.weight', 'code_encoder.encoder.layer.7.intermediate.dense.weight', 'code_encoder.encoder.layer.6.attention.output.dense.weight', 'query_encoder.encoder.layer.11.attention.self.value.bias', 'code_encoder.encoder.layer.4.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.output.dense.weight', 'code_encoder.encoder.layer.7.attention.output.dense.bias', 'query_encoder.encoder.layer.8.attention.self.query.bias', 'code_encoder.encoder.layer.9.attention.self.key.weight', 'query_encoder.encoder.layer.7.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.output.dense.weight', 'query_encoder.encoder.layer.0.attention.output.dense.weight', 'query_encoder.encoder.layer.1.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.6.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.key.weight', 'code_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.output.dense.weight', 'query_encoder.encoder.layer.9.output.LayerNorm.weight', 'code_encoder.encoder.layer.7.attention.self.query.bias', 'query_encoder.encoder.layer.3.intermediate.dense.weight', 'code_encoder.encoder.layer.5.attention.self.value.bias', 'code_encoder.encoder.layer.1.intermediate.dense.weight', 'query_encoder.encoder.layer.2.attention.self.query.bias', 'query_encoder.encoder.layer.10.output.LayerNorm.weight', 'code_encoder.encoder.layer.7.output.dense.bias', 'query_encoder.encoder.layer.4.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.output.dense.bias', 'query_encoder.encoder.layer.11.intermediate.dense.bias', 'query_encoder.encoder.layer.1.intermediate.dense.bias', 'code_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.9.attention.output.dense.bias', 'code_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.attention.self.key.weight', 'query_encoder.encoder.layer.1.attention.self.key.bias', 'query_encoder.encoder.layer.3.output.dense.weight', 'query_encoder.encoder.layer.6.output.dense.weight', 'code_encoder.encoder.layer.9.attention.self.value.bias', 'query_encoder.encoder.layer.11.attention.self.query.weight', 'query_encoder.encoder.layer.1.attention.output.dense.weight', 'code_encoder.encoder.layer.8.output.dense.weight', 'query_encoder.encoder.layer.8.output.dense.weight', 'query_encoder.encoder.layer.11.output.dense.weight', 'code_encoder.encoder.layer.2.attention.self.query.bias', 'code_encoder.encoder.layer.0.attention.self.value.weight', 'code_encoder.encoder.layer.3.attention.self.key.bias', 'code_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.10.attention.output.dense.bias', 'code_encoder.encoder.layer.3.output.LayerNorm.bias', 'code_encoder.encoder.layer.8.attention.self.query.bias', 'code_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.0.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.attention.self.value.weight', 'code_encoder.embeddings.LayerNorm.bias', 'query_encoder.encoder.layer.10.output.dense.weight', 'query_encoder.encoder.layer.4.output.dense.weight', 'query_encoder.encoder.layer.7.attention.self.query.bias', 'query_encoder.embeddings.word_embeddings.weight', 'code_encoder.encoder.layer.6.attention.self.value.bias', 'query_encoder.encoder.layer.9.intermediate.dense.bias', 'query_encoder.encoder.layer.11.attention.self.value.weight', 'query_encoder.encoder.layer.7.attention.self.value.bias', 'code_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.attention.output.dense.weight', 'query_encoder.encoder.layer.0.attention.self.value.bias', 'code_encoder.encoder.layer.1.attention.self.key.bias', 'code_encoder.encoder.layer.11.attention.self.key.bias', 'query_encoder.encoder.layer.3.output.LayerNorm.bias', 'code_encoder.encoder.layer.11.intermediate.dense.weight', 'code_encoder.encoder.layer.8.output.LayerNorm.bias', 'code_encoder.encoder.layer.10.attention.self.query.bias', 'query_encoder.encoder.layer.5.attention.self.key.bias', 'code_encoder.encoder.layer.0.intermediate.dense.bias', 'query_encoder.encoder.layer.0.output.dense.weight', 'code_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.11.output.dense.bias', 'code_encoder.embeddings.LayerNorm.weight', 'code_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.9.attention.self.query.weight', 'code_encoder.encoder.layer.1.output.LayerNorm.bias', 'code_encoder.encoder.layer.7.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.attention.self.key.weight', 'code_encoder.encoder.layer.3.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.output.dense.weight', 'query_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.8.attention.self.query.weight', 'query_encoder.encoder.layer.7.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.5.attention.self.query.weight', 'query_encoder.encoder.layer.10.intermediate.dense.bias', 'query_encoder.encoder.layer.1.attention.self.query.bias', 'code_encoder.encoder.layer.10.attention.output.dense.bias', 'query_encoder.encoder.layer.7.attention.output.dense.weight', 'query_encoder.encoder.layer.10.attention.self.query.bias', 'code_encoder.encoder.layer.11.output.dense.weight', 'code_encoder.encoder.layer.11.attention.self.query.bias', 'code_encoder.encoder.layer.10.output.dense.bias', 'query_encoder.embeddings.position_ids', 'query_encoder.encoder.layer.7.attention.self.key.weight', 'code_encoder.encoder.layer.2.output.LayerNorm.bias', 'code_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.10.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.value.weight', 'code_encoder.encoder.layer.4.attention.self.value.bias', 'query_encoder.encoder.layer.3.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.intermediate.dense.weight', 'code_encoder.encoder.layer.4.output.dense.bias', 'code_encoder.encoder.layer.1.output.dense.bias', 'query_encoder.encoder.layer.4.attention.self.key.bias', 'query_encoder.encoder.layer.8.attention.output.dense.bias', 'query_encoder.encoder.layer.9.output.dense.bias', 'query_encoder.encoder.layer.4.attention.output.dense.weight', 'code_encoder.encoder.layer.8.attention.output.dense.bias', 'code_encoder.encoder.layer.3.output.LayerNorm.weight', 'query_encoder.encoder.layer.4.attention.self.query.bias', 'code_encoder.encoder.layer.0.attention.self.query.weight', 'query_encoder.encoder.layer.5.output.dense.bias', 'code_encoder.encoder.layer.9.output.LayerNorm.weight', 'code_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.9.attention.self.value.bias', 'code_encoder.encoder.layer.4.attention.self.key.bias', 'query_encoder.encoder.layer.5.attention.self.key.weight', 'query_encoder.encoder.layer.2.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.key.bias', 'code_encoder.encoder.layer.5.output.dense.weight', 'code_encoder.encoder.layer.10.intermediate.dense.weight', 'query_encoder.encoder.layer.8.attention.self.value.bias', 'query_encoder.encoder.layer.7.intermediate.dense.weight', 'query_encoder.encoder.layer.0.attention.output.dense.bias', 'code_encoder.encoder.layer.11.intermediate.dense.bias', 'code_encoder.encoder.layer.7.attention.self.value.weight', 'code_encoder.encoder.layer.9.output.dense.bias', 'query_encoder.encoder.layer.0.output.dense.bias', 'code_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.2.output.LayerNorm.weight', 'code_encoder.encoder.layer.5.attention.output.dense.bias', 'code_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'query_encoder.encoder.layer.11.attention.output.dense.bias', 'code_encoder.encoder.layer.5.attention.self.value.weight', 'code_encoder.encoder.layer.10.output.LayerNorm.bias', 'query_encoder.encoder.layer.2.attention.self.key.weight', 'query_encoder.encoder.layer.2.attention.output.dense.bias', 'query_encoder.encoder.layer.8.intermediate.dense.weight', 'query_encoder.encoder.layer.6.attention.output.dense.bias', 'code_encoder.encoder.layer.3.attention.self.value.bias', 'query_encoder.encoder.layer.1.intermediate.dense.weight', 'code_encoder.encoder.layer.1.attention.self.key.weight', 'code_encoder.encoder.layer.8.intermediate.dense.weight', 'code_encoder.encoder.layer.3.attention.output.dense.bias', 'query_encoder.encoder.layer.6.intermediate.dense.weight', 'code_encoder.encoder.layer.0.output.dense.bias', 'code_encoder.encoder.layer.9.attention.self.query.weight', 'code_encoder.encoder.layer.0.attention.output.dense.weight', 'code_encoder.encoder.layer.2.attention.output.dense.bias', 'code_encoder.encoder.layer.10.intermediate.dense.bias', 'query_encoder.encoder.layer.6.attention.self.query.bias', 'query_encoder.encoder.layer.6.attention.self.key.weight', 'query_encoder.encoder.layer.8.intermediate.dense.bias', 'query_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.self.value.weight', 'query_encoder.encoder.layer.3.output.dense.bias', 'query_encoder.encoder.layer.1.attention.self.query.weight', 'query_encoder.encoder.layer.8.attention.self.value.weight', 'query_encoder.encoder.layer.7.attention.self.query.weight', 'query_encoder.encoder.layer.7.attention.self.value.weight', 'query_encoder.encoder.layer.9.attention.output.dense.bias', 'code_encoder.encoder.layer.0.attention.self.key.bias', 'query_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'code_encoder.encoder.layer.4.attention.output.dense.bias', 'code_encoder.encoder.layer.3.intermediate.dense.weight', 'code_encoder.encoder.layer.1.output.dense.weight', 'code_encoder.encoder.layer.6.output.dense.bias', 'code_encoder.encoder.layer.10.attention.self.value.bias', 'code_encoder.encoder.layer.2.output.LayerNorm.weight', 'code_encoder.encoder.layer.2.intermediate.dense.bias', 'code_encoder.encoder.layer.6.intermediate.dense.bias', 'query_encoder.encoder.layer.1.attention.self.value.bias', 'code_encoder.encoder.layer.7.attention.self.key.weight', 'query_encoder.encoder.layer.0.attention.self.key.bias', 'query_encoder.encoder.layer.6.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.output.dense.bias', 'query_encoder.encoder.layer.9.attention.self.query.bias', 'code_encoder.encoder.layer.6.output.LayerNorm.weight', 'query_encoder.embeddings.token_type_embeddings.weight', 'query_encoder.encoder.layer.5.attention.self.value.bias', 'query_encoder.encoder.layer.2.output.dense.bias', 'code_encoder.encoder.layer.2.attention.self.value.bias', 'query_encoder.encoder.layer.10.attention.self.value.weight', 'query_encoder.encoder.layer.4.output.dense.bias', 'code_encoder.encoder.layer.2.attention.self.value.weight', 'query_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'query_encoder.encoder.layer.3.attention.self.query.bias', 'code_encoder.encoder.layer.4.attention.self.query.bias', 'query_encoder.encoder.layer.7.attention.output.dense.bias', 'code_encoder.encoder.layer.7.attention.self.value.bias', 'query_encoder.encoder.layer.0.attention.self.query.bias', 'code_encoder.encoder.layer.8.attention.self.key.weight', 'query_encoder.encoder.layer.10.output.LayerNorm.bias', 'query_encoder.encoder.layer.10.attention.self.query.weight', 'query_encoder.embeddings.LayerNorm.weight', 'query_encoder.encoder.layer.0.attention.self.query.weight', 'code_encoder.embeddings.token_type_embeddings.weight', 'code_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.4.intermediate.dense.bias', 'code_encoder.encoder.layer.3.intermediate.dense.bias', 'query_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'code_encoder.encoder.layer.8.attention.self.value.weight', 'code_encoder.encoder.layer.10.output.LayerNorm.weight', 'query_encoder.encoder.layer.3.attention.self.value.bias', 'query_encoder.embeddings.position_embeddings.weight', 'code_encoder.encoder.layer.1.attention.self.query.weight', 'query_encoder.encoder.layer.5.attention.self.value.weight', 'code_encoder.encoder.layer.6.attention.self.query.weight', 'query_encoder.encoder.layer.2.output.dense.weight', 'code_encoder.encoder.layer.11.attention.self.query.weight', 'code_encoder.encoder.layer.5.intermediate.dense.bias', 'query_encoder.encoder.layer.0.intermediate.dense.bias', 'query_encoder.encoder.layer.4.attention.self.value.weight', 'query_encoder.encoder.layer.1.attention.self.value.weight', 'code_encoder.embeddings.word_embeddings.weight', 'query_encoder.encoder.layer.10.attention.output.dense.weight', 'query_encoder.encoder.layer.3.attention.self.key.weight', 'query_encoder.encoder.layer.0.attention.self.key.weight', 'code_encoder.encoder.layer.8.attention.output.dense.weight', 'query_encoder.encoder.layer.2.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./models/epoch_model_4 and are newly initialized: ['encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/epoch_model_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 13:15:33 - INFO - run_classifier -   Loading features from cached file ./data/cached_dev_valid_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 13:15:36 - INFO - run_classifier -   ***** Running evaluation  *****\n",
      "12/06/2022 13:15:36 - INFO - run_classifier -     Num examples = 83397\n",
      "12/06/2022 13:15:36 - INFO - run_classifier -     Batch size = 64\n",
      "Evaluating: 100%|██████████| 1304/1304 [04:48<00:00,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_': 0.5005815556914517, 'f1_': 0.667183404717765, 'acc_and_f1_': 0.5838824802046083}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Optional part 2 goes here\n",
    "# After training completes, let's see how well each of the models does on the test set. \n",
    "# We've already provided some code that does the evaluation on the best performing model. \n",
    "# Add more code so that it evaluates the models you just saved for each epoch \n",
    "# and make a plot of the performance on the test set\n",
    "\n",
    "tot_results = []\n",
    "# train_iterator = trange(args.start_epoch, int(args.num_train_epochs), desc=\"Epoch\")\n",
    "for idx in range(int(args.num_train_epochs)):\n",
    "    \n",
    "    checkpoint = os.path.join(args.output_dir, 'epoch_model_{}'.format(idx))\n",
    "    logger.info(\"Evaluate the following checkpoint: %s\", checkpoint)\n",
    "    \n",
    "    model = AutoModel.from_pretrained(checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "    model.to(args.device)\n",
    "\n",
    "    print(checkpoint)\n",
    "    global_step = \"\"\n",
    "    model = model_class.from_pretrained(checkpoint)\n",
    "    model.to(args.device)\n",
    "    result = evaluate(args, model, tokenizer, checkpoint=checkpoint, prefix=global_step)\n",
    "    result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
    "    print(result)\n",
    "    tot_results.append(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch index')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "result_df = pd.DataFrame(tot_results)\n",
    "ax = sns.barplot(x = result_df.index, y = result_df['acc_'])\n",
    "ax.set_xlabel('epoch index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch index')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.barplot(x = result_df.index, y = result_df['f1_'])\n",
    "ax.set_xlabel('epoch index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch index')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax2 = sns.barplot(x = result_df.index, y = result_df['acc_and_f1_'])\n",
    "ax2.set_xlabel('epoch index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/epoch_model_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 13:27:27 - INFO - run_classifier -   Loading features from cached file ./data/cached_test_test_data_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 13:27:30 - INFO - utils -   LOOKING AT ./data/test_data.txt\n",
      "Evaluating: 100%|██████████| 1482/1482 [05:26<00:00,  4.54it/s]\n",
      "12/06/2022 13:32:58 - INFO - __main__ -   ***** Writing relevance predictions *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/epoch_model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 13:33:01 - INFO - run_classifier -   Loading features from cached file ./data/cached_test_test_data_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 13:33:04 - INFO - utils -   LOOKING AT ./data/test_data.txt\n",
      "Evaluating: 100%|██████████| 1482/1482 [05:26<00:00,  4.54it/s]\n",
      "12/06/2022 13:38:32 - INFO - __main__ -   ***** Writing relevance predictions *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/epoch_model_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 13:38:34 - INFO - run_classifier -   Loading features from cached file ./data/cached_test_test_data_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 13:38:37 - INFO - utils -   LOOKING AT ./data/test_data.txt\n",
      "Evaluating: 100%|██████████| 1482/1482 [05:26<00:00,  4.54it/s]\n",
      "12/06/2022 13:44:06 - INFO - __main__ -   ***** Writing relevance predictions *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/epoch_model_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 13:44:08 - INFO - run_classifier -   Loading features from cached file ./data/cached_test_test_data_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 13:44:11 - INFO - utils -   LOOKING AT ./data/test_data.txt\n",
      "Evaluating: 100%|██████████| 1482/1482 [05:26<00:00,  4.54it/s]\n",
      "12/06/2022 13:49:39 - INFO - __main__ -   ***** Writing relevance predictions *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/epoch_model_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/06/2022 13:49:42 - INFO - run_classifier -   Loading features from cached file ./data/cached_test_test_data_pytorch_model.bin_200_codesearch\n",
      "12/06/2022 13:49:45 - INFO - utils -   LOOKING AT ./data/test_data.txt\n",
      "Evaluating: 100%|██████████| 1482/1482 [05:26<00:00,  4.54it/s]\n",
      "12/06/2022 13:55:13 - INFO - __main__ -   ***** Writing relevance predictions *****\n"
     ]
    }
   ],
   "source": [
    "# Optional part 3 goes here\n",
    "# To measure the impact on NDCG, \n",
    "# we'll need to calculate the different bi-encoders' relevance estimates to different files to use in Part 2. \n",
    "# You'll need to add more code that loads in each of these models from the checkpoint directories \n",
    "# and runs the inference\n",
    "tot_rel = []\n",
    "for idx in range(int(args.num_train_epochs)):\n",
    "    checkpoint = os.path.join(args.output_dir, 'epoch_model_{}'.format(idx))\n",
    "    \n",
    "    print(checkpoint)\n",
    "    model = model_class.from_pretrained(checkpoint)\n",
    "    model.to(args.device)\n",
    "    \n",
    "    model.eval()\n",
    "    eval_dataset, instances = load_and_cache_examples(args, \"codesearch\", tokenizer, ttype='test')\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "    relevance_predictions = np.array([])\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = {'query_token_ids': batch[0],\n",
    "                      'code_token_ids': batch[1],\n",
    "                      'labels': batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Let's pull out just the cosine similarity\n",
    "        _, cosine_sim = outputs[:2] \n",
    "        cosine_sims = cosine_sim.cpu().numpy()\n",
    "    \n",
    "        # Add these similarities to our current similarities\n",
    "        relevance_predictions = np.append(relevance_predictions, cosine_sims, axis=0)\n",
    "\n",
    "    if not os.path.exists(args.test_result_dir):\n",
    "        os.makedirs(args.test_result_dir)\n",
    "\n",
    "    output_test_file = os.path.join(args.test_result_dir, 'relevance-scores_{}.csv'.format(idx))\n",
    "\n",
    "    with open(output_test_file, \"w\") as outf:\n",
    "        logger.info(\"***** Writing relevance predictions *****\")\n",
    "        all_logits = relevance_predictions.tolist()\n",
    "    \n",
    "        # Note that we write these all as one big list. In the next step,\n",
    "        # we'll merge these with the data frame\n",
    "        outf.write(\",\".join([str(item) for item in all_logits]))\n",
    "    \n",
    "    tot_rel.append(relevance_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Part 4, just write the different models' predictions to separate files. \n",
    "# You might combine this with the part 3 code if it's easier to do there. \n",
    "# We haven't marked a spot for it explicitly, but you'll use these files in part 2.\n",
    "\n",
    "df = pd.read_csv('data/test_data.csv').rename(columns = {'Query_id': 'qid', 'Doc_id':'docno'})\n",
    "for idx in range(int(args.num_train_epochs)):\n",
    "    df_temp = df.copy()\n",
    "    df_temp['sim'] = tot_rel[idx]\n",
    "    df_temp[[\"sim\",'qid','docno']].to_csv('results/test_relevance_{}.csv'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN3HDF3vRE017G93FTr+2MX",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
